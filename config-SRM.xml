<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % sharedents SYSTEM "shared-entities.xml" >
%sharedents;
]>

<chapter id="cf-srm">

  <title>&dcache; Storage Resource Manager </title>

  <chapterinfo>
    <authorgroup>
      <author>
	<firstname>Gerd</firstname>
	<surname>Behrmann</surname>
      </author>
      <author>
	<firstname>Dmitry</firstname>
	<surname>Litvintsev</surname>
      </author>

      <author>
	<firstname>Timur</firstname>
	<surname>Perelmutov</surname>
      </author>

      <author>
	<firstname>Vladimir</firstname>
	<surname>Podstavkov</surname>
      </author>
    </authorgroup>
  </chapterinfo>

  <!--
  <para>(We assume a basic knowledge of the functionality and terminology of
  dCache. Pool, domain, cell, how to log into a cell, and execute
  commands)</para>
  -->
  <!--
     ##############################################################
     #            Introduction                                    #
     ##############################################################
   -->
  <section id="cf-srm-intro">
    <title>Introduction</title>

    <para>
      <firstterm>Storage Resource Managers</firstterm> (&srm;s) are
      middleware components whose function is to provide dynamic space
      allocation and file management on shared storage components on
      the Grid. &srm;s support protocol negotiation and a reliable
      replication mechanism. The &srm; specification standardizes the
      interface, thus allowing for a uniform access to heterogeneous
      storage elements.
    </para>

    <section>
      <title>General &srm; Concepts </title>

      <para>
	The &srm; interface consists of the five categories of functions:
	Space Management, Data Transfer, Request Status, Directory and
	Permission Functions. &srm; interface utilizes Grid Security
	Infrastructure (&gsi;) for authentications. &srm; service is a
	Web Service implementation of a published WSDL
	document. Please visit the <ulink
	url="http://sdm.lbl.gov/srm-wg/"> SRM Working Group
	Page</ulink> to see the &srm; Version 1.1 and &srm; Version
	2.2 protocol specification documents.
      </para>

      <section>
	<title>&surl;s</title>

	<para>
	  &srm; defines a protocol name &srm;, and introduces a way to
	  address the files stored in the &srm; managed storage by
	  Site &url; of the format
	  <literal>srm://&lt;host&gt;:&lt;port&gt;/[&lt;web service
	  path&gt;?SFN=]&lt;path&gt;</literal>. Examples of the Site
	  &url;s a.k.a. &srm; &url;s are:
	</para>

	<programlisting>srm://fapl110.fnal.gov:8443/srm/managerv2?SFN=//pnfs/fnal.gov/data/test/file1,
srm://fapl110.fnal.gov:8443/srm/managerv1?SFN=/pnfs/fnal.gov/data/test/file2
srm://srm.cern.ch:8443/castor/cern.ch/cms/store/cmsfile23</programlisting>


        <para>
	  All &srm; functions that operate on files use Site &url;s
	  (&surl;s) for file references.
	</para>
      </section>

      <section>
	<title>Data Transfer functions </title>

	<para>
	  There are three functions for performing data transfers in
	  &srm;, namely srmPrepareToGet, srmPrepareToPut and
	  srmCopy. These are &srm; Version 2.2 names, in &srm; Version
	  1.1 they were called get, put and copy, but their roles were
	  essentially the same. These functions take list of
	  sources(srmPrepareToGet), destinations(srmPrepareToPut) or
	  both (srmCopy). The role of the srmPrepareToGet function is
	  to prepare the system for the receipt of the data into the
	  given file names, to make sure that the system has enough
	  space to store the files, that the user has sufficient
	  privileges to create the files in the paths designated by
	  the &surl;s. The purpose of the srmPrepareToPut function is
	  to prepare the data stored in files, designated by the given
	  &surl;s , that are already a part of the system for the
	  network access; &srm; needs again to check that the user has
	  sufficient privileges to access the files in the paths
	  designated by the &surl;s.  One of the features of
	  thesrmPrepareToGet and srmPrepareToPut functions is that
	  they both support transfer protocol negotiation. This means
	  that in case of both of these functions client supplies a
	  list of supported transfer protocols and &srm; server
	  computes the Transfer &url; in the first protocol from the
	  list that it supports. Depending on the implementation the
	  real action in the Storage System performed in response to
	  this invocation may range from simple &surl; to &turl;
	  translation to a Stage from Tape to Disk Cache and dynamic
	  selection of the transfer host and transfer protocol
	  depending on the protocol availability and current load on
	  each of the transfer server load. It is a responsibility of
	  the client to perform the transfer and to notify the &srm;
	  that it is done with the files.  srmCopy function performs a
	  copy between a local and a remote storage system, it is
	  given a list of source – destination &url; pairs. At least one
	  of the &url;s in each pair must be an &surl; of file in the
	  &srm; system contacted with the srmCopy request. Second &url;
	  can be a local or remote &surl; or &url; in some other
	  transfer protocol. In case of srmCopy the &srm; system
	  performs data transfer itself, without data ever flowing
	  though the client’s computer.
	</para>

	<para>
	  The Data Transfer functions are asynchronous, initial &srm;
	  call leads to the start of the execution of the client’s
	  request, and the functions return the request statuses, that
	  contain unique request tokens, that can be used in
	  subsequent calls for periodic polling of the status of the
	  request. Once the &srm; completes the requests, and clients
	  are done with the data transfers, clients notify the system
	  that they are done with the files and are ready to release
	  the associated resources, the client notifies the system by
	  execution of the srmReleaseFiles in case of srmPrepareToGet
	  or srmPutDone in case of srmPrepareToPut. In case of
	  srmCopy, system knows when the transfer is competed and
	  resources can be released, so it requires no special
	  function at the end.
	</para>

	<para>
	  Clients are free to cancel the requests at any time by
	  execution of the srmAbortFiles or srmAbortRequest.
	</para>

      </section>

      <section>
	<title>Space Management functions</title>

	<para>
	  &srm; Version 2.2 introduces a concept of space
	  reservation. Space reservation is a promise by the storage
	  system to make certain amount of storage space of certain
	  type available for usage for a specified period of
	  time. Space reservation is made using srmReserveSpace
	  function. In case of successful reservation, a unique name,
	  called space token is assigned to the reservation. Space
	  token can be used during the transfer operations to tell the
	  system to put the files being manipulated or transferred
	  into an associated space reservation . A storage system
	  ensures that the reserved amount of the disk space is indeed
	  available, thus providing a guarantee that a client does not
	  run out of space until all space promised by the reservation
	  has been used. When files are deleted, the space is returned
	  to the space reservation.
	</para>

	<para>
	  A space reservation has a property called retention
	  policy. Possible values of retention policy are Replica,
	  Output and Custodial. The retention policy describes the
	  quality of the storage service that will be provided for the
	  data (files) stored in this space reservation. Replica
	  corresponds to the lowest quality of the service, usually
	  associated with storing a single copy of each file on the
	  disk. Custodial is the highest quality service, usually
	  interpreted as storage of the data on Tape. WLCG has decided
	  not to use Output retention policy in its data grid. Output
	  is an intermediate retention policy is stronger than Replica
	  and weaker than Custodial, and in &dcache; Output retention
	  policy will possibly be used for files managed by Resilient
	  Manager, which will make several internal copies of each
	  file, distributed on distinct instances of hardware. Once a
	  file is written into a given space reservation, it inherits
	  the reservation's retention policy.
	</para>

	<para>
	  Another property of the space reservation is called access
	  latency. The two values allowed are Nearline and
	  Online. Nearline means that the data stored in this
	  reservation are allowed to be stored in such a way that
	  retrieving them might require storage system to perform
	  additional preparatory steps (staging data from tape to a
	  disk cache for example). Online means that data is readily
	  available and it will not take long to start reading the
	  date. In case of &dcache; Online means that there will always
	  be a copy of the file on disk, while Nearline does not
	  provide such guarantee. As with retention policy, once a
	  file is written into a given space reservation, it inherits
	  the reservation's access latency.
	</para>

	<para>
	  DCache however only manages write space, i.e. only space on
	  disk can be reserved and only for write operations. Once
	  files are migrated to tape, and if no copy is required on
	  disk, space used by these files is returned back into space
	  reservation. When files are read back from tape and cached
	  on disk, they are not counted as part of any space.  &srm;
	  Space reservation can be assigned a non-unique description,
	  then the description cab be used in the future to discover
	  all space reservation with a given description.
	</para>

	<para>
	  Properties of the &srm; Space Reservations can be discovered
	  using SrmGetSpaceMetadata function. Space Reservations might
	  be released with srmReleaseSpace. For a complete description
	  of the available functions please see <ulink
	  url="http://sdm.lbl.gov/srm-wg/doc/SRM.v2.2.html"> SRM
	  Version 2.2 Specification</ulink>.
	</para>

      </section>


      <section>
	<title>Utilization of the Space Reservations for Data Storage </title>

	<para>
	  &srm; Version 2.2 srmPrepareToPut and srmCopy pull mode
	  transfers allow the user to specify a space reservation
	  token or a retention policy and an access latency. In the
	  protocol, any of these values are optional, and it is up to
	  the implementation to decide what to do, if these properties
	  are not specified. The specification does however require
	  that if a space reservation is given, then any access
	  latency or retention policy specified must match the same
	  properties of the space reservation.
	</para>
      </section>

      <section>
	<title>Directory functions</title>
	<para>
	  Starting from &srm; Version 2.2, interface provides a
	  complete set of the directory management functions. These
	  are srmLs, srmRm, srmMkDir, srmRmDir and srmMv.
	</para>
      </section>

      <section>
	<title>Permission functions </title>
	<para>
	  &srm; Version 2.2 support the following three space
	  permission functions, srmGetPermission, srmCheckPermission
	  and srmSetPermission. &dcache; contains a rudimentary
	  implementation of these functions that mostly allow setting
	  and checking of the Unix file permission.
	</para>
      </section>
    </section>

    <section>
      <title>The &cell-srm; service</title>

      <para>
	&dcache; &cell-srm; is implemented as a Web Service Interface running
	on a &jetty; application server and an
	<productname>Axis</productname> Web Services
	engine. The &jetty; server is executed as a cell,
	embedded in &dcache; and started automatically by the &cell-srm; service.
	Other cells started automatically by &cell-srm; are for instance
	&cell-spacemngr;, &cell-pinmngr; and &cell-remotegsitransfermngr;.
	Of these services only &cell-srm; and &cell-spacemngr; require special
	configuration.
      </para>
    </section>

    <section>
      <title>&dcache; specific concepts </title>

      <section>
	<title>Link Groups</title>

	<para>
	  &dcache; 1.8 &cell-poolmngr; supports new type of objects
	  called <firstterm>LinkGroups</firstterm>. Each link group corresponds to
	  a number of &dcache; pools in the following way: A LinkGroup is a
	  collection of Links, each of which is a collection of
	  the PoolGroups associated (Linked, hence a name <quote>Link</quote>) with
	  a set of the Pool Selection Units or PSUs. Each link group
	  knows about its available size, which is a sum of all
	  available sizes in all the pools included in this link
	  group. In addition link group has 5 boolean properties
	  called replicaAllowed, outputAllowed, custodialAllowed,
	  onlineAllowed and nearlineAllowed, the values of these
	  properties (true or false) can be configured in
	  <filename>PoolManager.conf</filename>.
         </para>
      </section>

      <section>
	<title>Space Reservations </title>

	<para>
	  In &dcache; 1.8 each &srm; Space Reservation is made against
	  the total available disk space of a particular link
	  group. The total space in &dcache; that can be reserved is the
	  sum of the available sizes of all Link Groups. If &dcache; is
	  configured correctly each byte of disk space, that can be
	  reserved, belongs to one and only one Link Group. Therefore
	  it is important to make sure that no pool belongs to more
	  than one pool group, no Pool Group belongs to more than one
	  Link and no Link belongs to more than one LinkGroup.
	</para>

	<para>
	  Files written into a space made within a particular link
	  group will end up on one of the pools referred to by this
	  link group.  The difference between the Link Group's
	  available space and the sum of all the current space
	  reservation sizes is the available space in the link group.
	</para>
      </section>

      <section>
	<title>Explicit and Implicit Space Reservations for Data Storage in &dcache; </title>

	<para>
	  In &dcache;, if a space reservation is specified, the file
	  will be stored in it (assuming the user has permission to do
	  so in the name space).
	</para>

	<para>
	  If the reservation token is not specified, and implicit
	  space reservation is enabled, then a space reservation will
	  be performed implicitly for each &srm; v1.1 and &srm; 2.2
	  srmPrepareToPut or srmCopy in pull mode. If an Access
	  Latency and a Retention Policy are specified, the user
	  defined retention policy and default access latency. If the
	  user has not specified Access Latency or Retention Policy
	  (or if &srm; v1.1 is used) , the system will attempt to
	  extract special tags (not surprisingly called
	  <quote>AccessLatency</quote> and <quote>RetentionPolicy</quote>) from PNFS namespace
	  from the directory to which file is being written. If the
	  tags are present, then their values will determine the
	  default Access Latency or Retention Policy that will be used
	  for implicit space reservations. If the tags are not
	  present, then system wide defaults will be used. If no
	  implicit space reservation can be made, the transfer will
	  fail. (Note: some clients also have default values, which
	  are used when not explicitly specified by the user. I this
	  case server side defaults will have no effect. )
	</para>

	<para>
	  If the implicit space reservation is not enabled in &dcache;
	  1.8 the pools in the link groups will be excluded from
	  consideration and only the remaining pools will be
	  considered to be the candidates for storing the incoming
	  data, and classical pool selection mechanism will be
	  used. If the space reservation is not used and no LinkGroups
	  are specified, the system behavior will be exactly the same
	  as in &dcache; 1.7.
	</para>
      </section>

      <section>
	<title>&cell-spacemngr; access control</title>

	<para>
	  When &srm; Space Reservation request is executed, its
	  parameters, such as reservation size, lifetime, access
	  latency and retention policy as well as user's Virtual
	  Organization (VO) membership information is forwarded to the
	  &srm; &cell-spacemngr;.
	</para>

	<para>
	  &cell-spacemngr; uses a special file for listing all the
	  Virtual Organizations (VOs) and all the VO Roles that are
	  permitted to make reservations in the given link group. List
	  of the allowed VOs and VO Roles, together with the total
	  available space and replicaAllowed, outputAllowed,
	  custodialAllowed, onlineAllowed and nearlineAllowed
	  properties of the group is than matched against the
	  information from the user request in order to determine if a
	  given space reservation can be made in particular link
	  group.  Once a space reservation is created, no access
	  control is performed, any user can attempt to store the
	  files in this space reservation, provided he or she knows
	  the exact space token.
	</para>
      </section>
    </section>
  </section>

  <!--
     ##############################################################
     #            Choosing The right hardware                     #
     ##############################################################
   -->

  <section id="cf-srm-hrd-os">
    <title>Choosing The right hardware and OS for the &srm; node</title>

    <section>
      <title>Hardware</title>

      <para>
	We recommend to install &dcache; &srm; server on a separate node
	with sufficient memory and a fast disk optimized for database
	application.  For example Fermilab US-CMS T1 site uses the
	following hardware for &srm; node.  Dual Intel Xeon Duo, 4 GB
	RAM, 3ware raid disk array.
      </para>
    </section>

    <section>
      <title>Operating System</title>

      <para>
	Latest Scientific Linux or RHEL would do.
      </para>

      <para>
	The kernel.shmmax=1073741824 and kernel.shmall=1073741824  kernel parameters
	should be set for a 4GB RAM Machine. This can be accomplished by running:
      </para>

      <screen>&prompt-root; <userinput>echo 'kernel.shmmax=1073741824' >>  /etc/sysctl.conf</userinput>
&prompt-root; <userinput>echo 'kernel.shmall=1073741824' >>  /etc/sysctl.conf</userinput>
&prompt-root; <userinput>/bin/sysctl -p</userinput></screen>

      <para>
	The exact content of US-CMS T1 &srm;
	<filename>sysctl.conf</filename> is:
      </para>

      <programlisting>kernel.core_uses_pid = 1
kernel.sysrq = 1
kernel.panic = 60
fs.file-max = 131072
net.ipv4.ip_forward = 0
vm.vfs_cache_pressure = 10000
# Keep this amount of memory free for emergency, IRQ and atomic allocations.
vm.min_free_kbytes = 65535
# Network tune parameters
net.ipv4.tcp_timestamps = 0
net.ipv4.tcp_sack = 0
net.ipv4.tcp_window_scaling = 1
kernel.shmmax=1073741824
kernel.shmall=1073741824</programlisting>

    </section>
  </section>

  <!--
     ##############################################################
     #            Configuring Postgres Database                   #
     ##############################################################
   -->

  <section id="cf-srm-psql">
    <title>Configuring the Postgres Database</title>

    <para>
      Install the latest &psql; database from <ulink
      url="http://www.postgresql.org/download/">PostgreSQL web
      site</ulink>.  While some like RPMs, others find that they have
      100% guarantee of compatibility of the software only if they
      build it locally from sources. In later case source rpms or
      archive of sources are available.
    </para>

    <para>
      We highly recommend to make sure that &psql; database files are
      stored on a separate disk that is not used for anything else
      (not even &psql; logging).  BNL Atlas Tier one observed a great
      improvement in srm-database communication performance after they
      deployed postgres on a separate dedicated machine.
    </para>

    <para>
      To provide seamless local access to the database please make the
      following modifications:
    </para>

    <para>
      The file <filename>/var/lib/pgsql/data/pg_hba.conf</filename>
      should contain the following lines
    </para>

    <programlisting>local   all         all                        trust
host    all         all         127.0.0.1/32   trust
host    all         all         ::1/128        trust</programlisting>

    <para>
      If &srm; or srm monitoring is going to be installed on a
      separate node, you need to add entry for this node as well:
    </para>

    <programlisting>host    all         all       <replaceable>monitoring node</replaceable>    trust
host    all         all       <replaceable>srm node</replaceable>    trust</programlisting>

    <para>
     The <filename>postgresql.conf</filename> should contain the following:
    </para>

    <programlisting>#to enable network connection on the default port
max_connections = 100
port = 5432
...
shared_buffers = 114688
...
work_mem = 10240
...
#to enable autovacuuming
stats_row_level = on
autovacuum = on
autovacuum_vacuum_threshold = 500  # min # of tuple updates before
                                   # vacuum
autovacuum_analyze_threshold = 250      # min # of tuple updates before
                                        # analyze
autovacuum_vacuum_scale_factor = 0.2    # fraction of rel size before
                                        # vacuum
autovacuum_analyze_scale_factor = 0.1   # fraction of rel size before
#
# setting vacuum_cost_delay might be useful to avoid
# autovacuum penalize general performance
# it is not set in US-CMS T1 at Fermilab
#
# In IN2P3 add_missing_from = on
# In Fermilab it is commented out

# - Free Space Map -
max_fsm_pages = 500000

# - Planner Cost Constants -
effective_cache_size = 16384            # typically 8KB each</programlisting>

     <para>
       To enable &dcache; &srm; components access to the database
       server with the user <database class='user'>srmdcache</database>:
     </para>

     <screen>&prompt-root; createuser -U postgres --no-superuser --no-createrole --createdb --pwprompt srmdcache</screen>

     <para>
       &srm; will use the database <database>dcache</database> for
       storing its state information:
     </para>

     <screen>&prompt-root; createdb -U srmdcache dcache</screen>
  </section>

  <!--
     ##############################################################
     #            Configuring SRM Domain                          #
     ##############################################################
   -->

  <section id="cf-srm-srm">
    <title>Configuring the &cell-srm; service</title>

    <para>
    Like other services, he &cell-srm; service can be enabled in the layout-file of
    your &dcache; installation. For an overview of the layout file
    format, please see <xref linkend="in-install-layout" />.
    </para>

    <informalexample>

    <para>
    For instance, to enable a &srm; door in a separate
    <replaceable>srm-${host.name}Domain</replaceable> in &dcache;,
    add the following lines to your layout file:
    </para>

    <programlisting>[<replaceable>srm-${host.name}Domain</replaceable>]
[<replaceable>srm-${host.name}Domain</replaceable>/srm]</programlisting>
    </informalexample>

    <section>
    <title>Important &cell-srm; configuration options</title>

      <para>
      The defaults for the configuration parameters for the &cell-srm; service can be found in
      <filename>/opt/d-cache/share/defauls/dcache.properties</filename>.

      If you want to modify parameters, copy them to
      <filename>/opt/d-cache/etc/dcache.conf</filename>
      and update their value.
      </para>

      <para>
        This section explains the most important &cell-srm; configuration parameters
        in detail.
      </para>

      <programlisting>srmVacuum=false
srmDbName=dcache
srmDbUser=srmdcache</programlisting>

      <para>
	Make sure that both <varname>srmCopyReqThreadPoolSize</varname> and
	<varname>remoteGsiftpMaxTransfers</varname> are set to the same values and the
	common value should be the roughly equal to the maximum number
	of the &srm; - to -&srm; copies your system can sustain.  So
	if you think about 3 gridftp transfer per pool and you have 30
	pools than the number should be 3x30=90.
      </para>

      <programlisting>srmCopyReqThreadPoolSize=90
remoteGsiftpMaxTransfers=90</programlisting>

      <para>
	Note US-CMS T1 has:
      </para>

      <programlisting>srmCopyReqThreadPoolSize=2000
remoteGsiftpMaxTransfers=2000</programlisting>

     <note>
        <para>
        &cell-srm; might produce a lot of log entries, especially if it run in
        debug mode. The need to run &cell-srm; in debug mode is greatly
        reduced if &srm; monitoring is installed. It is recommended to
        make sure that logs are redirected into a file on large
        disk.
        </para>
    </note>

    </section>

    </section>

  <!--
     ##############################################################
     #            SRM configuration for experts                   #
     ##############################################################
   -->

  <section id="cf-srm-expert-config">
    <title>&cell-srm; configuration for experts</title>

    <para>
      There are a few parameters in <filename>dcache.properties</filename>
      that you might find useful for nontrivial &cell-srm; deployment.
    </para>

    <section>
      <title>srmSpaceManagerEnabled</title>

      <para>
	<varname>srmSpaceManagerEnabled</varname> tells if the space
	management is activated in &cell-srm;.
      </para>

      <para>
	Possible values are <literal>yes</literal> and <literal>no</literal>. Default is <literal>yes</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmSpaceManagerEnabled=yes</programlisting>
    </section>

    <section>
      <title>srmImplicitSpaceManagerEnabled</title>

      <para>
	<varname>srmImplicitSpaceManagerEnabled</varname> tells if
	the space should be reserved for &srm; Version 1 transfers and
	for &srm; Version 2 transfers that have no space token
	specified. Will have effect only if srmSpaceManagerEnabled.
      </para>

      <para>
	Possible values are <literal>yes</literal> and
	<literal>no</literal>. This is enabled by default, disabled if
	<varname>srmSpaceManagerEnabled</varname> is set to
	<literal>no</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmImplicitSpaceManagerEnabled=yes</programlisting>
    </section>

    <section>
      <title>overwriteEnabled</title>

      <para>
	<varname>overwriteEnabled</varname> tells &srm; and
	&gridftp; servers if the overwrite is allowed. If enabled on
	the &srm; node, should be enabled on all &gridftp; nodes.
      </para>

      <para>
	Possible values are <literal>yes</literal> and
	<literal>no</literal>. Default is <literal>no</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>overwriteEnabled=yes</programlisting>
    </section>

    <section>
      <title>srmOverwriteByDefault</title>

      <para>
	<varname>srmOverwriteByDefault</varname> Set this to
	<literal>true</literal> if you want overwrite to be enabled
	for &srm; v1.1 interface as well as for &srm; v2.2 interface
	when client does not specify desired overwrite mode. This
	option will be considered only if
	<varname>overwriteEnabled</varname> is set to
	<literal>yes</literal>.
      </para>

      <para>
	Possible values are <literal>true</literal> and
	<literal>false</literal>. Default is <literal>false</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmOverwriteByDefault=false </programlisting>
    </section>

    <section>
      <title>srmDatabaseHost</title>

      <para>
	<varname>srmDatabaseHost</varname> tells &cell-srm; which
	database host to connect to. Do not change unless you know
	what you are doing.
      </para>

      <para>
	Default value is <literal>localhost</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmDatabaseHost=database-host.example.org</programlisting>
    </section>

    <section>
      <title>spaceManagerDatabaseHost</title>

      <para>
	<varname>spaceManagerDatabaseHost</varname> tells &cell-spacemngr;
	which database host to connect to. Do not change
	unless you know what you are doing.
      </para>

      <para>
	Default value is <literal>localhost</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>spaceManagerDatabaseHost=database-host.example.org</programlisting>
    </section>

    <section>
      <title>pinManagerDatabaseHost</title>
      <para>
	<varname>pinManagerDatabaseHost</varname> tells &cell-pinmngr;
    which database host to connect to. Do not change
	unless you know what you are doing.
      </para>

      <para>
	Default value is <literal>localhost</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>pinManagerDatabaseHost=database-host.example.org</programlisting>
    </section>

    <section>
      <title>srmDbName</title>

      <para>
	<varname>srmDbName</varname> tells &cell-srm; which database
	to connect to. Do not change unless you know what you are
	doing.
      </para>

      <para>
	Default value is  <literal>dcache</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmDbName=dcache</programlisting>
    </section>

    <section>
      <title>srmDbUser</title>

      <para>
	<varname>srmDbUser</varname> tells &cell-srm; which database
	user name to use when connecting to database. Do not change
	unless you know what you are doing.
      </para>

      <para>
	Default value is <literal>srmdcache</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmDbUser=srmdcache</programlisting>
    </section>

    <section>
      <title>srmDbPassword</title>

      <para>
	<varname>srmDbPassword</varname> tells &cell-srm; which database
	password to use when connecting to database. Do not change
	unless you know what you are doing.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmDbPassword=NotVerySecret</programlisting>
    </section>

    <section>
      <title>srmPasswordFile</title>

      <para>
	<varname>srmPasswordFile</varname> tells &cell-srm; which
	database password file to use when connecting to database.  Do
	not change unless you know what you are doing. It is
	recommended that MD5 authentication method is used. To learn
	about file format please see <ulink
	url="http://www.postgresql.org/docs/8.1/static/libpq-pgpass.html"/>. To
	learn more about authentication methods please visit <ulink
	url="http://www.postgresql.org/docs/8.1/static/encryption-options.html"/>,
	Please read "Encrypting Passwords Across A Network" section.
	<!-- TODO: better link? -->
      </para>

      <para>
	This option is not set by default.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmPasswordFile=/root/.pgpass</programlisting>
    </section>

    <section>
      <title>srmJdbcMonitoringLogEnabled</title>

      <para>
	<varname>srmJdbsMonitoringLogEnabled</varname> tells &cell-srm;
	to store the history of the &srm; request executions in the
	database. This option is useful if you are using SRMWatch web
	monitoring tool. Activation of this option might lead to the
	increase of the database activity, so if the &psql; load
	generated by &cell-srm; is excessive, disable it.
      </para>

      <para>
	Possible values are <literal>true</literal> and
	<literal>false</literal>. Default is <literal>false</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmJdbsMonitoringLogEnabled=false</programlisting>
    </section>

    <section>
      <title>srmDbLogEnabled</title>

      <para>
	<varname>srmDbLogEnabled</varname> tells &cell-srm; to store the
	information about the remote (copy, srmCopy) transfer details
	in the database. This option is useful if you are using
	SRMWatch web monitoring tool. Activation of this option might
	lead to the increase of the database activity, so if the
	&psql; load generated by &cell-srm; is excessive, disable it.
      </para>

      <para>
	Possible values are <literal>true</literal> and
	<literal>false</literal>. Default is <literal>false</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmDbLogEnabled=false</programlisting>
    </section>

    <section>
      <title>srmVersion</title>

      <para>
	<varname>srmVersion</varname> is not used by &cell-srm;; it was
	mentioned that this value is used by some publishing scripts.
      </para>

      <para>
	Default is <literal>version1</literal>.
      </para>
    </section>

    <section>
      <title>pnfsSrmPath</title>

      <para>
	<varname>pnfsSrmPath</varname> tells &cell-srm; what the root
	of all &srm; paths is in pnfs. &cell-srm; will prepend path to all
	the local &surl; paths passed to it by &srm; client. So if the
	<varname>pnfsSrmPath</varname> is set to
	<literal>/pnfs/fnal.gov/THISISTHEPNFSSRMPATH</literal> and
	someone requests the read of
	<uri>srm://srm.example.org:8443/file1</uri>, &srm; will
	translate the &surl; path <filename>/file1</filename> into
	<filename>/pnfs/fnal.gov/THISISTHEPNFSSRMPATH/file1</filename>. Setting
	this variable to something different from <literal>/</literal>
	is equivalent of performing Unix <command>chroot</command> for
	all &srm; operations.
      </para>

      <para>
	Default value is <literal>/</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>pnfsSrmPath="/pnfs/fnal.gov/data/experiment"</programlisting>
    </section>

    <section>
      <title>parallelStreams</title>

      <para>
	<varname>parallelStreams</varname> specifies the number of the
	parallel streams that &cell-srm; will use when performing third
	party transfers between this system and remote &gsiftp;
	servers, in response to &srm; v1.1 copy or &srm; V2.2 srmCopy
	function. This will have no effect on srmPrepareToPut and
	srmPrepareToGet command results and parameters of &gridftp;
	transfers driven by the &srm; clients.
      </para>

      <para>
	Default value is <literal>10</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>parallelStreams=20</programlisting>
    </section>

    <section>
      <title>srmBufferSize</title>

      <para>
	<varname>srmBufferSize</varname> specifies the number of bytes
	to use for the in memory buffers for performing third party
	transfers between this system and remote &gsiftp; servers, in
	response to &srm; v1.1 copy or &srm; V2.2 srmCopy
	function. This will have no effect on srmPrepareToPut and
	srmPrepareToGet command results and parameters of &gridftp;
	transfers driven by the &srm; clients.
      </para>

      <para>
	Default value is <literal>1048576</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmBufferSize=1048576</programlisting>
    </section>

    <section>
      <title>srmTcpBufferSize</title>

      <para>
	<varname>srmTcpBufferSize</varname> specifies the number of
	bytes to use for the tcp buffers for performing third party
	transfers between this system and remote &gsiftp; servers, in
	response to &srm; v1.1 copy or &srm; V2.2 srmCopy
	function. This will have no effect on srmPrepareToPut and
	srmPrepareToGet command results and parameters of &gridftp;
	transfers driven by the &srm; clients.
      </para>

      <para>
	Default value is <literal>1048576</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmTcpBufferSize=1048576</programlisting>
    </section>

    <section>
      <title>srmAuthzCacheLifetime</title>

      <para>
	<varname>srmAuthzCacheLifetime</varname> specifies the
	duration that authorizations will be cached. Caching decreases
	the volume of messages to the &cell-gplazma; cell or other
	authorization mechanism.  To turn off caching, set the value
	to <literal>0</literal>.
      </para>

      <para>
	Default value is <literal>120</literal>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmAuthzCacheLifetime=60</programlisting>
    </section>


    <section>
      <title>srmGetLifeTime, srmPutLifeTime and srmCopyLifeTime</title>

      <para>
	<varname>srmGetLifeTime</varname>,
	<varname>srmPutLifeTime</varname> and
	<varname>srmCopyLifeTime</varname> specify the lifetimes of
	the srmPrepareToGet (srmBringOnline) srmPrepareToPut and
	srmCopy requests lifetimes in millisecond. If the system is
	unable to fulfill the requests before the request lifetimes
	expire, the requests are automatically garbage collected.
      </para>

      <para>
	Default value is <literal>14400000</literal> (4 hours)
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmGetLifeTime=14400000
srmPutLifeTime=14400000
srmCopyLifeTime=14400000</programlisting>
    </section>

    <section>
      <title>srmGetReqMaxReadyRequests, srmPutReqMaxReadyRequests, srmGetReqReadyQueueSize and srmPutReqReadyQueueSize </title>

      <para>
	<varname>srmGetReqMaxReadyRequests</varname> and
	<varname>srmPutReqMaxReadyRequests</varname> specify the
	maximum number of the files for which the transfer &url;s will
	be computed and given to the users in response to &srm; get
	(srmPrepareToGet) and put (srmPrepareToPut) requests. The rest
	of the files that are ready to be transfered are put on the
	<literal>Ready</literal> queues, the maximum length of these
	queues are controlled by
	<varname>srmGetReqReadyQueueSize</varname> and
	<varname>srmPutReqReadyQueueSize</varname> parameters. These
	parameters should be set according to the capacity of the
	system, and are usually greater than the maximum number of the
	&gridftp; transfers that this &dcache; instance &gridftp;
	doors can sustain.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmGetReqReadyQueueSize=10000
srmGetReqMaxReadyRequests=2000
srmPutReqReadyQueueSize=10000
srmPutReqMaxReadyRequests=1000</programlisting>
    </section>

    <section>
      <title>srmCopyReqThreadPoolSize and remoteGsiftpMaxTransfers</title>

      <para>
	<varname>srmCopyReqThreadPoolSize</varname> and
	<varname>remoteGsiftpMaxTransfers</varname>.
	srmCopyReqThreadPoolSize is used to specify how many parallel
	srmCopy file copies to execute simultaneously. Once the
	&cell-srm; contacted the remote &srm; system, and obtained a Transfer
	&url; (usually &gsiftp; &url;), it contacts a Copy Manager
	module (usually &cell-remotegsitransfermngr;), and asks it to
	perform a &gridftp; transfer between the remote &gridftp; server
	and a &dcache; pool. The maximum number of simultaneous
	transfers that &cell-remotegsitransfermngr; will support is
	<varname>remoteGsiftpMaxTransfers</varname>, therefore it is important that
	<varname>remoteGsiftpMaxTransfers</varname> is greater than or equal to
	<varname>srmCopyReqThreadPoolSize</varname>.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmCopyReqThreadPoolSize=250
remoteGsiftpMaxTransfers=260</programlisting>
    </section>

    <section>
      <title>srmCustomGetHostByAddr</title>

      <para>
	<varname>srmCustomGetHostByAddr</varname>
	srmCustomGetHostByAddr enables using the BNL developed
	procedure for host by IP resolution if standard InetAddress
	method failed.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>srmCustomGetHostByAddr=true</programlisting>
    </section>

    <section>
      <title>RecursiveDirectoryCreation</title>

      <para>
	<varname>RecursiveDirectoryCreation</varname> allows or
	disallows automatic creation of directories via &srm;,
	allow=true, disallow=false.
      </para>

      <para>
	Automatic directory creation is allowed by default.
      </para>

      <para>
	Usage example:
      </para>

      <programlisting>RecursiveDirectoryCreation=true</programlisting>
    </section>

  </section>

  <!--
     ##############################################################
     #            SRM Space Manager configuration                 #
     ##############################################################
   -->

  <section id="cf-srm-space">
    <title>&cell-spacemngr; configuration</title>

    <section>
      <title>&srm; &cell-spacemngr; and LinkGroups</title>

      <para>
	&cell-spacemngr; is making reservations agains space in
	<firstterm>LinkGroups</firstterm>, LinkGroup is an object created by the
	&cell-poolmngr;, that consists of several Links. The total
	space available in the given LinkGroup is a sum of available
	spaces in all links. The available space in each link is a sum
	of the available spaces in all pools assinged to a given
	link. Therefore for the space reservation to work correctly it
	is essential that each pool belongs to one and only one link,
	and each link belongs to only one LinkGroup. LinkGroups are
	assigned several parameters that determine what kind of space
	the LinkGroup correspond to and who can make reservation
	against this space.
      </para>
    </section>

    <section>
      <title>Definition of the LinkGroups in the PoolManager.conf</title>

      <para>
	To configure &cell-poolmngr; to create the new LinkGroup (a new
	reservable entity in &dcache;), please use following example
	(given in the &cell-poolmngr;). Here we assume that write-link
	link already exists:
      </para>

      <screen>&dc-prompt-pm; <userinput>psu create linkGroup write-link-group</userinput>
&dc-prompt-pm; <userinput>psu addto linkGroup  write-link-group write-link</userinput></screen>

      <para>
	To tell &cell-spacemngr; if the LinkGroup will be able to store
	files with given AccessLatency and RetentionPolicy,
  LinkGroups have 5 attributes: <varname>custodialAllowed</varname>,
  <varname>outputAllowed</varname>, <varname>replicaAllowed</varname>,
  <varname>onlineAllowed</varname> and <varname>nearlineAllowed</varname>.
  These attributes can be specified with the following commands:
      </para>

      <screen>&dc-prompt-pm; <userinput>psu set linkGroup custodialAllowed &lt;group name&gt; &lt;true|false&gt;</userinput>
&dc-prompt-pm; <userinput>psu set linkGroup outputAllowed &lt;group name&gt; &lt;true|false&gt;</userinput>
&dc-prompt-pm; <userinput>psu set linkGroup replicaAllowed &lt;group name&gt; &lt;true|false&gt;</userinput>
&dc-prompt-pm; <userinput>psu set linkGroup onlineAllowed &lt;group name&gt; &lt;true|false&gt;</userinput>
&dc-prompt-pm; <userinput>psu set linkGroup nearlineAllowed &lt;group name&gt; &lt;true|false&gt;</userinput></screen>

       <para>
	 Please note that that it is up to administrators that the
	 link groups attributes are specified correctly. For example
	 dcache will not complain if the LinkGroup that does not
	 support tape backend will be declared as one that supports
	 custodial.
      </para>
    </section>

    <section>
      <title>Activating &srm; &cell-spacemngr;</title>

      <para>
	In order to enable the new space reservation: add (uncomment)
	the following definition in <filename>dcache.conf</filename>
      </para>

      <programlisting>srmSpaceManagerEnabled=yes</programlisting>
    </section>

    <section id="cf-srm-spaceman-params">
      <title>&srm; &cell-spacemngr; parameters in <filename>dcache.conf</filename></title>

      <section>
	<title>SpaceManagerDefaultRetentionPolicy</title>

	<para>
	  If space reservation request does not specify a retention
	  policy we will assign
	  <varname>SpaceManagerDefaultRetentionPolicy</varname>
	  retention policy by default.
	</para>

	<para>
	  Possible values are <literal>REPLICA</literal>,
	  <literal>OUTPUT</literal> and <literal>CUSTODIAL</literal>.
	</para>

	<para>
	  Usage example:
	</para>

	<programlisting>SpaceManagerDefaultRetentionPolicy=CUSTODIAL</programlisting>
      </section>

      <section>
	<title>SpaceManagerDefaultAccessLatency</title>

	<para>
	  If a space reservation request does not specify an access latency
	  we will assign
	  <varname>SpaceManagerDefaultAccessLatency</varname> this
	  access latency by default.
	</para>

	<para>
	  Possible values are <literal>ONLINE</literal> and
	  <literal>NEARLINE</literal>.
	</para>

	<para>
	  Usage example:
	</para>
	<programlisting>SpaceManagerDefaultAccessLatency=NEARLINE</programlisting>
      </section>

      <section>
	<title>SpaceManagerReserveSpaceForNonSRMTransfers</title>
	<para>
	  If
	  <varname>SpaceManagerReserveSpaceForNonSRMTransfers</varname>
	  is set to <literal>true</literal>, and if the transfer
	  request comes from the door, and there was no prior space
	  reservation made for this file, &cell-spacemngr; will try to
	  reserve space before satisfying the request.
	</para>

	<para>
	  Possible values are <literal>true</literal> and
	  <literal>false</literal>.
	</para>

	<para>
	  Usage example:
	</para>

	<programlisting>SpaceManagerReserveSpaceForNonSRMTransfers=false</programlisting>
      </section>

      <section>
	<title>SpaceManagerLinkGroupAuthorizationFileName</title>

	<para>
	  <varname>SpaceManagerLinkGroupAuthorizationFileName</varname>
	  specifies a file that contains the list of FQANs that are
	  allowed to make space reservations in a given link
	  group. The file syntax is described in the next section.
	</para>

	<para>
	  This parameter is not set by default.
	</para>

	<para>
	  Usage example:
	</para>

	<programlisting>SpaceManagerLinkGroupAuthorizationFileName=/opt/d-cache/etc/LinkGroupAuthorization.conf</programlisting>

      </section>

    </section>

   <section>
     <title>Implicit Space Reservations</title>

      <para>
	As it was described in <xref linkend="cf-srm-intro"/>,
	&dcache; can perform implicit space reservations for &srm;
	Version 1 data transfers and for &srm; Version 2.2 data
	transfers that are not given the space token explicitly. The
	parameter that enables this behavior is
	<varname>srmImplicitSpaceManagerEnabled</varname>, which is
	described in <xref linkend="cf-srm-expert-config"/>. In case
	of &srm; version 1.1 data transfers, when the Access Latency
	and Retention Policy cannot be specified, and in case of &srm;
	V2.2 clients, when Access Latency and Retention Policy are not
	specified, the default values will be used. First &srm; will
	attempt to use the values of <literal>AccessLatency</literal>
	and <literal>RetentionPolicy</literal> tags from the directory
	to which a file is being written. If the tags are present,
	then the Access Latency and Retention Policy will be set on
	basis of the system wide defaults, which are controlled by
	<varname>SpaceManagerDefaultRetentionPolicy</varname> and
	<varname>SpaceManagerDefaultAccessLatency</varname> variables
	in <filename>dcache.conf</filename>; these variable are described in details in
    <xref linkend="cf-srm-spaceman-params" />.
      </para>

      <para>
	If you have a direct access to the namespace, you can check if
	the <varname>AccessLatency</varname> and <varname>RetentionPolicy</varname>
	tags are present by using the following commands:
      </para>

      <screen>&prompt-root; <userinput>cd <replaceable>pnfsDir</replaceable></userinput>
&prompt-root; <userinput>cat ".(tags)()"</userinput>
.(tag)(OSMTemplate)
.(tag)(file_family)
.(tag)(storage_group)
.(tag)(AccessLatency)
.(tag)(RetentionPolicy)</screen>

      <para>
	If the output contains the lines saying
	<literal>(tag)(AccessLatency)</literal> and
	<literal>.(tag)(RetentionPolicy)</literal> than the tags are
	already present and you can get the actual values of these
	tags by executing the following commands, which are shown
	together with example outputs:
      </para>

      <screen>&prompt-root; <userinput>cat ".(tag)(AccessLatency)" ONLINE</userinput>
&prompt-root; <userinput>cat ".(tag)(RetentionPolicy)" CUSTODIAL</userinput></screen>

      <para>
	To create/change the values of the tags, please execute :
      </para>

      <screen>&prompt-root; <userinput>echo <replaceable>"New Access Latency"</replaceable> > ".(tag)(AccessLatency)"</userinput>
&prompt-root; <userinput>echo  <replaceable>"New Retention Policy"</replaceable> > ".(tag)(RetentionPolicy)"</userinput></screen>

      <para>
	The valid <literal>AccessLatency</literal> values are
	<literal>ONLINE</literal> and <literal>NEARLINE</literal>,
	valid <literal>RetentionLatency</literal> values are
	<literal>REPLICA</literal>, <literal>OUTPUT</literal> and
	<literal>CUSTODIAL</literal>.
      </para>

      <para>
	Below are the reals examples of these commands:
      </para>

      <screen>&prompt-root; <userinput>echo "ONLINE" > ".(tag)(AccessLatency)"</userinput>
&prompt-root; <userinput>echo "REPLICA" > ".(tag)(RetentionPolicy)"</userinput></screen>
   </section>

 </section>


  <!--
     ##############################################################
     #            SRM Space Manager VO bases Access control      #
     ##############################################################
   -->

    <section id="cf-srm-space-ac">
    <title>&srm; &cell-spacemngr; Virtual Organization based access control configuration</title>

    <section>
      <title>VO based Authorization Prerequisites</title>

      <para>
	In order to be able to take advantage of the Virtual
	Organization (VO) infrastructure and VO based authorization
	and VO Based Access Control to the Space in &dcache;, certain
	things need to be in place:
      </para>

      <itemizedlist>
	<listitem>
	  <para>
	    User needs to be registered with the VO.
	  </para>
	</listitem>

	<listitem>
	  <para>
	    User needs to use <command>voms-proxy-init</command> to
	    create a vo proxy.
	  </para>
	</listitem>

	<listitem>
	  <para>
	    &dcache; needs to use &cell-gplazma; and not
	    &cell-gplazma; with <literal>dcache.kpwd</literal> plugin,
	    but other modules that know how to extract VO attributes
	    from the proxy. (see <xref linkend="cf-gplazma"/>, have a
	    look at <literal>gplazmalite-vorole-mapping</literal>
	    plugin.
	  </para>
	</listitem>
      </itemizedlist>

      <para>
	Only if these 3 conditions are satisfied the VO based
	authorization of the &cell-spacemngr; can work.
      </para>

      <para>
	If a client uses a regular grid proxy, created with
	<command>grid-proxy-init</command>, and not a Virtual
	Organization (VO) proxy, which is created with the
	<command>voms-proxy-init</command>, when he is communicating
	with &srm; server in &dcache;, then the VO attributes can not
	be extracted its
	credential. <command>voms-proxy-init</command> adds a Fully
	Qualified Attribute Name (FQAN) section(s) to the grid proxy,
	which contains information about the user's VO membership, in
	particular it contains the VO Group name and the VO Role that the
	client intends to play at this time. In this case the name of
	the user is extracted on basis of the direct Distinguished
	Name (DN) to use name mapping. For the purposes of the space
	reservation the name of the user is used as its VO Group name,
	and the VO Role is left empty.
      </para>
    </section>

    <section>
      <title>VO based Access Control configuration</title>

      <para>
	&dcache; Space Reservation Functionality Access Control is
	currently performed at the level of the LinkGroups. Access
	to making reservations in each LinkGroup is controlled by the
	<varname>SpaceManagerLinkGroupAuthorizationFile</varname>
	property.
      </para>

      <section>
	<title>SpaceManagerLinkGroupAuthorizationFile syntax </title>

	<para>
	  The file described by
	  <varname>SpaceManagerLinkGroupAuthorizationFile</varname>
	  has following syntax:
	</para>

	<para>
	  LinkGroup Name followed by the list of the Fully Qualified
	  Attribute Names (FQANs), each FQAN on separate line,
	  followed by an empty line, which is used as a record
	  separator, or by the end of file. FQAN is usually a string
	  of the form &lt;VO&gt;/Role=&lt;VORole&gt;. Both &lt;VO&gt;
	  and &lt;VORole&gt; could be set to <literal>*</literal>, in
	  this case all VOs or VO Roles will be allowed to make
	  reservations in this LinkGroup. Any line that starts with #
	  is a comment and may appear anywhere.
	</para>

	<para>
	  The file location is specified by defining
	</para>

	<programlisting>SpaceManagerLinkGroupAuthorizationFileName=<replaceable>FILENAME</replaceable></programlisting>

	<para>
	  in <filename>dcache.conf</filename>.
	</para>

      </section>

      <section>
	<title>Example of the SpaceManagerLinkGroupAuthorizationFile </title>

	<programlisting># this is comment and is ignored

LinkGroup LFSOnly-LinkGroup
/atlas/Role=/atlas/role1

LinkGroup CMS-LinkGroup
/cms/Role=*
#/dteam/Role=/tester

LinkGroup default-LinkGroup
#allow anyone :-)
*/Role=*
#/dteam/Role=/tester</programlisting>

        <para>
	  Successful VO and Experiment specific examples of &dcache;
	  &srm; &cell-spacemngr; configurations are or will be published
	  at <ulink
	  url="http://trac.dcache.org/trac.cgi/wiki/manuals/index">
	  &dcache; WIKI documentation pages </ulink>.
	</para>
      </section>

    </section>

    </section>



  <!--
     ##############################################################
     #            SRMWatch, SRM Monitoring Tool                   #
     ##############################################################
   -->

   <section id="cf-srm-monitor">
    <title>SRMWatch, &srm; Monitoring Tool</title>

    <para>
      For large sites in order to avoid interference from Tomcat
      activities related to web interface, we recommend installation
      of &srm; monitoring on a separate node.
    </para>

    <section>
      <title>Separate Node Installation</title>

      <itemizedlist>
	<listitem>
	  <para>
	    Install JDK1.5
	  </para>
	</listitem>

	<listitem>
	  <para>
	    Download, install and start latest tomcat 5.5 from
	    <ulink url="http://tomcat.apache.org/"> Tomcat Web Site
	    </ulink>
	  </para>
	</listitem>

	<listitem>
	  <para>
	    Download srmwatch RPM from <ulink
	    url="http://www.dcache.org"/>.
	  </para>
	</listitem>

	<listitem>
	  <para>
	    Install RPM. Installation can be performed using this
	    command:
	  </para>

	  <screen>&prompt-root; <userinput>rpm -Uvh srmwatch-1.0-0.i386.rpm</userinput></screen>
	</listitem>

	<listitem>
	  <para>
	    Edit configuration file
	    <filename>/opt/d-cache/srmwatch-1.0/WEB-INF/web.xml</filename>
	    in the line saying:
	  </para>

	  <programlisting>&lt;param-value&gt;jdbc:postgresql://localhost/dcache&lt;/param-value&gt;</programlisting>

	  <para>
	    Make sure that the localhost is in jdbc url substitutes
	    with &srm; database host name. For example:
	  </para>

	  <programlisting>&lt;param-value&gt;jdbc:postgresql://fledgling06.fnal.gov/dcache&lt;/param-value&gt;</programlisting>
	</listitem>

	<listitem>
	  <para>
	    Execute
	  </para>

	  <screen>&prompt-root; <userinput>export CATALINA_HOME=<replaceable>YOUR_TOMCAT_LOCATION</replaceable></userinput></screen>
	</listitem>

	<listitem>
	  <para>
	    Execute
	  </para>

	  <screen>&prompt-root; <userinput>/opt/d-cache/srmwatch-1.0/deploy_srmwatch</userinput></screen>
	</listitem>

	<listitem>
	  <para>
	    &srm; Monitoring page should be visible at
	    http://<replaceable>srm-monitoring-node</replaceable>:8080/srmwatch/
	  </para>
	</listitem>
      </itemizedlist>
    </section>


    <section>
      <title>Same Node Installation</title>

      <itemizedlist>
	<listitem>
	  <para>
	    Download srmwatch rpm from <ulink
	    url="http://www.dcache.org>"/>.
	  </para>
	</listitem>

	<listitem>
	  <para>
	    Install rpm after srm server is installed and running,
	    with <filename>/opt/d-cache/etc/srm_setup.env</filename>
	    containing before the installation
	  </para>

	  <programlisting>TOMCAT_HTTP_ENABLED=true</programlisting>

	  <para>
	    RPM installation can be performed using this command:
	  </para>

	  <screen>&prompt-root; <userinput>rpm -Uvh srmwatch-1.0-0.i386.rpm</userinput></screen>
	</listitem>

	<listitem>
	  <para>
	    &srm; Monitoring page should be visible at
	    http://<replaceable>srmnode</replaceable>:8080/srmwatch/
	  </para>
	</listitem>
      </itemizedlist>
    </section>
   </section>
</chapter>
<!-- </namespacewrapper> -->

