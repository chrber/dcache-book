<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN" "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd">

<!-- <namespacewrapper xmlns:xi="http://www.w3.org/2001/XInclude"> -->
<chapter id="cf-srm">
  
  
  <title>Configuring the Storage Resource Manager Component</title>
  <partauthors>Dmitry Litvintsev, Timur Perelmutov, Vladimir Podstavkov</partauthors>
  <!--
  <para>(We assume a basic knowledge of the functionality and terminology of
  dCache. Pool, domain, cell, how to log into a cell, and execute
  commands)</para>
  -->

  <para><firstterm>Storage Resource Managers</firstterm> (SRMs) are 
  middleware components whose 
  function is to provide dynamic space allocation and file management on 
  shared storage components on the Grid. SRMs support protocol negotiation 
  and a reliable replication mechanism. The SRM specification standardizes 
  the interface, thus allowing for a uniform access to heterogeneous storage 
  elements.</para>

  <para>SRM interface consists of the five categories of functions: Space 
  Management, Data Transfer, Request Status, Directory and Permission 
  Functions. SRM interface utilizes Grid 
  Security Infrastructure (GSI) for authentications. SRM service is a Web 
  Service implementation of a published WSDL document.</para>

  <para>dCache SRM is implemented as Web Service Interface running under 
  Apache Tomcat application server and Axis Web Services engine. This service
  starts a dCache <literal> SRM </literal>domain with a main SRM cell and a 
  number of other cells srm service relies on. These are SrmSpaceManager, 
  PinManager, RemoteGsiftpCopyManager, etc.  Of these serivices only 
  SrmSpaceManager requires special configuration.</para>


  <section id="cf-srm-hrd-os">
    <title>Choosing The right hardware and OS for the SRM node</title>
	<section>
	    <title> Hardware </title>
        <para>We recommend to install dCache SRM server on a separate node with 
	    sufficient memory and a fast disk optimized for database application. 
         For example Fermilab US-CMS T1 site uses the following hardware for SRM node.
        Dual Intel Xeon Duo, 4 GB RAM, 3ware raid disk array.
        </para>
    </section>
    <section>
      <title>Operating System</title>

      <para>Latest Scientific Linux  or RHEL would do.</para>
	  <para>
	  The kernel.shmmax=1073741824 and kernel.shmall=1073741824  kernel parameters 
	  should be set for a 4GB RAM Machine. This can be accomplished by running:
	  
	  <screen>
	  <rootprompt/><command>echo</command> 'kernel.shmmax=1073741824' >>  /etc/sysctl.conf
	  </screen>
	  <screen>
	  <rootprompt/><command>echo</command> 'kernel.shmall=1073741824' >>  /etc/sysctl.conf
	  </screen>
	  <screen>
	  <rootprompt/><command>/bin/sysctl</command> -p
	  </screen>

	  
	  The exact content of US-CMS T1 SRM sysctl.conf is:
	  <programlisting>
	  kernel.core_uses_pid = 1
	  kernel.sysrq = 1
	  kernel.panic = 60
	  fs.file-max = 131072
	  net.ipv4.ip_forward = 0
	  vm.vfs_cache_pressure = 10000
	  # Keep this amount of memory free for emergency, IRQ and atomic allocations.
	  vm.min_free_kbytes = 65535
	  # Network tune parameters
	  net.ipv4.tcp_timestamps = 0
	  net.ipv4.tcp_sack = 0
	  net.ipv4.tcp_window_scaling = 1
	  kernel.shmmax=1073741824
	  kernel.shmall=1073741824
	  </programlisting>
	  </para>
    </section>
 </section>
 
  <section id="cf-srm-psql">
    <title>Configuring Postgres Database</title>
    <para>
    Install the latest  postgres database from 
    <ulink url="http://www.postgresql.org/download/"> PostgreSQL web site </ulink>. 
    While some like rpms, others find that they have 100% guarantee of compatibility 
    of the software only if they build it locally from sources. In later case source 
    rpms or archive of sources are available. 
    </para>
    <para> We highly recommend to make sure that postgres database files are stored 
    on a separate disk that is not used for anything else (not even postgres logging). 
    BNL Atlas Tier one observed a great improvement in srm-database communication 
    performance after they deployed postgres on a separate dedicated machine.
    </para>
    <para>
     To provide seamless local access to the database please make the following modifications:
     
     The file /var/lib/pgsql/data/pg_hba.conf should contain 
      <programlisting>
	...
	local   all         all                        trust
	host    all         all         127.0.0.1/32   trust
	host    all         all         ::1/128        trust
     </programlisting>
     If SRM  or srm monitoring is going to be installed on a separate node, you need to add 
     entry for this node as well:
      <programlisting>
        host    all         all       <replaceable>monitoring node</replaceable>    trust
        host    all         all       <replaceable>srm node</replaceable>    trust
     </programlisting>
     
     The postgresql.conf should contain the following :
      <programlisting>
	#to enable network connection on the default port
	max_connections = 100
	port = 5432
	...
	shared_buffers = 114688
	...
	work_mem = 10240
	...
	#to enable autovacuuming
	stats_row_level = on
		autovacuum = on
		autovacuum_vacuum_threshold = 500       # min # of tuple updates before
	                                        # vacuum
	autovacuum_analyze_threshold = 250      # min # of tuple updates before
        	                                # analyze
	autovacuum_vacuum_scale_factor = 0.2    # fraction of rel size before
	                                        # vacuum
	autovacuum_analyze_scale_factor = 0.1   # fraction of rel size before
	# 
	# setting vacuum_cost_delay might be useful to avoid
	# autovacuum penalize general performance
	# it is not set in US-CMS T1 at Fermilab
	#
	# In IN2P3 add_missing_from = on 
	# In Fermilab it is commented out
	
	# - Free Space Map -
	max_fsm_pages = 500000
	
	In the postgresql.conf file
	
	# - Planner Cost Constants -
	
	effective_cache_size = 16384            # typically 8KB each
	.......................
     </programlisting>
     </para>
     <para>
     To enable dCache SRM components access to the database server with the user "srmdcache": 
     <screen>
	  <rootprompt/>createuser -U postgres --no-superuser --no-createrole --createdb --pwprompt srmdcache
     </screen>	  
     SRM will use the database "dcache" for storing its state information: 
     <screen>
	  <rootprompt/> createdb -U srmdcache dcache
     </screen>	  
     </para>
     </section>  
     
     <section id="cf-srm-srm">
        <title>Configuring SRM Domain</title>
	<para>
	Once database and  and jvm are installed and database is running, you may install dCache SRM. 
	</para>
        <section >
         <title>Install dCache server.rpm</title>
	 <para>
	  <screen>
	  <rootprompt/> <command>rpm</command> -Uvh <replaceable>dcache.server.rpm</replaceable>
	  </screen>
	  </para>
	 
        </section>
	<section >
         <title> node_config </title>
	 <para>
	 copy /pt/d-cache/etc/node_config.template into /opt/d-cache/etc/node_config
	 </para>
	 <para>
	 edit /opt/d-cache/etc/node_config
NODE_TYPE=custom
...
SRM=yes
...
# all other parameters should be turned off on "srm only" node
         </para>
        </section>
        <section >
         <title>srm_setup.env</title> 
	 <para>
	 edit /opt/d-cache/etc/srm_setup.env
	<itemizedlist>
	<listitem>
         <para>Make sure that JAVA_HOME is set to correct value, for example
         <programlisting>
         JAVA_HOME=/usr/java/jdk1.5.0_07
         </programlisting>
	 </para>
	</listitem>
	<listitem>
         <para> Tomcat port does not interfere with with services that are 
	 already using network
         <programlisting>
          TOMCAT_PORT=8080
         </programlisting>
         </para>
	</listitem>
	<listitem>
         <para>If you are going to run the monitoring on the same node:
        <programlisting>
        TOMCAT_HTTP_ENABLED=true
       JAVA_OPTS="-Xmx512m -Djava.awt.headless=true"
        </programlisting>
         </para>	
	</listitem>
	 </itemizedlist>
         </para>
        </section>
        <section >
         <title>install dCacheSetup </title>
	 <para>
	 copy /opt/d-cache/etc/dCacheSetup.template into /opt/d-cache/config/dCacheSetup 
	 and edit it so  that, serviceLocatorHost and serviceLocatorPort point to 
	 central dcache node:
         <programlisting>
	  serviceLocatorHost=<replaceable>host of central node</replaceable>
	  serviceLocatorPort=<replaceable>11111</replaceable>
          </programlisting>
	</para>
	<para>
        following SRM parameters should be configured as following:
      <programlisting>
	srmVacuum=false
	srmDbName=dcache
	srmDbUser=srmdcache
      </programlisting>
       </para>
       <para>
          Make sure that both srmCopyReqThreadPoolSize and remoteGsiftpMaxTransfers 
	  are set to the same values and the common value should be the roughly equal 
	  to the maximum number of the SRM - to -SRM copies your system can sustain. 
	  So if you think about 3 gridftp transfer per pool and you have 30 pools than 
	  the number should be 3x30=90.
      <programlisting>
       srmCopyReqThreadPoolSize=90
       remoteGsiftpMaxTransfers=90
      </programlisting>
        Note US-CMS T1 has:
     <programlisting>
        srmCopyReqThreadPoolSize=2000
        remoteGsiftpMaxTransfers=2000 
      </programlisting>

           </para>
        </section>
        <section >
         <title>tomcat/axis deployment </title>
	 <para>
	   run 
          <screen>
	  <rootprompt/> <command>/opt/d-cache/install/install.sh </command>
          </screen>
	 </para>
        </section>
        <section >
         <title> Starting and stopping SRM domain </title>
	 <para>
	 run 
	<screen>
	  <rootprompt/> <command>/opt/d-cache/bin/dcache-core</command> start
          </screen>
	 to start srm domain
	 </para>
	 <para>
	 run 
	<screen>
	  <rootprompt/> <command>/opt/d-cache/bin/dcache-core</command> stop
          </screen>
	 to stop srm domain
	 </para>
        </section>
        <section >
         <title>SRM Logs</title>
	   <para>srm might produce a lot of logs, especially if it run in debug mode.
	    Need to run SRM in debug mode is greatly reduced if SRM monitoring is 
	    installed. It is recommended to make sure that logs are redirected into a 
	    file on large disk. dCache SRM 1.7 logs into 
	    /opt/d-cache/libexec/apache-tomcat-5.5.20/logs/catalina.out.
	    </para>
       </section>
   </section>  
   
   <section id="cf-srm-monitor">
    <title>SRMWatch, SRM Monitoring Tool</title>
    <para>
    For large sites in order to avoid interference from Tomcat activities related to 
    web interface, we recommend installation of srm monitoring on a separate node.
    </para>
      <section>
        <title> Separate Node Installation</title>
	<para>
	<itemizedlist>
	<listitem>
	<para>
 	 Install JDK1.5
	</para>
	</listitem>
	<listitem>
	<para>
	  Download, install and start latest tomcat 5.5 from http://tomcat.apache.org/
	</para>
	</listitem>
	<listitem>
  	<para>
       Download srmwatch rpm from http://www.dcache.org.
	</para>
	</listitem>
	<listitem>
	<para>
          Install rpm. Installation can be performed using this command:
         <screen>
	  <rootprompt/> <command>rpm</command> -Uvh srmwatch-1.0-0.i386.rpm
	 </screen>
	</para>
	</listitem>
	<listitem>
	<para>
          Edit configuration file /opt/d-cache/srmwatch-1.0/WEB-INF/web.xml
           in the line saying:
          <programlisting>
             &lt;param-value&gt;jdbc:postgresql://localhost/dcache&lt;/param-value&gt;
          </programlisting>

           Make sure that the localhost is in jdbc url substitutes with srm 
	   database host name. For example:
          <programlisting>
           &lt;param-value&gt;jdbc:postgresql://fledgling06.fnal.gov/dcache&lt;/param-value&gt;
          </programlisting>
	</para>
	</listitem>
	<listitem>
	<para>
          execute
        <screen>
	  <rootprompt/> <command>export</command> CATALINA_HOME=<replaceable>YOUR_TOMCAT_LOCATION</replaceable>
        </screen>
	</para>
	</listitem>
	<listitem>
	<para>
            execute
       <screen>
	  <rootprompt/> <command>/opt/d-cache/srmwatch-1.0/deploy_srmwatch</command>
       </screen>
	</para>
	</listitem>
	<listitem>
	<para>
           Srm Monitoring page should be visible at 
            http://<replaceable>srm-monitoring-node</replaceable>:8080/srmwatch/
	</para>
	</listitem>
     	</itemizedlist>
 
	</para>
      </section>
      <section>
        <title> Same Node Installation</title>
	<para>
	<itemizedlist>
	<listitem>
	<para>
          download srmwatch rpm from http://www.dcache.org.
	</para>
	</listitem>
	<listitem>
	<para>
         Install rpm after srm server is installed and running, with 
          /opt/d-cache/etc/srm_setup.env containing before the installation
     <programlisting>
         TOMCAT_HTTP_ENABLED=true
     </programlisting>
   	rpm installation can be performed using this command:
         <screen>
	  <rootprompt/> <command>rpm</command> -Uvh srmwatch-1.0-0.i386.rpm
	 </screen>
	</para>
	</listitem>
	<listitem>
	<para>
          Srm Monitoring page should be visible at 
	  http://<replaceable>srmnode</replaceable>:8080/srmwatch/
	</para>
	</listitem>
	</itemizedlist>
       </para>
      </section>
    </section>
    
   <section id="cf-srm-space">
    <title>SRM Space Manager configuration</title>
    <section >
    <title>SRM Space Manager and LinkGroups</title>
    <para>
      Space Manager is making reservations agains space in LinkGroups, LinkGroup 
      is an object created by the PoolManager, that consists of several "Links". 
      The total space available in the given LinkGroup is a sum of available 
      spaces in all links.An available space in each link is a sum of  the 
      available spaces in all pools assinged to a given link. Therefore for the 
      space reservation to work correctly it is essential that each pool belongs to 
      one and only one link, and each link belongs to only one LinkGroup.
      LinkGroups are assigned several parameters that determine what kind of 
      space the LinkGroup correspond to and who can make reservation against this
      space. The main parameter is HSM, If its value is "none" then the space in this
      LinkGroup is considered non-Custodial, and reservations of space  with "Replica"
      and "Output" RetentionPolicy can reserve space in it. If HSM parameter is assinged 
      any other value, this value is considered to be the name of the backend tape 
      storage system and  the LinkGroup is then considered Custodial. Only space 
      reservations with "Custodial" RetentionPolicy will be able to reserve space in such 
      LinkGroup. 
    </para>  
    <para>
       AccessLatency parameter of the space reservation does not influence the the 
       LinkGroup selection for a particular reservation.
    </para>
    </section>
    <section>
    <title> Definition of the LinkGroups in the PoolManager.conf</title>
    <para>
     To configure PoolManager to create the new LinkGroup (a new reservable entity in 
     dCache), please use following example (given in the PoolManager). 
     Here we assume that write-link link already exists:
     <programlisting>
	psu create linkGroup write-link-group
	psu addto linkGroup  write-link-group write-link
     </programlisting>
     </para>
     <para>
      To tell Space Manager if the LinkGroup will be able to store the Custodial files, 
      we need to specify the Hsm Type property of the LinkGroup. If the Hsm Type is "None" 
      then Custodial files can not go into the link Group, anything else is enterpreted as 
      tape backend and allows Custodial file. For example to specify the Hsm Type to be 
      "enstore" add following command:
     <programlisting>
         psu set linkGroup attribute -r write-link-group HSM=enstore
     </programlisting>
     </para>
     <para>
      If no Vo information is assigned to the LinkGroup, any Vo group can make reservation 
      please see the following examples of how to assign particular VOs and VO Groups to a 
      given link: 1) assign VOs to a LinkGroup. While PoolManager does not have a ability 
      to specify VO and Role it's given to it as attributes of the linkGroup.
     <programlisting>
	psu set linkGroup attribute write-link-group VO=alice
	psu set linkGroup attribute write-link-group VO=cms
	psu set linkGroup attribute write-link-group VO=atlas
     </programlisting>
     </para>
     <para>
       To add Roles for a VO see the following example. Here syntax is important. The role 
       attribute name is constructed from + "Role".
     <programlisting>
	psu set linkGroup attribute write-link-group cmsRole=/cms/NULL/production
	psu set linkGroup attribute write-link-group cmsRole=/cms/NULL/mc
     </programlisting>
     </para>
     <para>
       Ff no vos or roles are excluded, "*" can used for both vo and role. If the vo groups 
       and roles are not specified at all, then every user of dCache is allowed to make 
       reservations.
     </para>
    </section>
    <section >
    <title>Activating SRM Space Manager</title>
    <para>
     In order to enable the new space reservation: add (uncomment) the following 
     definition in dCacheSetup
     <programlisting>
       srmSpaceManagerEnabled=yes
     </programlisting>
    </para>
     </section>
   </section>
   

</chapter>
<!-- </namespacewrapper> -->
