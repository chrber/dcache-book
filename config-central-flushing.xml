<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN" "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd">     

<chapter id="cf-flushing">

  <title>Central Flushing to tertiary storage systems</title>
  <partauthors>Patrick Fuhrmann</partauthors>
  <!--
    I solemly swear that i'm not at all good.
  -->
  <para>
  This chapter is of interest for dCache instances connected to a tertiary 
  storage system or making use of the mass storage interface for any other reason.
    <warning>
       <para>
          The central flush control is still in the evaluation phase. 
          The configuration description within this chapter is mainly for
          the dCache team to get it running on their test systems. The final
          prodution version will have most of this stuff already be configured.
       </para>
    </warning>

  dCache instances, connected to tertiary storage systems, collect incoming
  data, sort it by storage class and flush it as soon as certain thresholds
  are reached. All this is done autonomously by each individual write pool. 
  Consequently
  those flush operations are coordinated on the level of a pool but not 
  globally wrt a set of write pools or even to the whole dCache instance.
  Experiences during the last years show, that for various purposes a 
  global flush management would be desirable.
  </para>
     <blockquote>
     <title>Separation of read/write operations on write pools</title>
     <para>
       The total thoughput of various disk storage systems tend to drop
       significantly
       if extensive read and write operations have to be performed
       in parallel on datasets exceeding the filesystem caches. To overcome
       this technical obstacle, it would be good if disk storage systems would 
       either allow writing into a pool or flushing data out of a pool
       into the HSM system, but never both at the same time.
     </para>
     </blockquote>

     <blockquote>
     <title>Overcoming HSM limitations and restictions</title>
     <para>
        Some HSM systems, mainly those not coming with their own
        scheduler, apply certain restrictions on the number of
        requests being accepted simultaniously. For those,
        a central flush control system would allow for limiting
        the number of requests or the number of storage classes
        being flushed at the same time. 
     </para>
  
     </blockquote>
  <section id="cf-flushing-configuration">
  <title>Basic configuration (Getting it to run)</title>
    <para>
       This section describes how to setup a central flush control
       manager. With the production release 1.7.0 most of those
       steps will become obsolete.
       <itemizedlist>
       <listitem>
       <para>
         Whitin the <cellname>PoolManager</cellname>, a pool-group 
         (<replaceable>flushPoolGroup</replaceable>) has 
         to be created and populated with
         pools planned to be controlled by the central flush mechanism.
         An arbitrary number of flush control managers may run within the
         same dCache instance as long as each can work on its own
         pool-group and no pool is member of more than one <replaceable>flushPoolGroup</replaceable>.
         
       </para>
       </listitem>
       <listitem>
       <para>
         To start the flush control system, an corresponding 
         dCache batch file has to be setup, installed and started.
         As input parameter, the HsmFlushControl cell needs the name
         of the <replaceable>flushPoolGroup</replaceable>) and the name of
         the driver, controlling the flush behaviour. Within the same
         batch file more than one flush control manager may be started
         as long as they get different cell-names and different pool-groups
         assigned.
       </para>
       </listitem>
       <listitem>
       <para>
         The flush control web pages  have to be defined in the <filename>httpd.batch</filename>.
       </para>
       </listitem>
       </itemizedlist>
    </para>
     <section>
     <title>Creating the flush pool group</title>
       <para>
       Creating <replaceable>flushPoolGroup</replaceable> and adding pools is done within the
       <filename>config/PoolManager.config</filename> setup file or using
       the <cellname>PoolManager</cellname> command line 
       interface. Pools may be member of other pool-groups, as long as those
       pool-groups are not managed by other flush control managers.
       <programlisting>
psu create pool <replaceable>pool-1</replaceable>
psu create pool <replaceable>...</replaceable>
#
psu create pgroup <replaceable>flushPoolGroup</replaceable>
#
psu addto pgroup <replaceable>flushPoolGroup</replaceable>  <replaceable>pool-1</replaceable>
psu addto pgroup <replaceable>flushPoolGroup</replaceable>  <replaceable>...</replaceable>
#
       </programlisting>
       </para>
     </section>
     <section>
     <title>Creating and activating the hsmcontrol batch file</title>
       <para>
       <programlisting>
#
set printout default errors
set printout CellGlue none
#
onerror shutdown
#
check -strong setupFile
#
copy file:${setupFile} context:setupContext
#
import context -c setupContext
#
check -strong serviceLocatorHost serviceLocatorPort
#
create dmg.cells.services.RoutingManager  RoutingMgr
#
create dmg.cells.services.LocationManager lm \
     "${serviceLocatorHost} ${serviceLocatorPort}"
#
create diskCacheV111.hsmControl.flush.HsmFlushControlManager <replaceable>FlushManagerName</replaceable>  \
        "<replaceable>flushPoolGroup</replaceable>  \
         -export   -replyObject \
         -scheduler=<replaceable>SchedulerName</replaceable>  \
         <replaceable>Scheduler specific options</replaceable> \
        "
#
       </programlisting>
       Which the following meaning of the variables :
       <itemizedlist>
         <listitem>
            <para>
               <replaceable>flushPoolGroup</replaceable> needs to be the name of the pool group
               defined in the PoolManager.conf files.
            </para>
         </listitem>
         <listitem>
            <para>
               <replaceable>SchedulerName</replaceable> is the name of a class implementing
               the <literal>diskCacheV111.hsmControl.flush.HsmFlushSchedulable</literal> interface.
            </para>
         </listitem>
         <listitem>
            <para>
               <replaceable>Scheduler specific options</replaceable> may be options specific
               to the selected scheduler.
            </para>
         </listitem>
       </itemizedlist>
       Initially there are three schedulers available :
       <itemizedlist>
         <listitem>
           <para>
             <literal>diskCacheV111.hsmControl.flush.driver.HandlerExample</literal> may be used as
             an example implementation of the HsmFlushScheduler interface. The functionality is 
             useless in an production environment but can be useful to check the functionality of
             the central flush framework. If one allows this drive to take over control it will
             initiate the flushing of data as soon as it becomes aware of it. One the other hand
             it supports a mode where is doesn't do anything except preventing the individual 
             pools from doing the flush autonomously. In that mode, the driver assumes the flushes
             to be steered manually by the flush web pages decribed in the next paragraph. 
             The latter mode is enabled by
             starting the flush driver with the <replaceable>Scheduler specific options</replaceable>
             set to <literal>-do-nothing</literal>
           </para>
         </listitem>
         <listitem>
           <para>
             <literal>diskCacheV111.hsmControl.flush.driver.AlternateFlush</literal> is intended to
             provide suffient functionality to cope with issues described in the introduction of the
             paragraph. Still quite some code and knowledge has to go into this driver.
           </para>
         </listitem>
         <listitem>
           <para>
             <literal>diskCacheV111.hsmControl.flush.driver.AlternatingFlushSchedulerV1</literal> is certainly
             the most useful driver. It can be configured to flush all pools on a single machine
             simultaniously. It is trigger by space consumption, number of files within a pool
             or the time the oldest file resides on a pool without having been flushed.
             Please checkout the next section for details on configuration and usage.
           </para>
         </listitem>
       </itemizedlist> 
       </para>
     </section>
     <section>
     <title>The AlternatingFlushSchedulerV1 driver</title>
       <para>
       This is an alternating driver as well, which means that it either allows data to flow into
       a pool, or data going from a pool onto an HSM system  but never both at the same time. 
       Data transfers from pools to other pools or from pools to clients are not
       controlled by this driver. In order to minimize the latter one should configure
       HSM write pools to not allow transfers to clients but doing pool to pool transfers
       first.
       </para>
       <section>
       <title>Configuration</title>
       <para>
       <programlisting>
#
create diskCacheV111.hsmControl.flush.HsmFlushControlManager <replaceable>FlushManagerName</replaceable>  \
        "<replaceable>flushPoolGroup</replaceable>  \
         -export   -replyObject \
         -scheduler=diskCacheV111.hsmControl.flush.driver.AlternatingFlushSchedulerV1  \
         -driver-config-file=${config}/<replaceable>flushDriverConfigFile</replaceable> \
        "
#</programlisting>
        Where <replaceable>flushPoolGroup</replaceable> is a PoolGroup defined in the 
        <filename>PoolManager.conf</filename> file, containing all pools which are intended to
        be managed by this FlushManager. <replaceable>flushDriverConfigFile</replaceable>
        is a file within the dCache <filename>config</filename> directory holding 
        property values for this driver. The driver reloads the file whenever it changes
        its modification time. One should allow for a minute of two before new setting
        are getting activated. The configuration file has to contain key value pairs, separated
        by the = sign. Keys, not corresponding to a driver property are silently ignored.
        Properties, not set in the configuration file, are set to some reasonable default value. 
       </para>
       </section>       
       <section>
       <title>Properties</title>
       <para>
         <table>
          <title>Driver Properties</title>
          <tgroup cols="3" align="left">
            <colspec colnum="1" colname="Property Name" colwidth="50"/>
            <colspec colnum="2" colname="Default Value"/>
            <colspec colnum="3" colname="Meaning" colwidth="100"/>
            <thead>
              <row>
                <entry>Property Name</entry>
                <entry>Default Value</entry>
                <entry>Meaning</entry>
              </row>
            </thead>
            <tbody>
              <row>
                <entry>max.files</entry><entry>500</entry>
                <entry>Collect this number of files per pool, before flushing</entry>
              </row><row>
                <entry>max.minutes</entry><entry>120</entry>
                <entry>Collect data for this amount of minutes before flushing</entry>
              </row><row>
                <entry>max.megabytes</entry><entry>500 * 1024</entry>
                <entry>Collecto this number of megabytes per pool before flushing</entry>
              </row><row>
                <entry>max.rdonly.fraction</entry><entry>0.5</entry>
                <entry>Do not allow more than this percentage of pools to be set read only</entry>
              </row><row>
                <entry>flush.atonce</entry><entry>0</entry>
                <entry>Never flush more than that in one junk</entry>
              </row><row>
                <entry>timer</entry><entry>60</entry>
                <entry>Interval timer (minimum resolution)</entry>
              </row><row>
                <entry>print.events</entry><entry>false</entry>
                <entry>Print events delivered by the FlushManager</entry>
              </row><row>
                <entry>print.rules</entry><entry>false</entry>
                <entry>Print remarks from the rule engine</entry>
              </row><row>
                <entry>print.poolset.progress</entry><entry>false</entry>
                <entry>Print progress messages</entry>
              </row>
            </tbody>
           </tgroup>
          </table> 
          </para>
       </section>
       <section>
       <title>The process</title>
         <para>
         A pool is becoming a flush candidate if either the 
         number of files collected exceeds <literal>max.files</literal> or the number of
         megabytes collected exceeds <literal>max.megabytes</literal> or the oldest file,
         not flushed yet, is becoming older than <literal>max.minutes</literal>.         

         Pool Candidates are sorted according to a metric, which is essentially the sum of three items.
         The number of files devided by <literal>max.files</literal>, the number of megabytes devided
         by <literal>max.megabytes</literal> and the age of the oldest file devided by <literal>max.minutes</literal>.

         The pool with the highest metric is chosen first. The driver determines the hardware unit, this pools
         resides on. The intention is to flush all pools of this unit simulatationsly. Depending on the
         configuration, the unit can be either a disk partition or the host. After the hard unit is determined, the
         driver adds the number of pools on that unit to the number of pools already in rdOnly mode.
         If the sum exceeds the total number of pools in the flush pool group, multiplied by the
         <literal>max.rdonly.fraction</literal> property, the pool is NOT selected. The process proceeds
         until a pool, resp. a hardware unit complies with these contrains.
         
         The hardware unit, a pool belongs to, is set by the 'tag.hostname' in the 
         <filename>config/<replaceable>hostname</replaceable></filename> file.
         </para>
     </section>
     </section>
     <section>
     <title>Setting up and using the flush control web pages.</title>
       <para>
       In order to keep track on the flush activities the flush control
       web pages need to be activated. Add the following lines somewhere between the
       <command>define context httpdSetup endDefine</command> and the
       <command>endDefine</command> command in the <filename>/opt/d-cache/config/httpd.batch</filename>
       file.
       
       <programlisting>
set alias flushManager class diskCacheV111.hsmControl.flush.HttpHsmFlushMgrEngineV1 mgr=<replaceable>FlushManagerName</replaceable>
       </programlisting>
       Additional flush managers may just be added to this command, separated by commas.
       After restarting the 'httpd' service, the flush control pages are available at
       <literal>http://<replaceable>headnode</replaceable>:2288/flushManager/mgr/*</literal>.
       </para>
       <para>
       The flush control web page is split into 5 parts. The top part is a switchboard, pointing
       to the different flush control managers installed (listed in the mgr= option of the
       <command>set alias flushManager</command> in the <filename>config/httpd.config</filename>).
       The top menu is followed by a <literal>reload</literal> link. Its important to use this
       link instead of the 'browsers' reload button. The actual page consists of tree tables.
       The top one presents common configuration information. Initially this is the name of the
       flush cell, the name of the driver and whether the flush controller has actually taken over
       control or not. Two action buttons allow to switch between centrally and locally controlled
       flushing. The second table lists all pools managed by this controller. Information is provided
       on the pool mode (readonly vers. readwrite), the number of flushing storage classes, the total 
       size of the pool and the  amount of precious space per pool. Action buttons allow to 
       toggle individual pools between <literal>ReadOnly</literal> and <literal>ReadWrite</literal> mode.
       Finally the third table presents all storage classes currently holding data to be flushed.
       Per storage class and pool, characteristic properties are listed, like total size, precious size,
       active and pending files. Here as well, an action button allows to flush individual 
       storage classes on individual pools.
       <warning>
         <para>
         The possibilty to interactively interact with the flush manager needs to be supported
         by the driver choosen. 
         </para>
       </warning>  
       </para>
     </section>
  </section>
  
  <section id="cf-flushing-examples">
     <title>Examples</title>
     
     <para>
      
     </para>
     
     <section>
        <title>xxx</title>
     
      <para>
      </para>    
     </section>
     
     <section>
        <title>Choosing 'random pool selection' for incoming trafic only</title>
     
      <para>

      </para>    
     </section>
  </section>
  
 </chapter>
