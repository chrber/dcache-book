<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % sharedents SYSTEM "shared-entities.xml" >
%sharedents;
]>

<chapter id="cf-hsm">

  <title>The Interface to a Tertiary Storage System</title>

  <chapterinfo>
    <author>
      <firstname>Patrick</firstname>
      <surname>Fuhrmann</surname>
    </author>
  </chapterinfo>

  <para>
    &dcache; installations, used as a frontend to tertiary storage
    system, need, at some point, to exchange data this such a system
    in order to store new, precious files and to retrieve files from
    the &hsm; if not yet, or no longer, available on one of the
    &dcache; pools. Unfortunately there is no well defined interface
    for such &hsm; operations. So the &dcache; overcomes this problem
    by calling configurable (&dcache; external) shell scripts or
    binaries whenever an &hsm; store or retrieve operation becomes
    necessary. The local &hsm; administrator is responsible for
    providing this procedure and to make it available and known to the
    &dcache;.  This small writeup defines the way &dcache; will call
    such an external method.
  </para>

  <note>
    <para>
      Most &dcache; installations do not use an &hsm; backend
      system. However, if you have an &hsm; backend system please
      check in your layout file in <filename
      class='directory'>/opt/d-cache/etc/layouts</filename> and in the
      <filename>/opt/d-cache/etc/dcache.conf</filename> file that the
      option <literal>lfs=precious</literal> is not specified.
    </para>
  </note>

  <section id="cf-hsm-define">
    <title>Defining the &hsm; interface.</title>

    <para>
      Each individual pool, which is expected to exchange data with an
      &hsm;, has to define a &dcache; external method to flush/fetch
      datasets into/from one or more connected &hsm;'s.  The command
      decribed below has either to be given in the command line
      interface of the corresponding pool while the pool is active
      (don't forget so <quote>save</quote>) or may be added to the
      pool setup file commands prior to starting the pool.
    </para>
    
  <screen>     Syntax : hsm set &lt;hsmName&gt; -command=&lt;fullPathToExternalCommand&gt;
     
     Example :
         
	     hsm set osm -command=/usr/d-cache/jobs/osm-hsmcp.sh
	 or
	     hsm set enstore -command=/usr/d-cache-deployment/jobs/real-enstore.sh</screen>

    <para>
      The external method, which might be a shell script or a binary,
      is called by the &dcache; with a set of positional arguments
      (see below). In addition, options may be specified which are
      appended to the regular argument list on calling the external
      method.
    </para>

    <screen>     Syntax : hsm set &lt;hsmName&gt; -&lt;key&gt;=&lt;value&gt;
     
     Example :
         
	     hsm set osm -command=/usr/d-cache/jobs/osm-hsmcp.sh
	     hsm set osm -pnfs=/pnfs/desy.de -somethingElse=true</screen>

    <para>
      This will result in excuting the following command line whenever
      a file has to be exchanged with an &hsm;.
    </para>

    <screen>/usr/d-cache/jobs/osm-hsmcp.sh put|get &lt;pnfsId&gt; &lt;LocalFilename&gt; \
        -si=&lt;See Below&gt; \
        -pnfs=/pnfs/desy.de   \
        -somethingElse=true</screen>
  </section>


  <section id="cf-hsm-calling">
    <title>Calling sequence</title>
    
    <para>
      The external script or binary is launed with 3 positional
      arguments and at least one option
      (<literal>-si=&lt;storageInfo&gt;</literal>).  Additonial
      options may follow if defined so with the <emphasis>hsm
      set</emphasis> pool command. Arguments and options are separated
      by at least on blank character.
    </para>

      <screen>    Syntax :
    
        &lt;binary&gt; put|get &lt;pnfsid&gt; &lt;localFileName&gt;   \
               -si=&lt;storageInfo&gt; [more options]</screen>

    <para>
      The <emphasis>put|get</emphasis> argument determines the data
      transfer direction seen from the &hsm;. <emphasis>put</emphasis>
      means, that data has to be stored into the &hsm; while
      <emphasis>get</emphasis> means it has be fetched out of the
      &hsm;.
    </para>

    <para>
      The &lt;storageInfo&gt; option is a collection of key value
      pairs, separated by semicola. All these values are derived from
      the pnfs database. The possible keys slightly differ, depending
      on which &hsm; is addressed. The order of the key value pairs is
      not determined and may vary between calls. The
      <literal>-si=</literal> string shouldn't contain blank TAB or
      newline characters.
    </para>

    <para>
      Example:
    </para>

    <screen>    -si=size=1048576000;new=true;stored=false;sClass=desy:cms-sc3;cClass=-;hsm=osm;Host=desy;</screen>

    <table>
      <title>Mandatory StorageInfo keys</title>

      <tgroup cols="2" align="left">
	<colspec colnum="1" colname="Key" colwidth="50"/>
	<colspec colnum="2" colname="Meaning" colwidth="*"/>
	<thead>
	  <row>
	    <entry>Key</entry>
	    <entry>Meaning</entry>
	  </row>
	</thead>
	<tbody>
	  <row>
	    <entry>size</entry>
	    <entry>
	      Size of the file in bytes
	    </entry>
	  </row>
	  <row>
	    <entry>new</entry>
	    <entry>&false; if file already in the &dcache;
	    </entry>
	  </row>
	  <row>
	    <entry>stored</entry>
	    <entry>&true; if file already stored in the &hsm;
	    </entry>
	  </row>
	  <row>
	    <entry>sClass</entry>
	    <entry>
	      &hsm; depended. Used by the &cell-poolmngr; for pool
	      attraction
	    </entry>
	  </row>
	  <row>
	    <entry>cClass</entry>
	    <entry>
	      Parent Director tag (<literal>cacheClass</literal>). Used by the
	      &cell-poolmngr; for poolattraction. May be '-'
	    </entry>
	  </row>
	  <row>
	    <entry>hsm</entry>
	    <entry>
	      Storage Manager name (enstore/osm). Can be overwritten
	      by parent directory tag (<literal>hsmType</literal>).
	    </entry>
	  </row>
	</tbody>
      </tgroup>
    </table>

    <table>
      <title>Optional StorageInfo keys but used by all &hsm;'s</title>

      <tgroup cols="2" align="left">
	<colspec colnum="1" colname="Key" colwidth="50"/>
	<colspec colnum="2" colname="Meaning" colwidth="*"/>
	<thead>
	  <row>
	    <entry>Key</entry>
	    <entry>Meaning</entry>
	  </row>
	</thead>
	<tbody>
	  <row><entry>flag-l</entry><entry>Size of the file (if size exceeds 2G)</entry></row>
	  <row><entry>flag-s</entry><entry><literal>*</literal> if file is defined sticky</entry></row>
	  <row><entry>flag-c</entry><entry>CRC value (currently <literal>1:&lt;hexAdler32&gt;</literal></entry></row>
	</tbody>
      </tgroup>
    </table>

    <table>
      <title>Enstore specific</title>

      <tgroup cols="2" align="left">
	<colspec colnum="1" colname="Key" colwidth="50"/>
	<colspec colnum="2" colname="Meaning" colwidth="*"/>
	<thead>
	  <row>
	    <entry>Key</entry>
	    <entry>Meaning</entry>
	  </row>
	</thead>
	<tbody>
	  <row><entry>group</entry><entry>Storage Group (e.g. cdf,cms ...)</entry></row>
	  <row><entry>family</entry><entry>File family (e.g. sgi2test,h6nxl8, ...)</entry></row>
	  <row><entry>bfid</entry><entry>Bitfile Id (GET only) (e.g. B0MS105746894100000)</entry></row>
	  <row><entry>volume</entry><entry>Tape Volume (GET only) (e.g. IA6912)</entry></row>
	  <row><entry>location</entry><entry>Location on tape (GET only) (e.g. : 0000_000000000_0000117)</entry></row>
	</tbody>
      </tgroup>
    </table>
    
    <table>
      <title>OSM specific</title>

      <tgroup cols="2" align="left">
	<colspec colnum="1" colname="Key" colwidth="50"/>
	<colspec colnum="2" colname="Meaning" colwidth="*"/>
	<thead>
	  <row>
	    <entry>Key</entry>
	    <entry>Meaning</entry>
	  </row>
	</thead>
	<tbody>
	  <row><entry>store</entry><entry>OSM store (e.g. zeus,h1, ...)</entry></row>
	  <row><entry>group</entry><entry>OSM Storage Group (e.g. h1raw99, ...)</entry></row>
	  <row><entry>bfid</entry><entry>Bitfile Id (GET only) (e.g. 000451243.2542452542.25424524)</entry></row>
	</tbody>
      </tgroup>
    </table>

    <para>
      There might be more key values pairs which are used by the
      &dcache; internally and which should not affect the behaviour of
      the hsm copy script.
    </para>

    <table>
      <title>Return codes</title>

      <tgroup cols="4" align="left">
	<colspec colnum="1" colname="ReturnCode" colwidth="70"/>
	<colspec colnum="2" colname="Meaning" colwidth="130"/>
	<colspec colnum="3" colname="IntoHSM" colwidth="100"/>
	<colspec colnum="4" colname="FromHSM" colwidth="*"/>
	<thead>
	  <row><entry morerows="1">Return Code</entry>
	    <entry morerows="1">Meaning</entry>
	    <entry namest="IntoHSM" nameend="FromHSM">Pool Behaviour</entry></row>
	  <row><entry>Into HSM</entry><entry>From HSM</entry></row>
	</thead>
	<tbody>
	  <row><entry align="center">30 &lt;= rc &lt; 40</entry>
	    <entry align="center">User defined</entry>
	    <entry align="center">Deactivates request</entry>
	    <entry align="center">Reports Problem to PoolManager</entry>
	  </row>
	  <row><entry align="center">41</entry>
	    <entry align="center">No Space Left on device</entry>
	    <entry align="center" morerows="2"><para>Pool Retries</para></entry>
	    <entry align="center" morerows="2"><para>Disables Pool</para><para>Reports Problem to PoolManager</para></entry>
	  </row>
	  <row><entry align="center">42</entry>
	    <entry align="center">Disk Read I/O Error</entry>
	  </row>
	  <row><entry align="center">43</entry>
	    <entry align="center">Disk Write I/O Error</entry>
	  </row>
	  <row><entry align="center">All other</entry>
	    <entry>&nbsp;</entry>
	    <entry align="center">Pool Retries</entry>
	    <entry align="center">Reports Problem to PoolManager</entry>
	  </row>
	</tbody>
      </tgroup>
    </table>

    <section>
      <title>Special Cases and exceptions</title>

      <section>
	<title>Reading vers. Writing &hsm; files</title>

	<para>
	  When fetching a file from an &hsm;, the command line
	  contains sufficient information about the location of the
	  dataset within the &hsm; to get the file. No additional
	  interaction with &pnfs; is needed.  So &pnfs; doesn't need
	  to be mounted on read-only pools.
	</para>

	<para>
	  This is different for storing files into an &hsm;. As a
	  return from the actual &hsm; put operation, some data has to
	  be stored in &pnfs;. Currently this has to be done directly
	  by the corresponding external &hsm; script. So, other then
	  for read pools, write pools still need to have &pnfs;
	  mounted.
	</para>

	<para>
	  A future approache will be to transfer the necessay &hsm;
	  information from the &hsm; copy script into the &dcache;
	  using STDOUT. The &dcache; subsequently performs the
	  necessary &pnfs; store operation through the
	  &cell-pnfsmngr;.
	</para>

      </section>

      <section>
	<title>Precious files are removed from &pnfs;</title>

	<para>
	  In case a precious file is removed from &pnfs;
	  <emphasis>before</emphasis> the hsmcopy-Script
	  (<filename>osmcp.sh</filename> or
	  <filename>real-encp.sh</filename>) is called, the copy on
	  disk is removed and the hsmcopy-Script is not called.
	</para>
	
	<para>
	  If the file is removed while the hsmcopy-Script is active,
	  the script will encounter an error when writing &hsm; data
	  into the various &pnfs; layers.  In this case it's
	  recommended to return an error code in the 30&ndash;39 range
	  to have the request deactivated. So manual intervention is
	  needed to get the situation cleaned up but no attempt is
	  made by the &dcache; to get the corresponding dataset stored
	  into the &hsm; again.
	</para>

      </section>
    </section>
  </section>

  <section id="cf-hsm-rmfiles">
    <title>Removing files from an backend &hsm;, triggered by &dcache;</title>
    
    <para>
      Whenever a file entry is removed from pnfs (the &dcache;
      namespace), &dcache; takes care that all copies of this file are
      removed from the various pools. In case, &dcache; is attached to
      one or more tertiary storage systems, it provides an interface
      to allow removing the file from those external systems as well.
    </para>

    <para>
      As soon as a file entry is removed from &pnfs;, a new file is
      created within a special, so called, trash directory. (For
      details, see next paragraph) The name of this newly created file
      is identical to its inode, resp. pnfsId. A pnfsId is an internal
      unique identifier for each file within the &dcache;. PnfsIds
      don't change if files are renamed and pnfsIds are never
      reused. The content of the this (new) file is exactly the
      information written into <option>level 1</option> during the
      &hsm; store prodecure, discussed in the sections above. This
      information is usually sufficient to get the file removed from
      the backend &hsm; storage system.
    </para>

    <para>
      The <option>trash</option> directory is a local directory
      residing on the head node, or to be more precise, on the server
      node where pnfs/chimera is running. In regular &dcache;
      installations, the directory is
      <filename>/opt/pnfsdb/pnfs/trash/1</filename>.  After the
      installation of pnfs, only the path section
      <filename>/opt/pnfsdb/pnfs/trash</filename> exists. In order to
      activate the signaling on pnfs file removes, a subdirectory
      named <filename>1</filename> has to be created within
      <filename>/opt/pnfsdb/pnfs/trash</filename>.  From that point in
      time, this directory is populated with file entries for each
      file removed from pnfs/chimera. The mechanism, taking this file
      information and doing the appropriate &hsm; specific actions, is
      resposible for removing those entries if no longer needed,
      resp. if the file has been removed from the backend &hsm;.
    </para>

    <para>
      For exotic &dcache; installations, the entry
      <option>trash=</option> in
      <filename>/usr/etc/pnfsSetup</filename> points to the
      <option>trash</option> directory within the local filesystem.
    </para>
  </section>

	<!--
  <pre>
        The storage info options is a collection of        
    get 0000000000000000000034F8 
        /home/patrick/demo/d-cache-1.4.3/databases/pools/elchtop-0/data/0000000000000000000034F8 
        -si=size=500000;
	    new=false;
	    stored=true;
	    sClass=demo:users;
	    cClass=-;
	    hsm=osm;
	    alloc-size=500000;
	    onerror=default;
	    timeout=30;
	    flag-c=1:a1890001;
	    uid=500;
	    store=demo;
	    group=users;
	    bfid=bfid.0000000000000000000034F8;
        -waitTime=300 
	-hsmBase=/home/patrick/demo/d-cache-1.4.3/databases/pools/hsm 
	-pnfs=/pnfs/dcache.org 
	-command=/home/patrick/demo/d-cache-1.4.3/jobs/hsmcp.sh


   put 0001000000000000002A3C70  
       /export/write-pool-1/data/0001000000000000002A3C70 
       -si=size=836873;
           new=true;
           stored=false;
           sClass=cdf.sgi2test;
           cClass=-;
           hsm=enstore;
           alloc-size=836873;
           onerror=default;
           timeout=-1;
           uid=5744;
           ;
           path=<Unknown>;
           group=cdf;
           family=sgi2test;
           bfid=<Unknown>;
           volume=<unknown>;
           location=<unknown>;
        -pnfs=/pnfs/fs 
        -command=/var/enstore/dcache-deploy/scripts/real-encp.sh

   get 00020000000000000062A7C0  
       /export/read-pool-1/data/00020000000000000062A7C0 
       -si=size=436920618;
           new=false;
           stored=true;
           sClass=cdf.h6nxl8;
           cClass=-;
           hsm=enstore;
           uid=8637;
           location=;
           ;
           path=<Unknown>;
           group=cdf;
           family=h6nxl8;
           bfid=B0MS105746894100000;
           volume=IA6912;
           location=0000_000000000_0000117;
       -pnfs=/pnfs/fs 
       -command=/var/enstore/dcache-deploy/scripts/real-encp.sh


    put 000000000000000000001680 
        /home/patrick/demo/d-cache-1.4.3/databases/pools/elchtop-0/data/000000000000000000001680
        -si=size=50000;
           new=true;
           stored=false;
           sClass=demo:users;
           cClass=-;
           hsm=osm;
           alloc-size=50000;
           onerror=default;
           timeout=-1;
           StorageGroup=-#0;
           uid=500;
           StoreName=demo;
           store=demo;
           group=users;
           bfid=<Unknown>;
        -hsmBase=/home/patrick/demo/d-cache-1.4.3/databases/pools/hsm 
        -pnfs=/pnfs/dcache.org 
        -command=/home/patrick/demo/d-cache-1.4.3/jobs/hsmcp.sh

 
  </pre>
  </blockquote>
  -->
  
</chapter>
