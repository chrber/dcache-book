<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN" "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd">

<chapter id="in">
  <!-- <?dbhtml dir="install"?> -->
  <title>Installation Manual</title>
  
  <partauthors>Michael Ernst, Patrick Fuhrmann, Mathias de Riese</partauthors>
  
  <para>
    The first section describes the installation of a fresh <dcache/>
    instance using RPM files downloaded from <ulink
    url="http://www.dcache.org">the <dcache/> homepage</ulink>. It
    follows a guide to upgrading an existing installation. In both
    cases we assume standard requirements of a small to medium sized
    <dcache/> instance without an attached <glossterm
    linkend="gl-tss">tertiary storage system</glossterm>. The third
    section contains some pointers on extended features.
  </para>
  
  <section id="in-install">
    <title>Installation of a Basic <dcache/> Instance</title>

    <para>
      In the following the installation of a central admin node of a
      <dcache/> instance and of an arbitrary number of <dcache/> nodes
      will be described. These nodes may each contain several
      <glossterm linkend="gl-pool"><dcache/> pools</glossterm> and
      optionally one <srm/>, one GridFTP door, and/or one GSIdCap
      door. On the admin node, a <pnfs/> server and several central
      <dcache/> components are installed. The <pnfs/> server, some
      central components, and each <srm/> need an PostgreSQL server
      installed locally on the node. The first section describes the
      configuration of a PostgreSQL server. After that the
      installation of the <pnfs/> server and of the <dcache/>
      components will follow. During the whole installation process
      root access is required.
    </para>

    <section>
      <title>Prerequisites</title>
      
	<para>
 	
	In order to install dCache the following requirements must be met:

	<itemizedlist>
	<listitem>
	<para>
	A RPM-based Linux distribution is required.
	</para>
	</listitem>
	<listitem>
	<para>
	Java 1.4.2, either SDK or JRE, needs to be installed.
	</para>
	</listitem>
	<listitem>
	<para>
	PostgreSQL must be installed and running. See <xref
	linkend="cb-postgres-install"/> for more details. It is
	strongly recommended to use version 8 or higher.
	</para>
	</listitem>
	</itemizedlist>       
	
	</para>
	</section>

<section>
<title>Installation of the <dcache/> Software</title>
    <para>
      The RPM packages may be installed right away on each node:

<screen><rootprompt/><command>rpm</command> -ivh dcache-server-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm
<rootprompt/><command>rpm</command> -ivh dcache-client-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm</screen>

      and the <pnfs/> server software on the admin node:

<screen><rootprompt/><command>rpm</command> -ivh pnfs-postgresql-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm</screen>

    </para>
</section>


<section>
<title>Configuring the PostgreSQL Server</title>

      
      <para>
	For <dcache/> version 1.6.6 release 1, please make sure the file
	<filename>/var/lib/pgsql/data/postgresql.conf</filename>
	contains

<programlisting>...
add_missing_from = on
...</programlisting>

	This will not be necessary in future releases.
	
      </para>

      <para>
	The file <filename>/var/lib/pgsql/data/pg_hba.conf</filename>
	should contain
	
<programlisting>...
local   all         all                        trust
host    all         all         127.0.0.1/32   trust
host    all         all         ::1/128        trust</programlisting>

	For these changes to take effect, restart the server, e.g. with
      
<screen><rootprompt/><command>/etc/init.d/postgresql</command> restart</screen>
	
      </para>
      
      <para>
        Prepare the PostgreSQL users and databases as they are needed
        for the components of <dcache/> and/or the <pnfs/> server: The <pnfs/>
        server only needs a database user. We suggest to call it
        <quote>pnfsserver</quote>. Create it with

<screen><rootprompt/><command>createuser</command> -U postgres --no-superuser --no-createrole --createdb --pwprompt pnfsserver</screen>

        Several databases will be created by this user. At initial installation,
        as described below, two databases will be created:
        <quote>admin</quote> and <quote>data1</quote>. These databases
        will contain the information about the namespace of the <pnfs/>
        filesystem. If the information in these databases is lost, the whole
        data in the <dcache/> instance is not accessible anymore. Therefore,
        make sure these databases are backed up regularly and also stored on
        appropriately reliable hardware. Further advice may be found in <xref
          linkend="cb-postgres"/> of the <dcache/> book.
      </para>

      <para>
        The <dcache/> components will access the database server with the user
        <quote>srmdcache</quote>:

<screen><rootprompt/><command>createuser</command> -U postgres --no-superuser --no-createrole --createdb --pwprompt srmdcache</screen>

        Several central components running on the admin node as well as each
        <srm/> will use the database <quote>dcache</quote> for state
        information:

<screen><rootprompt/><command>createdb</command> -U srmdcache dcache</screen>

        There might be several of these on several hosts. Each is used by the
        <dcache/> components running on the respective host.
      </para>

      <para>
        The <pnfs/> companion uses the database <quote>companion</quote> to
        store the pools all files are located on. On the admin node create and 
        initialize it with

<screen><rootprompt/><command>createdb</command> -U srmdcache companion
<rootprompt/><command>psql</command> -U srmdcache companion -f /opt/d-cache/etc/psql_install_companion.sql</screen>

        (It has to be located on the same host as the <pnfs/> server.)
      </para>

      <para>
        If the resilience feature provided by the <glossterm
          linkend="gl-replicamanager">replica manager</glossterm> is used, the
        database <quote>replicas</quote> has to be prepared on the admin node

<screen><rootprompt/><command>createdb</command> -U srmdcache replicas
<rootprompt/><command>psql</command> -U srmdcache replicas -f /opt/d-cache/etc/psql_install_replicas.sql</screen>
        
        Note that the disk space will at least be cut in half if the replica
        manager is used.
      </para>

      <para>
        If the billing information should also be stored in a database
        (in addition to files) the database <quote>billing</quote> has
        to be created:

<screen><rootprompt/><command>createdb</command> -U srmdcache billing</screen>
        
	However, we strongly advise against using the same database
	server for the <pnfs/> server and the billing information.
	For how to configure the billing cell to write into this database, see below. <!-- <xref linkend="cb-accounting"/>. -->
      </para>

    </section>

    <section>
      <title>Installing the <pnfs/> Server</title>

      <para>
	The <pnfs/> server software is installed in the directory
	<filename class="directory">/opt/pnfs/</filename>. For the
	installation copy the file
	<filename>/opt/pnfs/etc/pnfs_config.template</filename> to
	<filename>/opt/pnfs/etc/pnfs_config</filename>. It contains

<programlisting>PNFS_INSTALL_DIR = /opt/pnfs
PNFS_ROOT = /pnfs
PNFS_DB = /opt/pnfsdb
PNFS_LOG = /var/log
PNFS_OVERWRITE = no
PNFS_PSQL_USER = pnfsserver</programlisting>

	and should be suitable for most installations. Next, run

<screen><rootprompt/><command>/opt/pnfs/install/pnfs-install.sh</command>
PNFS_PSQL_USER = pnfsserver
 Checking nfs servers : Ok
      Preparing setup : Ok

 Creating database admin
 Creating database data1

 Starting pnfs server   ... Ok
 Trying to talk to dbserver 0 [1122] ... Ok
 Trying to talk to dbserver 1 [1122] ... Ok
             Trying to mount &apos;pnfs&apos; : Ok
        Correcting pnfs permissions : Ok
 Detecting wormhole target (config) : 0000000000000000000010E0
                  Digging wormholes :  dig-0-ok dig-1-ok Done
             Creating database link : Ok
 Setting mount permissions to world : Ok

 Remarks :
   ii) Any host may now mount this pnfs server
         mount -o intr,rw,noac,hard  &lt;thisServerName&gt;:/pnfs /&lt;mountdir&gt;

Installation of PNFS completed - stop PNFS
 Stopping Heartbeat ....  Ready
 Killing pnfsd  Done
 Killing pmountd  Done
 Killing dbserver . Done
 Removing 8 Clients  0+ 1+ 2+ 3+ 4+ 5+ 6+ 7+
 Removing 8 Servers  0+ 1+ 2+ 3+ 4+ 5+ 6+ 7+
 Removing main switchboard ... O.K.
</screen>

	This will write the central configuration file
	<filename>/usr/etc/pnfsSetup</filename> and initialize the
	databases in the PostgreSQL server as well as configuration
	information below <filename
	class="directory">/opt/pnfsdb/</filename> (as configured by
	<varname>PNFS_DB</varname> in
	<filename>/opt/pnfs/etc/pnfs_config</filename>.)
      </para>

      <para>
	The <pnfs/> server may now be started with

<screen><rootprompt/><command>/opt/pnfs/bin/pnfs</command> start
Starting dcache services:  Shmcom : Installed 8 Clients and 8 Servers
Starting database server for admin (/opt/pnfsdb/pnfs/databases/admin) ... O.K.
Starting database server for data1 (/opt/pnfsdb/pnfs/databases/data1) ... O.K.
Waiting for dbservers to register ... Ready
Starting Mountd : pmountd
Starting nfsd : pnfsd
</screen>

	This script may be copied to <filename
	class="directory">/etc/init.d/</filename> and the <pnfs/> may be
	configured to start at boot time with

<screen><rootprompt/><command>chkconfig</command> --add pnfs
<rootprompt/><command>chkconfig</command> pnfs on</screen>

      </para>

    </section>

    <section>
      <title>Installing <dcache/> Components</title>

      <para>
	Quick <dcache/> installation:
      </para>

      <para>
	Use the templates of the configuration files found in
	<filename class="directory">/opt/d-cache/etc/</filename> to
	create the following files.
      </para>

      <para>
	The central configuration file of a <dcache/> instance is
	<filename>/opt/d-cache/config/dCacheSetup</filename>. For most
	installation it is only necessary to set the variable
	<varname>java</varname> to the binary of the java VM and the
	variable <varname>serviceLocatorHost</varname> to the hostname
	of the admin node. Note that the file has to go into the
	subdirectory <filename class="directory">config/</filename>
	even though the template is found in <filename
	class="directory">etc/</filename>.
      </para>

      <para>
	The installation and start-up scripts use the information in
	<filename>/opt/d-cache/etc/node_config</filename>. The
	variable <varname>NODE_TYPE</varname> controls whether the
	admin node should be installed or just pools and/or
	doors. Accordingly set it to <quote>admin</quote> or
	<quote>pool</quote> (for doors, as well). All other variables
	may be left at their default value.
      </para>

      <para>
	Whether and how many pools should be installed on the current
	node is configured by
	<filename>/opt/d-cache/etc/pool_path</filename>. Each line in
	this file describes one pool. The format is as follows:

<programlisting><replaceable>poolDataDirectory</replaceable> <replaceable>poolSizeInGB</replaceable> <replaceable>reinstallPoolYesNo</replaceable></programlisting>

	where <replaceable>poolDataDirectory</replaceable> is the full
	path to the directory which will contain the data files as
	well as some of the configuration of the pool,
	<replaceable>poolSizeInGB</replaceable> is the size of the
	pool. Make shure that there is always enough space under
	<replaceable>poolDataDirectory</replaceable>. Be aware that
	only pure data content is counted by <dcache/>. Leave enough
	room for configuration files and filesystem overhead.
     </para>

      <para>
	For authorization of grid users the file
	<filename>/opt/d-cache/etc/dcache.kpwd</filename> is
	needed. Note that it may be generated from the standard
	<filename>/etc/grid-security/grid-mapfile</filename> with the
	tool <filename>grid-mapfile2dcache-kpwd</filename> which is
	distributed with the LCG software.
      </para>

      <para>
	When all configuration files are prepared configure the system with

<screen><rootprompt/><command>/opt/d-cache/install/install.sh</command>

[INFO]  No 'SERVER_ID' set in 'node_config'. Using SERVER_ID=<replaceable>your.domain</replaceable>.

[INFO]  Moving /opt/d-cache/bin/dcache-opt out of the way, because it is obsolete.

[INFO]  Creating link /pnfs/ftpBase --> /pnfs/fs which is used by the GridFTP door.
[INFO]  Creating link /pnfs/<replaceable>your.domain</replaceable> --> /pnfs/fs/usr/


[INFO]  Checking on a possibly existing dCache/PNFS configuration ...

[INFO]  Configuring pnfs export '/pnfsdoors' (needed from version 1.6.6 on)
        mountable by world.
[INFO]  You may restrict access to this export to the GridFTP doors which
        are not on the admin node. See the documentation.

[INFO]  Generating ssh keys:
Generating public/private rsa1 key pair.
Your identification has been saved in ./server_key.
Your public key has been saved in ./server_key.pub.
The key fingerprint is:
e3:4a:13:d5:33:45:e0:cd:69:a3:fb:d7:a8:64:df:73 root@grid-se3.desy.de


[INFO]  Creating Pool <replaceable>hostname</replaceable>_1
[INFO]  Creating Pool <replaceable>hostname</replaceable>_2
</screen>

	and start the central components (only on the admin node) with

<screen><rootprompt/><command>/opt/d-cache/bin/dcache-core</command> start
Starting dcache services:
Starting lmDomain  6 5 4 3 2 1 0 Done (pid=6802)
Starting dCacheDomain  6 5 4 3 2 1 0 Done (pid=6875)
Starting dirDomain  6 5 4 3 2 1 0 Done (pid=6973)
Starting doorDomain  6 5 4 3 2 1 0 Done (pid=7058)
Starting adminDoorDomain  6 5 4 3 2 1 0 Done (pid=7144)
Starting httpdDomain  6 5 4 3 2 1 0 Done (pid=7234)
Starting utilityDomain  6 5 4 3 2 1 0 Done (pid=7330)
Starting pnfsDomain  6 5 4 3 2 1 0 Done (pid=7436)
Starting gridftp-clintonDomain  6 5 4 3 2 1 0 Done (pid=7569)
Starting gsidcap-clintonDomain  6 5 4 3 2 1 0 Done (pid=7672)
Starting srm-clintonDomain  6 5 4 3 2 1 0 Done (pid=7777)
</screen>

	the configured pools are started with

<screen><rootprompt/><command>/opt/d-cache/bin/dcache-pool</command> start
Starting dcache pool: Starting clintonDomain  6 5 4 3 2 1 0 Done (pid=7990)
</screen>

	These scripts may be copied to <filename
	class="directory">/etc/init.d/</filename> and <dcache/> is
	configured to start at boot-time with 

<screen><rootprompt/><command>chkconfig</command> --add dcache-core
<rootprompt/><command>chkconfig</command> --add dcache-pool
<rootprompt/><command>chkconfig</command> dcache-core on
<rootprompt/><command>chkconfig</command> dcache-pool on</screen>

      </para>

      <para>
	It is advisable to create a basic directory structure in the
	<pnfs/> namespace where each directory uses a separate <pnfs/>
	database. This is still true for the PostgreSQL version of
	<pnfs/> since the <pnfs/> server uses global locks on each
	database. <xref linkend="cf-pnfs-db"/> describes how it is
	done. LCG sites should use at least two databases for each VO
	they support: One for the directory
	<filename>/pnfs/<replaceable>domainName</replaceable>/data/<replaceable>voName</replaceable>/</filename>
	and one for the generated files, i.e. for
	<filename>/pnfs/<replaceable>domainName</replaceable>/data/<replaceable>voName</replaceable>/generated/</filename>.
      </para>

      <para>
	<xref linkend="cb-general-lcg"/> describes how to activate the
	new info provider included since version 1.6.6-2 based on an
	earlier LCG installation.
      </para>

    </section>
    
  </section>

  <section>
    <title>Upgrading a <dcache/> Instance</title>

    <para>
      Upgrading to bugfix releases within one version (e.g. from
      1.6.6-1 to 1.6.6-3) may be done by shutting down the server and
      upgrading the packages with

<screen><rootprompt/><command>rpm</command> -Uvh <replaceable>packageName</replaceable></screen>

      For details on the changes, pease refer to the <link
      linkend="rf-changelog">change log</link>.
    </para>

    <para>
      This section describes the upgrade of <dcache/> instances
      installed with the previous version (currently version 1.6.5)
      and also some earlier versions - notably the one distributed
      with the previous or current LCG software. The first
      section will give a quick upgrade guide. It might not be
      applicable to complex setups. Not all features of the new version
      will be enabled after the quick upgrade guide. The next section
      will give pointers on how to enable them.
    </para>

    <para>
      Note that the upgrade of <dcache/> is independent of a
      conversion of the <pnfs/> database from GDBM to PostgreSQL. The
      conversion and upgrade to the PostgreSQL version of <pnfs/> may
      be performed any time before or after the upgrade of
      <dcache/>. Do not perform the <dcache/> upgrade and the <pnfs/>
      database conversion simultaneously. It is better to do them one
      after the other, and test the system inbetween. Do not be
      mislead by the fact the <dcache/> release only contains the
      PostgreSQL version of <pnfs/> (since version 1.6.6). See <xref
      linkend="cb-pnfs-postgres"/> for a guide to convert and upgrade
      the <pnfs/> server.
    </para>

    <para>
      In case you are already using PostgreSQL (e.g. for the <srm/>),
      it is a good idea to upgrade to version 8 now, because prior to
      dCache Version 1.6.6 no precious data is stored and therefore
      can be wiped off, allowing a PostgreSQL 8 installation from
      scratch. Starting from <dcache/> version 1.6.6 PostgreSQL will
      be utilized more heavily, making migration a complex
      task. Another advantage of PostgreSQL 8 is an integrated
      mechanism for automatic backups.
    </para>

    <section>
      <title>Quick Upgrade Guide</title>

      <para>
	Stop the <dcache/> services on all nodes of the
	instance:

<screen><rootprompt/><command>/opt/d-cache/bin/dcache-pool</command> stop
<rootprompt/><command>/opt/d-cache/bin/dcache-opt</command> stop
<rootprompt/><command>/opt/d-cache/bin/dcache-core</command> stop
</screen>

	Leave the <pnfs/> server running. In LCG installations, there
	might be a <quote>meta-package</quote> installed which can
	prevent the update to the current version. It should be
	deinstalled. The following command will do that and will not
	harm if the metapackage is not installed. Therefore, go ahead
	and do it anyway:

<screen><rootprompt/><command>rpm</command> -e lcg-SE_dcache
</screen>

	Upgrade the <dcache/> RPM packages with

<screen><rootprompt/><command>rpm</command> -Uvh dcache-server-1.6.6-1.i386.rpm dcache-client-1.6.6-1.i386.rpm
</screen>

	For this quick upgrade you have to keep your old configuration
	files (i.e. <filename>config/dCacheSetup</filename>,
	<filename>config/PoolManager.conf</filename>
	<filename>etc/node_config</filename>,
	<filename>etc/door_config</filename>, and
	<filename>etc/pool_path</filename>). Do not use the
	templates. 
      </para>

      <para>
	However, make sure that <filename>etc/node_config</filename>
	contains

<programlisting>PNFS_OVERWRITE=no</programlisting>

	and that a single value is assigned to
	<varname>NODE_TYPE</varname>.  Check that
	<filename>etc/pool_path</filename> contains
	<quote><literal>no</literal></quote> in the third field of
	each line. You might also want to doublecheck the contents of
	<filename>etc/door_config</filename> on each node.
      </para>

      <para>
	Run the install script:

<screen><rootprompt/><command>/opt/d-cache/install/install.sh</command>
</screen>

	And start the server again with

<screen><rootprompt/><command>/opt/d-cache/bin/dcache-core</command> start
<rootprompt/><command>/opt/d-cache/bin/dcache-pool</command> start</screen>

	Note that the start-up script for the optional components is
	not needed anymore. Therefore, it is probably best to remove them:

<screen><rootprompt/><command>rm</command> /opt/d-cache/bin/dcache-opt /etc/init.d/dcache-opt
</screen>

</para>

    </section>

    <section>
      <title>After The Upgrade</title>

      <para>
	This section gives a few hints for solving problems and
	fine-tuning after the upgrade.
      </para>

      <para>
	Check if the information given in the files
	<filename>/opt/d-cache/etc/node_config</filename> and
	<filename>/opt/d-cache/etc/door_config</filename> is correct:
      </para>

      <para>
	Check, that a single value is assigned to
	<varname>NODE_TYPE</varname> in
	<filename>/opt/d-cache/etc/node_config</filename>.  If the
	assignment contains several words, the behaviour of some
	previous versions might be different from the new one.
      </para>

      <para>
	Check that the doors which are started in the (now obsolete)
	<filename>/opt/d-cache/bin/dcache-opt</filename> start-up
	script are also enabled in
	<filename>/opt/d-cache/etc/door_config</filename>. The latter
	file is now evaluated by the start-up script
	<filename>/opt/d-cache/bin/dcache-core</filename> and not by
	the install script any more. This might lead to a different
	behaviour.
      </para>

      <para>
	If, prior to the upgrade, you changed anything in a batch file
	(<filename>config/<replaceable>domainName</replaceable>.batch</filename>)
	these changes will be moved to files with names
	<filename>config/<replaceable>domainName</replaceable>.batch.rpmsave</filename>. There
	have been major changes to the batch files. Therefore it is
	necessary to reapply your changes. However, keep in mind that
	the batch files are considered to be part of the software and
	not configuration files. 
      </para>

      <para>
	It should not be necessary to change them in most
	situations. Try to find a prober configuration variable in
	<filename>config/dCacheSetup</filename>. (See the template in
	<filename>etc/dCacheSetup.template</filename> for hints.)  If
	it should still be necessary to change a batch file, contact
	<email>support@dcache.org</email> and report a <quote>request
	for enhancement</quote>. (See <xref linkend="cf-cellpackage"/>
	for background information on the batch files.)
      </para>

      <para>
	Your old <filename>config/PoolManager.conf</filename> will not
	be overwritten by the upgrade. Its format did not
	change. Therefore, it is fine to keep your old one. In case
	you did not customize the pool manager configuration, make
	sure that the <literal>set costcuts</literal> line reads

<programlisting>set costcuts -idle=0.0 -p2p=2.0 -alert=0.0 -halt=0.0 -fallback=0.0</programlisting>

	Prior versions installed a
	<filename>config/PoolManager.conf</filename> with
	<literal>-idle=1.0</literal> which will lead to undesired
	behaviour of the pool manager. 
      </para>

      <para>
	Before switching on the companion in
	<filename>config/dCacheSetup</filename> with the line

<programlisting>cacheInfo=companion</programlisting>

	you have to be aware of the following: 
      </para>

      <para>
	For each file the list of pools the file is stored on (the
	cache info) is now stored within the <pnfs/> namespace
	metadata. When switching on the companion, the <dcache/>
	system expects it to be stored in a PostgreSQL database. You
	should first create this database:

<screen><rootprompt/><command>createdb</command> -U srmdcache companion
<rootprompt/><command>psql</command> -U srmdcache companion -f /opt/d-cache/etc/psql_install_companion.sql</screen>

	on the node where the PnfsManager will run (normally the
	admin node). Now, put 

<programlisting>cacheInfo=companion</programlisting>

	into <filename>config/dCacheSetup</filename> and restart the
	PnfsManager:

<screen><command>/opt/d-cache/bin/dcache-core</command> restart</screen>

	Now the <dcache/> system will not be aware of any files stored
	on the pools. To make it aware again, you have to go through
	the following steps: Since this will take a while and will put
	a considerable load on the PnfsManager, take care that this is
	done with one pool at a time. You should also plan for a
	downtime:
      </para>

      <para>
	In the admin interface (see <xref linkend="intouch-admin"/>)
	go to a pool, e.g.

<screen><dcprompt select="local"/><command>cd</command> <replaceable>hostname</replaceable>_1</screen>

	and issue the command

<screen><dcpoolprompt/><command>pnfs register</command></screen>

	Then go to the pnfs manager:

<screen><dcpoolprompt/><command>..</command>
<dcprompt select="local"/><command>cd</command> PnfsManager</screen>

	Check the output of the <quote><command>info</command></quote>
	command repeatedly:

<screen><dcprompt select="PnfsManager"/>info
...
Threads (4) Queue
    [0] 10
    [1] 12
    [2] 9
    [3] 13
...</screen>

	and wait till the value for all four queues is zero. Then go
	to the next pool and repeat the process.
      </para>
      
    </section>

    
    
  </section>

  <section>
    <title>Additional Components</title>

    <para>
      Some features of <dcache/> are switched off by default after an
      installation. The following describes how to put them to use: 
    </para>

    <section>
      <title>Accounting/Billing Information in Database</title>

      <para>
	The billing information which is normally written to <filename
	class="directory">/opt/d-cache/billing/</filename> on the
	admin node will also be written to a database if
	<filename>config/dCacheSetup</filename> contains

<programlisting>billingToDb=yes</programlisting>

	A PostgreSQL server is expected to run on the admin node with
	a database user <quote><literal>srmdcache</literal></quote>
	and a database <quote><literal>billing</literal></quote> with

<screen><rootprompt/><command>createuser</command> -U postgres --no-superuser --no-createrole --createdb --pwprompt srmdcache
<rootprompt/><command>createdb</command> -U srmdcache billing</screen>
      </para>

    </section>

    <section>
      <title>Space Reservation</title>

      <para>
	The space reservation feature between the <srm/> and the
	GridFTP door may be switched on with

<programlisting>spaceReservation=true</programlisting>

      </para>

    </section>

  </section>
  
  <section id="in-rpm">
  <title>Old Installation Manual</title>
  <screen>
The latest release of the dCache distribution is version 1-6-5.
Release notes describing new features and bug fixes can be found at

http://www.dcache.org/manuals/experts_docs/rel-dcache-1-6-5.html 

Note for SRM users
------------------
 - The latest SRM client (used with srmcp) has an extended set of
   parameters. Therefore it is necessary to renew the config file
   that is typically located in the home directory of the user running
   the client command (~/.srmconfig/config.xml). Simply remove the
   file config.xml, it will be re-generated following the new format 
   when running srmcp again.
 - The SRM client code in release 1-6-5 provides compatibility with 
   CERN's CASTOR SRM implementation. Therefore all users that are
   interested in SRM based data transfer between CASTOR and their
   dCache instance should upgrade to the client RPM that is part of
   the 1-6-5 distribution.
=======================================================================

Note: From version 1.2.2-6 on it is required to have a Postgres database
      installed and activated on the node that is running the SRM server.

      Note: If you have installed version 1.2.2-6(-1) or a later version 
      you need to drop the postgres tables. This is required because of 
      a db schema change.

Perform the following steps to remove the tables:
1. locate configuration file  srm.batch for dcache srm and find values of  
parameters jdbcUrl, jdbcUser and jdbcPass the last element of the jdbc url 
is your database name, for example if the 
value of jdbcUrl is dbc:postgresql://host/dcache then the name of the 
database is dcache.

2. Use these parameters and the "psql" postgress client to connect to the 
sql server:

$psql -U <replaceable>user</replaceable> -h <replaceable>host</replaceable> <replaceable>database name</replaceable>

Once psql connects to the server the command prompt will appear:

dbname=&gt;

3. Execute the following commands (you can just cut-and-paste the following text
into the psql):

DROP TABLE  copyfilerequests ;
DROP TABLE  copyfilerequests_b ;
DROP TABLE  copyrequests ;
DROP TABLE  copyrequests_b ;
DROP TABLE  getrequests_protocols ;
DROP TABLE  getrequests_protocols_b ;
DROP TABLE  getfilerequests ;
DROP TABLE  getfilerequests_b ;
DROP TABLE  getrequests ;
DROP TABLE  getrequests_b ;
DROP TABLE  pins ;
DROP TABLE  pinrequests ;
DROP TABLE  srmnextrequestid ;
DROP TABLE  putrequests_protocols ;
DROP TABLE  putrequests_protocols_b ;
DROP TABLE  putfilerequests ;
DROP TABLE  putfilerequests_b ;
DROP TABLE  putrequests ;
DROP TABLE  putrequests_b ;
DROP TABLE  srmrequestcredentials ;

4. Make sure all tables have been dropped, type at the prompt to list all remaining tables
   \dt;
   Drop eventually remaining tables as described above.
 
You are done.



      The database is used to maintain state information about ongoing
      transfers in order to make them persistent to allow a restart of
      transfers in case of an interrupt (e.g. server failure/maintenance,
      network disconnect etc.). 

      Though there is no specific version required we recommend using a
      recent version that is usually part of the Linux distribution
      running on your system.
      Hints concerning the PostgreSQL configuration are provided below.
------------------------------------------------------------------------
Note: From version 1.2.2-7 on doors (GridFTP, SRM, gsidcap) can be 
      installed and configured on a selective basis, and, if required, on
      a node other than the admin node.
      Find the details below.

------------------------------------------------------------------------
       How to update a standard stand-alone dCache installation
------------------------------------------------------------------------
Because of the new SRM there are quite a few changes in the configura-
tion files. An old installation which has not been customized very much
is therefore updated most easily by doing a reinstall following these
rules: (The data in the pools will be preserved.)
-- Save copies of your old config files in /opt/d-cache/etc/ and
   /opt/d-cache/config/. Remove the old packages with 'rpm -e' or just
   by removing the whole directory /opt/d-cache/.
-- Install the d-cache packages according to the guide below with the
   following additions:
   - The PNFS system should stay as it is.
   - Use your old "etc/pool_path", but set the last column
      to "No". Otherwise the data in your pools would be deleted!
   - Use the old "etc/node_config" and "etc/dcache.kpwd" if needed
   - Create a new "config/dCacheSetup" starting from
     "etc/dCacheSetup.template" as described or with the aid of the old
     file.
     (Try: diff old-etc/dCacheSetup.template old-config/dCacheSetup)

For a customized installation it might be better to use the existing
configuration directories /opt/d-cache/etc/ and /opt/d-cache/config/ and
adjust them to the new version of the SRM. Especially the file
"config/srm.batch" has to be adjusted. A detailed description of the
parameters in this file is given at the end of these instructions.
------------------------------------------------------------------------

Find a set of rpms (as of 01/23/2005) to install a dCache based Disk Pool
Management system (no HSM support) at

 http://www.dcache.org/downloads/dcache-v1.2.2-7-j14.tgz


Get the tarball

 wget http://www.dcache.org/downloads/dcache-v1.2.2-7-j14.tgz

Unzip the tarball

 tar xvzf dcache-v1.2.2-7-j14.tgz

You should find the following files

Release.notes	
d-cache-core-1.5.2-xx.i386.rpm
dCache-installation-instructions.txt       
d-cache-client-1.0-xx.i386.rpm	
d-cache-opt-1.5.3-xx.i386.rpm
dcache-user-instructions.txt  
pnfs-3.1.10-xx.i386.rpm


The tar file contains 4 rpms:
 - pnfs manager
 - dCache core (admin/pool node)
 - dCache optional components for admin node 
   (srm/gridftp servers and the gsidcapdoor)
 - client (32 and 64 bit support for dcap access combined in a single lib
   (/opt/d-cache/dcap/lib/libdcap.so), e.g dc_lseek, dc_lseek64)
           

To set up a dCache instance that allows to access it via the dCap protocol
the following components need to be installed
 - pnfs        The namespace manager (appears as a filesystem to the user)
 - admin node  Provides all functionalities to manage a distributed disk pool
               (can also hold a pool)
 - pool node   A node that provides storage capacity the dCache instance which
               is managed by the PoolManager running on the admin node
To extend accessibility through GridFTP and SRM optional software components can 
be installed in addition to the core RPM. It is sufficient to just install the
RPM. No additional step is required.

Note: With the installation of the dCache core some configuration parameters 
      are stored in pnfs. Therefore the pnfs manager needs to be installed
      first.
      Though the pnfs manager and the dCache core (admin node) by design can
      be installed on different nodes this version of the installation package
      assumes that both are installed on the same physical machine. 


Prerequisites
-------------

I.   The dCache software is written in Java and requires a recent version of either
     the JAVA developer kit (jdk) or the runtime environment (jre) to be installed.

II.  In case the dCache is going to be accessed via GridFTP and/or SRM a host
     certificate is required. Contact the CA responsible for your community for
     details. The certificate is expected to be installed in
     /etc/grid-security.


III. PostgreSQL needs to be installed on the node running the cntral dCache
     services (i.e. the admin node). The db is used by the SRM server, SRM Pin Manager
     and the Resilience Manager. In case these services are not running on the node
     the db is installed on make sure it is allowed to connect to the db. Add
     a "host" entry to the table as described below.
     Get a recent version from the Linux distribution that is running on your system.
     Alternatively, RPMs can be found at

     http://www.postgresql.org/ftp/

     A version that is suitable for current versions of RH SL3 can be found at
     http://www.postgresql.org/ftp/binary/v8.0.4/linux/rpms/redhat/rhel-es-3.0/
     
     Client, Server and JDBC support is needed.

     The following instructions shall be used to configure and initialize the
     databases. They need to be executed only following the installation of
     the database. An upgrade of the dCache code does not require the 
     commands to be executed again. All commands shall be carried out by
     user 'postgres'

     su postgres
     # Create directory the db will live in
     mkdir <replaceable>database_directory_name</replaceable>/data

     # Command to initialize DB
     initdb -D <replaceable>database_directory_name</replaceable>/data
     
     # Enable network access in postgres config file (default port 5432 is used)
     <replaceable>database_directory_name</replaceable>/data/postgresql.conf
     #
     tcpip_socket = true

     # Edit <replaceable>database_directory_name</replaceable>/data/pg_hba.conf to allow hosts to connect
     # to the DB (records at the bottom of the file)
     # TYPE  DATABASE    USER        IP-ADDRESS        IP-MASK           METHOD

     local   all         all                                             trust
     host    all         all         127.0.0.1         255.255.255.255   trust
     host    all         all         <replaceable>IP of DB host</replaceable>   255.255.255.255   trust
     host    all         all         <replaceable>IP of SRM host</replaceable>  255.255.255.255   trust (if SRM host != DB host)
     #
     # Command to start the DBMS, make sure the log file exists and
     # user 'postgres' has write permission
     postmaster -i -D <replaceable>database_directory_name</replaceable>/data >logfile 2>&amp;1 &amp;
     [Note: You may want to create an rc-script under /etc/init.d 
            to automatically start the DB upon start of the system]

     # Command to create the DB for the SRM
     createdb dcache
     
     # Command to connect to the DB
     psql -U postgres dcache

     # Create DB user 'srmdcache'
     create user srmdcache password 'srmdcache';
     # Disconnect from dcache db
     \q

     # All tables required for SRM operation will be created by the SRM
     # server

     # Command to create the DB for the Resilience Manager
     createdb -O srmdcache replicas

     # Initialize db tables for the Resilience Manager
     # This step requires the dcache-core RPM (v 1.5.2-80 or higher) to be installed
     psql -d replicas -U srmdcache -f /opt/d-cache/etc/pd_dump-s-U_enstore.sql     

     # Just for completeness: Command to stop the DBMS (as user 'postgres')
     # pg_ctl stop -D <replaceable>database_directory_name</replaceable>/data

To install the pnfs manager follow the instructions below
-------------------------------------------------------

 1. install the pnfs rpm

 2. copy the template /opt/pnfs.3.1.10/pnfs/etc/pnfs_config.template =&gt;
                      /opt/pnfs.3.1.10/pnfs/etc/pnfs_config
    and customize pnfs_config according to your needs

    The pnfs config file contains

PNFS_INSTALL_DIR = /opt/pnfs.3.1.10/pnfs
PNFS_ROOT = /pnfs
PNFS_DB = /opt/pnfsdb
PNFS_LOG = /var/log/pnfsd.log
PNFS_OVERWRITE = no
    - don't overwrite pnfsdb if one exists in the place specified above

 3. run the install script at
    /opt/pnfs.3.1.10/pnfs/install/pnfs-install.sh
    - It generates the file "pnfsSetup" in /usr/etc/

 4. Start/Stop pnfs
    /opt/pnfs.3.1.10/pnfs/bin/pnfs start|stop
    - starts pnfs and mounts it at /pnfs/fs

 5. Security
    In order to minimize the administrative overhead the pnfs filesystem (/pnfs
    and /fs) is exported world-wide by default. /pnfs is required by local clients 
    utilizing the dcap protocol, while /fs is needed by dCache doors (SRM, GridFTP,
    gsidcap) that are not running on the host the pnfs filesystem is installed on.
    
    The ability to mount these filesystems can be limited by applying a kind of 
    "network mask" as a file name.
    The installation of pnfs installs a file called 0.0.0.0..0.0.0.0 in
    /pnfs/fs/admin/etc/exports. Suppose the ability to mount /pnfs (/fs) should be 
    limited to local hosts living in network 123.111. Therefore  the file would have 
    to be renamed to 255.255.0.0..123.111.0.0 or 255.255.255.0..123.111.1.0 for a 
    class C network. This can further be limited to individual hosts and particular
    pnfs subtrees, e.g. the host with IP address 123.111.1.1 is allowed to mount 
    /pnfs/theorie
    - create a file named 123.111.1.1
    - content of the file is (one line)
      /theorie   /0/root/fs/usr/data/theorie  30 rw,soft
    The mechanism as it is implemented will first look for the host IP address and will
    apply the rule if the file exists. If it doesn't it will select the one with the
    "mask" and will apply the rule therein respectively.
    

To install the admin node and or pool node(s) follow the instructions below
---------------------------------------------------------------------------

 1. Install the dCache core rpm. In case you want to install optional
    components, like the srm/gridftp servers and/or the client 
    components, it's a good time to install the "d-cache-opt" rpm(s) as well. 
    (Can also be done later.)
    NOTE: In case of the intent to access data using SRM based transfers 
          (srmcp) with an installation with multiple pool nodes the 
          d-cache-opt rpm need to be installed on every pool node. In
          addition to the software components each pool node needs a 
          host certificate and full access to the public Internet for
          TCP connections in the port range from port 20000 - 50000. 

    From dcache-core RPM rev 1.5.2-80 on the Resilience Manager is included.
    Please find more information about its functionality and configuration at
    http://cmsdcam.fnal.gov/dcache/resilient/Resilient_dCache_v1_0.html.

    The Resilience Manager is preconfigured but not automatically started
    with the core services. The dCache core start-up script contains the
    instructions required to start/stop the replica domain, but they are
    commented out. Remove the "#" at the beginning of the related lines.


 2. configure the installation by using the following template files
    in /opt/d-cache/etc. The arrow indicates the name of the customized file
    - node_config.template  --&gt; /opt/d-cache/etc/node_config
    - dcache.kpwd.template  --&gt; /opt/d-cache/etc/dcache.kpwd
    - dCacheSetup.template  --&gt; /opt/d-cache/config/dCacheSetup
    - pool_path.template    --&gt; /opt/d-cache/etc/pool_path
    - door_config.template  --&gt; /opt/d-cache/etc/door_config   (!!! NEW !!!)
    
    In case of a virgin machine (not an upgrade of an existing dCache
    installation) copy the .template file to its base name (e.g.
    cp node_config.template node_config) and customize the latter
    according to your requirements.
    Note: the final place of the dCacheSetup file is 
          /opt/d-cache/config/dCacheSetup. You need to copy it
          manually from /opt/d-cache/etc to the config directory.

 2.1. etc/node_config
    There is no dedicated rpm for the installation of a pool-node any
    longer. Selection of admin vs. pool node is done via the NODE_TYPE
    parameter in the node_config file. The admin node can also contain
    pools.

NODE_TYPE = dummy # either admin or pool
DCACHE_BASE_DIR = /opt/d-cache
PNFS_ROOT = /pnfs
PNFS_INSTALL_DIR = /opt/pnfs.3.1.10/pnfs
PNFS_START = yes               (start pnfs in case it's not running)
PNFS_OVERWRITE = no            (in case dCache config exists in pnfs)
POOL_PATH = /opt/d-cache/etc   (in case pools are to be configured on
                                admin node; for details see pool instr.)
NUMBER_OF_MOVERS = 100
      
      Copy the template to its base name, if required, and edit the resulting
      file as desired.

 2.2. etc/dcache.kpwd

      The dcache.kpwd authentication file.
      The template needs to be customized and is expected as
      /opt/d-cache/etc/dcache.kpwd

      In case there is an existing dcache.kpwd it will not be overwritten
      See the release notes for further information on the format.

 2.3. config/dCacheSetup

      Important note: If an existing dCacheSetup file is going to be re-used
                      make sure the Java classpath setting is uptodate. The 
                      setting that is required by this version of the software
                      can be found in /opt/d-cache/etc/dCacheSetup.template

                      From version 1.2.2-7 on there is a new parameter to
                      support remote db connections for the SRM server
                      "srmDbHost=<replaceable>your.dbHost.org</replaceable>"

      this is the primary configuration file for the dCache core and
      optional components, i.e. srm/gridftp
      Things that need attention (anything else has reasonable defaults)
      - java path
      - serviceLocatorHost
        - use the host name of the node that is running pnfs as it is
          defined in your DNS, replace string "SERVER" by the host name
      - pnfsSrmPath (default is /)
      - srmDbHost=<replaceable>your.dbHost.org</replaceable> to let the SRM server know about the db host
      - The following parameters should be set to "true" if the dCache
        installation is going to be used as a LCG Storage Element
        - RecursiveDirectoryCreation=true
        - AdvisoryDelete=true
      If dCache was previously not running on this machine or if there is no
      dCacheSetup file in the config directory copy the dCacheSetup.template
      file to /opt/d-cache/config and customize it according to your needs.

 2.4. etc/pool_path

      The template contains pool parameters (path, size, etc)

      The format of the pool_path file is (3 columns)

      /path/to/pool  size[GB]  "overwrite if exists (yes/no)"
   
      [Note: GB means 1024^3; space for inodes etc. is not accounted for]
      Copy the template to its base name, if required, and edit the resulting file
      as desired. Use an empty file if no pools are wanted (e.g. on a pure admin node).

 2.5. Install and configure the doors (GridFTP, SRM, gsidcap) -
      etc/door_config
 
      A "door" node (neither an "admin" nor a "pool" node) requires the core
      and the opt RPMs to be installed. However, only the following
      installation script needs be executed

      /opt/d-cache/install/install_doors.sh 
      (DON'T run /opt/d-cache/install/install.sh)

      Make sure the template (/opt/d-cache/etc/door_config.template was copied
      to /opt/d-cache/etc/door_config and customized before running the install_doors.sh
      script.

      The format of the door_config file is (2 columns)

      ADMIN_NODE      <replaceable>name of admin node running pnfs</replaceable>

      door          active   (default is all active)   
      --------------------
      GSIDCAP         yes    (or "no")
      GRIDFTP         yes    (or "no")
      SRM             yes    (or "no")

      Also the dCacheSetup.template file needs to be copied to
      /opt/d-cache/config/dCacheSetup and customized accordingly.

      If a door or multiple different doors are to be added to an "admin"
      and/or a "pool" node

      - Install the dcache-opt RPM on each node
      - Make sure the template (/opt/d-cache/etc/door_config.template was
        copied to /opt/d-cache/etc/door_config and customized before running
        the install_doors.sh
      - On a "pool" node, copy the dcache authentication file (../etc/dcache.kpwd) 
        from the admin node to /opt/d-cache/etc on the "pool" node(s)

 3. To install an "admin" or a "pool" node run the install script at

    - /opt/d-cache/install/install.sh

      For an "admin" node this will do all dCache specific preparations in pnfs, etc. 
      If there is a pool location configured in pool_path it will also install a pool
      (in case the file is empty it will not install/configure any pool related
      stuff).

    - /opt/d-cache/install/install_doors.sh

      in case one or multiple different of the following doors are supposed to 
      be installeda on the admin and/or the pool nodeB
      - GridFTP
      - SRM
      - gsidcap
      This will update the ../bin/dcache-opt script accordingly.         
      Note: "door" nodes need to mount the pnfs fs. Make sure NFS related
            communication is enabled between the "admin" and the "door" node(s).
            For pnfs installations prior to the one which is part of the 1.2.2-7
            distribution do the following
            
            - Make sure pnfs is running
            - cp /pnfs/fs/admin/etc/exports/127.0.0.1 \
              /pnfs/fs/admin/etc/exports/0.0.0.0..0.0.0.0
              (overwrite existing file)

    - Monitoring of the door domains via the Web page
      In the recent setup, the srm and the gridftp door(s) have changed their
      name(s) so that the web page is asking the wrong cell whether or not it's alive.
      The srm/gridftp name changed from SRM/GFTP to SRM/GFTP-<replaceable>HOSTNAME</replaceable> (where HOSTNAME is
      the host, the SRM/GFTP door is running on). To properly update the status page you 
      have to manually modify the config/httpd.batch on the headnode (or the node were the 
      http service is running).
      At the end of the httpd.batch file you will find a list of 'cells'. The ones you
      need to change are called SRM and GFTP. Please change them to SRM-<replaceable>HOSTNAME</replaceable> and
      GFTP-<replaceable>HOSTNAME</replaceable> respectively. In case multiple gridftp doors are configured you need 
      to add as many lines as there are gridftp doors.
      You need to restart the httpd service to activate the changes.
      In case you don't want to restart the services you may
      as well make the changes in the batch file (for future
      restarts ) and use the ssh interface to make temp.
      changes :

      (local) admin  cd collector@httpdDomain
      >>(collector@httpdDomain) admin > unwatch SRM
      > >>>(collector@httpdDomain) admin > unwatch GFTP
      > >>>(collector@httpdDomain) admin > unwatch DCap-gsi
      > >>>(collector@httpdDomain) admin > 
      > >>>(collector@httpdDomain) admin > watch SRM-<replaceable>HOSTNAME</replaceable>
      > >>>(collector@httpdDomain) admin > watch GFTP-<replaceable>HOSTNAME</replaceable>
      > >>>(collector@httpdDomain) admin > watch DCap-gsi-<replaceable>HOSTNAME</replaceable>
            
 4. Start/stop the dCache services

    Make sure pnfs is running and the pnfs filesystem is mounted
    - To start/stop pnfs 
     /opt/pnfs.3.1.10/pnfs/bin/pnfs start|stop
     Starting pnfs will also mount the fs, stopping it will unmount pnfs.

    Start the core services
    /opt/d-cache/bin/dcache-core start|stop

    [in case dCache optional components (srm/gridftp/gsidcapdoor) are
    installed on an "admin", a "pool" or a "door" node they are
    started/stopped with
     /opt/d-cache/bin/dcache-opt start|stop ]

    To start|stop a pool use
    /opt/d-cache/bin/dcache-pool start|stop


 5. Client installation
    Client components are installed under /opt/d-cache.

    The libraries (32 and 64-bit versions) can be found under
     /opt/d-cache/dcap/lib. libdcap.so and libpdcap.so
    are symbolic links pointing to the 64-bit version.
    Also the gsidcap tunnel lib (libgsiTunnel.so) is
    installed here. In case the 32 bit version is supposed to be
    the default the links can be customized accordingly.

    Besides the libraries header files (/opt/d-cache/dcap/include)
    and the dccp binary (/opt/d-cache/dcap/bin) are installed with
    the Client RPM).    
    It is sufficient to install the Client RPM. No further installation
    step is required to make the client functions operational.

 6. Log files

    Common location for all dCache related log files is
    /opt/d-cache/log
    The default location for the PNFS log file is
    /var/log/pnfsd.log

The default is all pools register automatically with the default
pgroup.

dCache SRM installation and configuration instructions
======================================================

Requirements on the srm and pool nodes

   1. The nodes on which srm server (srm cell) and pool 
      nodes are installed need to have the grid host certificate 
      installed. Please refer to the instructions from your Certification 
      Authority on how to obtain a grid host certificate. 
   2. There should be a postgres database server running on a 
      machine accessible by the srm server, and there should be a 
      postgres user account created, capable of creating new 
      tables.
      Instructions on the installation are provided in section
      "Prerequisites", top III

Non-standard DCache Services that srm relies upon
[Note: The configuration options as they are described below are
       already part of the srm.batch file as it is coming with
       this package. Ususally no modifications are necessary.
       Only when an existing installation is upgraded _AND_ the
       admin is going to reuse the old srm.batch file the "pin
       manager" config entries need to be added. The standard 
       RPM upgrade mechanism overwrites that file.] 

   1. Pin Manager.
      Pin Manager is used by srm to perform the so 
      called file "pin in cache" operation. When a file is in pinned 
      state, it will not be deleted from the cache to make room for 
      other incoming files. The Pin Manager cell can be started by 
      adding the following lines to one of the dcache domain 
      configuration "batch" files (e.g. srm.batch):
    
    #
    #pin manager
    #
    create diskCacheV111.services.PinManager PinManager \
    " default -export  \
     -jdbcUrl=jdbc:postgresql://localhost/dcache \
     -jdbcDriver=org.postgresql.Driver \
     -dbUser=<replaceable>user</replaceable> \
     -dbPass=<replaceable>password</replaceable>"

     [Note: defaults for <replaceable>user</replaceable>=srmdcache, <replaceable>password</replaceable>=srmdcache]

     The configurable parameters are the folowing:
    -jdbcUrl url is pointing to the type and the location of the 
	database, which will be used by the pin manager. For example, 
	if the database is running on a host "hosta" on a nonstandard 
        port "12345", and the database name is "name1", this option 
        value would be "jdbc:postgresql://hosta:12345/name1".
     -jdbcDriver specifies the class name for the driver. Should 
        remain the same for the postgres database.
     -dbUser a name of the database user
     -dbPass a password for the database user, could be an arbitrary 
	string if the host on which the pin manager is running is 
	included in the postgres list of the trusted hosts.
     The two optional parameters are the 
     -poolManager and -pnfsManager, which allow the specification 
       of alternative names for the PoolManager and PnfsManager cells.
    2. GsiftpTransferManager
       This service is used by srm to perform the transfers 
       from a remote server to the dcache via the gsiftp protocol.
       The GsiftpTransferManager cell is started by the folowing 
       "batch" command:

      #
      # RemoteGsiftpTransferManager
      #
      create diskCacheV111.services.GsiftpTransferManager
      RemoteGsiftpTransferManager
      \
        "default -export \
        -pool_manager_timeout=60 \
        -pnfs_manager_timeout=60 \
        -pool_timeout=300 \
        -mover_timeout=86400 \
        -max_transfers=30 \
      "
      The configurable parameters are the folowing:

      -pool_manager_timeout is the timeout in seconds for PoolManager 
       message exchanges.

      -pnfs_manager_timeout is the timeout in seconds for PnfsManager 
       message exchanges.

      -pool_timeout is the timeout in seconds before the first pool 
       message, confirming the the creation of the mover

      -mover_timeout is the time before the transfer manager will 
       stop waiting for the completion of the started transfer. If
       expired it will try to kill the mover, and report the error 
       back to the caller (srm).

      -max_transfers is the maximum number of simultaneous transfers.
       If more transfers are scheduled, the transfer manager will fail 
       them.
      
      3. Copy Manager, 
	This service is used by the srm when the source and the destination
        files in the srm copy request are both local to the storage.
        Its configuration parameters are mostly the same as of the
        GsiftpTransferManager. The example startup command follows:
        
	create diskCacheV111.doors.CopyManager CopyManager \
	"default -export \
	-pool_manager_timeout=60 \
	-pool_timeout=300 \
	-mover_timeout=86400 \
	-max_transfers=30 \
        "

SRM Configuration Guide
-----------------------
Note: The package is coming with a set of suitable parameters. We expect that
      modifications are necessary in rare cases only.
  
      We provide deep technical information for those who are interested in
      the details. Those details are not required to operate the SRM/dCache. 


	In order to start the srm server, the instance of the cell
	diskCacheV111.srm.dcache.Storage needs to be created. Here is 
 	the example of a dcache batch file command starting the srm cell, 
	illustrating most of the configurable parameters:

	create diskCacheV111.srm.dcache.Storage  SRM \
            "default -srmport=${srmPort1} \
            -export \
            -kpwd-file=${config}/dcache.kpwd \
            -pnfs-srm-path=/ \
            -buffer_size=1048576 \
            -tcp_buffer_size=1048576 \
            -parallel_streams=10 \
            -debug=true \
            -get-lifetime=86400000 \
            -put-lifetime=86400000 \
            -copy-lifetime=86400000 \
            -get-req-thread-queue-size=1000 \
            -get-req-thread-pool-size=30 \
            -get-req-max-waiting-requests=1000 \
            -get-req-ready-queue-size=1000 \
            -get-req-max-ready-requests=30 \
            -get-req-max-number-of-retries=10 \
            -get-req-retry-timeout=60000 \
            -get-req-max-num-of-running-by-same-owner=10 \
            -put-req-thread-queue-size=1000 \
            -put-req-thread-pool-size=30 \
            -put-req-max-waiting-requests=1000 \
            -put-req-ready-queue-size=1000 \
            -put-req-max-ready-requests=30 \
            -put-req-max-number-of-retries=10\
            -put-req-retry-timeout=60000 \
            -put-req-max-num-of-running-by-same-owner=10 \
            -copy-req-thread-queue-size=1000 \
            -copy-req-thread-pool-size=8 \
            -copy-req-max-waiting-requests=1000 \
            -copy-req-max-number-of-retries=30\
            -copy-req-retry-timeout=6000 \
            -copy-req-max-num-of-running-by-same-owner=10 \
            -recursive-dirs-creation=true \
            -jdbcUrl=jdbc:postgresql://localhost/dcache \
            -jdbcDriver=org.postgresql.Driver \
            -dbUser=srmdcache \
            -dbPass=srmdcache \
            "    
       
	The available configuration options are:

        -kpwd-file specifies the location of the dcache authorization 
	"database" file.

        -pnfs-srm-path specifies the root of the srm within the pnfs
	namespace. Essentially this means that the value of this option 
	will be prepended to all the local storage paths given to the srm 
        server.

        -buffer_size and -tcp_buffer_size specify the size of memory, in
	bytes, and socket buffer size, in bytes, to be used with the embedded 
	gsiftp clients, when performing transfers between the storage and 
	a gsiftp server.

        -parallel_streams specifies the max. number of parallel streams to be
	used by the embedded gsiftp client.

        -debug tells if extra debug info should be logged. Most of the debug 
        logging can be turned off by setting the printout domain variable to
	error (2). Usually this is done in the first line of the dcache batch
	file.

        -recursive-dirs-creation turnes on and off the automatic creation
	 of unexsistent directories, in case of put/copy requests.

        -jdbcUrl, -jdbcDriver, -dbUser, -dbPass: these options have exactly
	 same meaning as the same options of the PinManager.
         -jdbcUrl url is pointing to the type and the location of the
          database, which will be used by the pin manager. For example,
          if the database is running on a host "hosta" on a nonstandard
          port "12345", and the database name is "name1", this option
          value would be "jdbc:postgresql://hosta:12345/name1".
         -jdbcDriver specifies the class name for the driver. Should  
          remain the same for the postgres database.

        -get-lifetime, -put-lifetime, -copy-lifetime specify the lifetimes, in 
        milliseconds, of the srm get, put and copy requests respectively.

        In order to develop a better understanding of the rest of the 
        parameters we will first describe how the request scheduler works.
        Please note that the following explanation is a simplification.
        The SRM Scheduler executes the instances of the SRM Job classes. For
	the scheduler, execution of the job is the execution of the job's run
	method in one of the threads. Jobs are initially in the Pending state. 
	Once the scheduler receives the job, it puts it in the TQueued state and 
	transfers it into the Thread Queue.
	The Scheduler takes the java threads from the pool and will actually execute 
	the jobs' "run" methods. Once a thread in the pool becomes available,
	the first job from the Thread Queue is removed, and this job's state is
	changed to running. The thread starts the execution of the job's run method.
        Once the run method returns, the state of the job can still remain
	"Running", or it might have changed to  "Done" or "AsyncWait". 

        If the state is still "Running", it will be placed on the ready queue.
	Once the job execution is completed, it now waits to be put to the "Ready"
	state by the scheduler.
	If it is "AsyncWait" this means that the job is partially completed, and
	it now waits for the internal event to continue execution; if it is "Done", 
	the job needs no further processing.
        
        To limit the number of simultaneous transfers by the srm client/user (in
	cases when the srm server does not perform the transfer itself, i.e.
	"get" and "put" requests) the number of jobs that are "Ready" can be
	limited according to the number set in the configuration. The rest of the 
        requests, which are prepared to be "Ready" are put on the "Ready" queue. 
        Once the clients finish the transfers, they notify the system by changing 
        the state of the put or get file requests to "Done". If users/clients never 
        perform this state change, the request changes its state automatically 
        upon the expiration of the request's lifetime. If "Ready" spots become 
        available, corresponding requests are removed from the Ready queue and 
        their state changes to "ready".
        
        In the dcache srm there are three instances of the scheduler, one for
	each possible type of srm requests: copy, get and put.

        The following options are described as follows:
         
        -[type]-req-thread-queue-size - maximum number of requests in the
	thread queue

        -[type]-req-thread-pool-size - maximum number of threads in the thread 
	 pool. This parameter is especially important for copy requests,
	 since the copy operation for each file is performed in a separate thread. 
         The number for copy requests should be less than the -max_transfers 
         parameter of the transfer managers.

        -[type]-req-max-waiting-requests - maximum number of requests in
         the async wait jobs
         
        -[type]-req-ready-queue-size - maximum number of requests in the 
	 ready queue. This and the following parameters are not important 
         for the copy scheduler.

        -[type]-req-max-ready-requests
	 this parameter is important for put and get requests, and it is 
         equivalent to the number of transfer urls given out to the clients,
         which are actively transferring (or intend to transfer).

        -[type]-req-max-number-of-retries
         number of times the job is allowed to fail and to be retried
	 before SRM should give up and return an error to the user.

        -[type]-req-max-num-of-running-by-same-owner
         The job owner is roughly equivalent to the user account in the kpwd
	 file.
         When the jobs are removed from the Thread and Ready Queue, their
	 "owner" is taken in consideration. If the number of jobs submitted by 
         the user exceeds the number in the configuration, jobs of this particular 
         user will not be removed from the queue, even if they are first. 
         If there are jobs belonging to another user, for whom this number was not 
	 reached yet, these will be executed first. This will not lead to 
         underutilization of the system. If only one owner is running jobs 
         they all will get scheduled to occupy all available scheduler threads 
         or ready spots.

	Other available options are (these are not recommended to be used so
	they are not explained here) :
        -poolManager, 
	-pnfsManager, 
        -proxies-directory
        -url-copy-command
        -timeout-command
        -usekftp
        -globus-url-copy
        -use-urlcopy-script
        -use-dcap-for-srm-copy
        -use-gsiftp-for-srm-copy
        -use-http-for-srm-copy
        -use-ftp-for-srm-copy
        -save-memory

        For more information refer to the dCache web site at http://www.dcache.org and the 
        FNAL SRM web site at http://www-isd.fnal.gov/srm.

System Monitoring
=================

http://admin.node.org:2288

 - allows monitoring of services only

Admin Interface
===============

The admin interface offers a very rich set of commands (to
be described elsewhere) allowing to alter system configuration
while the system is running and to solve eventual problems.

Since the distribution comes with an initial password it is
important to login right after the installation in order to
customize it.
Note: The following is supported in version 1.2.2-1 for the
      first time and will be supported in future releases.

   Assuming you are logged in on the admin node log in to the
   admin interface to set the password

    ssh -l admin -c blowfish -p 22223 localhost
    (passwd : dickerelch)

    (local) admin > cd acm
    (acm) admin > create user admin
    (acm) admin > set passwd <replaceable>newPasswd</replaceable> <replaceable>newPasswd</replaceable>
    (acm) admin > ..
    (local) admin > logoff

  From now on login as user admin will only be successful if
  newPasswd is presented.

  Note: When setting the password string in the shell one can
        disable the echo by typing "ctrl I" following "set
        password".

</screen>
    
<!--
  <section id="in-rpm-deinstall">
    <title>Deinstallation (for New Install on Testsystems)<replaceable/title>

    <section id="in-rpm-deinstall-pnfs">
      <title>Deinstallation of PNFS</title>

      <orderedlist>
        <listitem>
          <para>The <application>pnfs</application>-services are stopped with
          <screen><prompt>[root@machine ~ ] # </prompt><userinput><command>/opt/pnfs.3.1.10/pnfs/bin/pnfs</command> stop</userinput></screen></para>
        </listitem>

        <listitem>
          <para>To remove the installation of PNFS in
          <filename>/opt/pnfs.3.1.10/</filename> type <screen><prompt>[root@machine ~ ] # </prompt><userinput><command>rpm</command> -e pnfs</userinput></screen></para>
        </listitem>

        <listitem>
          <para>If the logfile has been created, delete it with <screen><prompt>[root@machine ~ ] # </prompt><userinput><command>rm</command> /var/log/pnfsd.log</userinput></screen>
          It is not always there (When is it created?)</para>
        </listitem>

        <listitem>
          <para>Remove the database directory, so that a new installation does
          not use it: <screen><prompt>[root@machine ~ ] # </prompt><userinput><command>rm</command> -r /opt/pnfsdb</userinput></screen></para>
        </listitem>

        <listitem>
          <para>Next, emove the mount point directory: <screen><prompt>[root@machine ~ ] # </prompt> <userinput><command>rm</command> -r /pnfs</userinput></screen>
          It might be better to only remove the directory
          <filename>/pnfs/fs</filename>, if the mount point is needed later
          for installation of a POOL-node.</para>
        </listitem>

        <listitem>
          <para>Since at least the configuration file
          <filename>/opt/pnfs.3.1.10/pnfs/etc/pnfs_config</filename> will not
          be removed by the <quote><userinput><command>rpm</command> -e
          pnfs</userinput></quote>-command, it is necessary to remove
          <filename>/opt/pnfs.3.1.10/</filename> manually: <screen><prompt>[root@machine ~ ] # </prompt> <userinput><command>rm</command> -r /opt/pnfs.3.1.10</userinput></screen>
          This should be done last, so that we are always able to check in the
          configuration file for the configured paths.</para>
        </listitem>
      </orderedlist>
    </section>

    <section id="in-rpm-deinstall-dcache">
      <title>Deinstallation of dCache</title>

      <orderedlist>
        <listitem>
          <para>Deinstall the old packages with rpm: <screen><prompt>[root@machine ~ ] # </prompt> <userinput><command>rpm</command> -e d-cache-core</userinput>
<prompt>[root@machine ~ ] # </prompt> <userinput><command>rpm</command> -e d-cache-opt</userinput>
<prompt>[root@machine ~ ] # </prompt> <userinput><command>rpm</command> -e d-cache-client</userinput></screen></para>
        </listitem>

        <listitem>
          <para>Remove the POOL-directories as listed in
          <filename>/opt/d-cache/etc/pool_path</filename>: <screen><prompt>[root@machine ~ ] # </prompt> <userinput><command>rm</command> -r /where/ever/the/pool/path/is</userinput></screen></para>
        </listitem>

        <listitem>
          <para>For the same reason as with NFS, the directory
          <filename>/opt/d-cache/</filename> has to be removed: <screen><prompt>[root@machine ~ ] # </prompt> <userinput><command>rm</command> -r /opt/d-cache/</userinput></screen></para>
        </listitem>
      </orderedlist>
    </section>
  </section>

  <section id="in-rpm-pool">
    <title>Installation of a Pool Node</title>

    <orderedlist>
      <listitem>
        <para>Mount the PNFS (needed?) <screen><prompt>[root@machine ~ ] # </prompt> <userinput>mkdir -p /pnfs/<replaceable>domain</replaceable></userinput>
<prompt>[root@machine ~ ] # </prompt> <userinput><command>mount</command> -o intr,rw,noac,hard  <replaceable>server</replaceable>:/pnfs /pnfs/<replaceable>doma</replaceable></userinput></screen></para>
      </listitem>

      <listitem>
        <para>q<screen><prompt>[root@machine ~ ] # </prompt> <userinput>mkdir -p /pnfs/<replaceable>domain</replaceable></userinput>
<prompt>[root@machine ~ ] # </prompt> <userinput><command>mount</command> -o intr,rw,noac,hard  <replaceable>server</replaceable>:/pnfs /pnfs/<replaceable>doma</replaceable></userinput></screen></para>
      </listitem>
    </orderedlist>
  </section>
-->
  </section>
<unfinished>
  <section id="in-other-os">
  <title>Running dCache on non Linux OS'es</title>
  <para>
  Because of the wide spread usage of dCache in the grid word, we provide the dCache binary packages
  mainly for a certain Linux flavour complaint to the rest of the LCG, gLite or OSG software.
  The dCache system itself is, except for the filesystem engine (pnfs) and the dCap client, 100% java,
  so that a port should be easy as long as the OS is coming with a resonably modern java virtual machine. 
  For now dCache is fine with java 1.4 but very soon we will move to 1.5. This chapter should give an
  estimate on how much effort is necessary to do the porting to other OS'es.
  </para>
  <section>
  <title>dCache Pools</title>
  <para>
  The dCache pool code is certainly the easiest part to port.
  </para>
  </section>
  </section>
</unfinished>
</chapter>
