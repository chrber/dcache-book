<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % sharedents SYSTEM "shared-entities.xml" >
%sharedents;
]>

<chapter id="in">

  <title>Installing &dcache;</title>

  <chapterinfo>
    <author>
      <firstname>Michael</firstname>
      <surname>Ernst</surname>
      <affiliation>
        <shortaffil>FNAL</shortaffil>
      </affiliation>
    </author>

    <author>
      <firstname>Patrick</firstname>
      <surname>Fuhrmann</surname>
      <affiliation>
        <shortaffil>DESY</shortaffil>
      </affiliation>
    </author>

    <author>
      <firstname>Mathias</firstname>
      <surname>de Riese</surname>
      <affiliation>
        <shortaffil>DESY</shortaffil>
      </affiliation>
    </author>
  </chapterinfo>

  <para>
    The first section describes the installation of a fresh &dcache;
    instance using RPM files downloaded from <ulink
    url="http://www.dcache.org">the &dcache; home-page</ulink>.  It is
    followed by a guide to upgrading an existing installation. In both
    cases we assume standard requirements of a small to medium sized
    &dcache; instance without an attached <glossterm
    linkend="gl-tss">tertiary storage system</glossterm>. The third
    section contains some pointers on extended features.
  </para>

  <section id="in-install">
    <title>Installing a Single Node &dcache; Instance</title>

    <para>
      In the following the installation of a single node &dcache;
      instance will be described.  The &chimera; name space provider,
      some management components, and the &srm; need a &psql; server
      installed. We recommend running this &psql; on the local
      node. The first section describes the configuration of a &psql;
      server. After that the installation of &chimera; and of the
      &dcache; components will follow. During the whole installation
      process root access is required.
    </para>

    <section>
      <title>Prerequisites</title>

      <para>
        In order to install &dcache; the following requirements must be met:
      </para>

      <itemizedlist>
        <listitem>
          <para>
            An RPM-based Linux distribution is required for the
            following procedure. For Debian derived systems the RPM
            may be converted to a DEB using alien. Solaris is
            supported using either the Solaris package or the tarball.
          </para>
        </listitem>

        <listitem>
          <para>
           &dcache; 1.9 requires Java 1.5 or 1.6 SDK. We recommend
           Java 1.6. It is recommended to use the newest Java release
           available within the release series used.
          </para>
        </listitem>

        <listitem>
          <para>
            &psql; must be installed and running. See <xref
            linkend="cb-postgres-install"/> for more details. It is
            strongly recommended to use version 8 or higher.
          </para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Installation of the &dcache; Software</title>

      <para>
        The RPM packages may be installed right away, for example
        using the command:
      </para>

      <screen>&prompt-root; <userinput>rpm -ivh dcache-server-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm</userinput>
&prompt-root; <userinput>rpm -ivh dcache-client-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm</userinput></screen>
    	<para>
       The actual sources lie at <ulink url="http://www.dcache.org/downloads.shtml"/>.
    	 To install for example Version 1.9.4-2 of the server you would use this:
    	 <screen>&prompt-root; <userinput>rpm -ivh http://www.dcache.org/downloads/1.9/dcache-server-1.9.4-2.noarch.rpm</userinput></screen>
    	 The Client can be found in the download-section of the above url, too.	
    	</para>	
    
    </section>
	

    <section>
      <title>Readying the &psql; server</title>

      <para>
        You must configure &psql; for use by &dcache; and create the
        necessary &psql; user accounts and database structure.  This
        section describes how to do this.
      </para>

      <section>
        <title>Configuring the &psql; server</title>

        <para>
          Using a &psql; server with &dcache; places a number of
          requirements on the database.  This section describes what
          configuration is necessary to ensure &psql; operates so
          &dcache; can use it.
        </para>

        <important>
          <title>Restarting &psql;</title>

          <para>
            If you have edited &psql; configuration files, you
            <emphasis>must</emphasis> restart &psql; for those changes
            to take effect.  On many systems, this can be done with
            the following command:
          </para>

          <screen>&prompt-root; <userinput>/etc/init.d/postgresql restart</userinput></screen>
        </important>


        <section>
          <title>Enabling &tcp; connections</title>
          
          <para>
            <important>For Versions of &psql; newer than 8.0 the &tcp;
            connections are already enabled and this section has to be
            ignored.</important>             
          </para>         
          
          <para>
            When connecting to &psql;, &dcache; will always use &tcp;
            connections.  So, for &dcache; to use &psql;, support for
            &tcp; sockets must be enabled. We realize UNIX domain
            sockets are easier to work with from a security point of
            view, however there is no way to use UNIX domain sockets
            from a Java application.
          </para>

          <para>
            In contrast to &dcache;, the &psql; stand-alone client
            application <command>psql</command> can connect using
            either a &tcp; socket or via a UNIX domain socket.
            Because of this, it is common for &psql; to disable &tcp;
            sockets by default, requiring the admin to explicitly
            configure &psql; so connecting via a &tcp; socket is
            supported.
          </para>

          <para>
            To enable &tcp; sockets, edit the &psql; configuration
            file <filename>postgresql.conf</filename>.  This is often
            found in the <filename
            class="directory">/var/lib/pgsql/data</filename>, but may
            be located elsewhere.  You should ensure that the line
            <literal>tcpip_socket</literal> is set to
            <literal>true</literal>; for example:
          </para>

          <programlisting>tcpip_socket = true</programlisting>
        </section>


        <section>
          <title>Enabling local trust</title>

          <para>
            Perhaps the simplest configuration is to allow
            password-less access to the database and the following
            documentation assumes this is so.
          </para>

          <para>
            To allow local users to access &psql; without requiring a
            password, ensure the file
            <filename>pg_hba.conf</filename>, usually located in
            <filename class="directory">/var/lib/pgsql/data</filename>,
            contains the following lines.
          </para>

          <programlisting>local   all         all                        trust
host    all         all         127.0.0.1/32   trust
host    all         all         ::1/128        trust</programlisting>

          <note>
            <para>
              Please note it is also possible to run &dcache; with all
              &psql; accounts requiring passwords.
            </para>
          </note>
        </section>

      </section>


      <section>
        <title>Configuring &chimera;</title>

        <para>
          &chimera; is a library providing a hierarhical name space
          with associated meta data. Where pools in &dcache; store the
          content of files, &chimera; stores the names and meta data
          of those files. &chimera; itself stores the data in a
          relational database. We will use &psql; in this tutorial.
        </para>

        <note>
          <para> &dcache; used to use another name space
          implementation called &pnfs;. &pnfs; is still available, we
          do however recommend that new installations use &chimera;.
          </para>
        </note>

        <section>
          <title>Initialize the database</title>

          <para>
            Create the <database>Chimera</database> user and database and add the
            Chimera-specific tables and stored procedures:
          </para>
      
          <screen>&prompt-root; <userinput>createdb -U postgres chimera</userinput>
CREATE DATABASE
                
&prompt-root; <userinput>createuser -U postgres --no-superuser --no-createrole --createdb --pwprompt chimera</userinput>
Enter password for new role:
Enter it again:
CREATE ROLE

&prompt-root; <userinput>psql -U chimera chimera -f /opt/d-cache/libexec/chimera/sql/create.sql</userinput>
psql:/opt/d-cache/libexec/chimera/sql/create.sql:23: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_inodes_pkey" for table "t_inodes"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:35: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_dirs_pkey" for table "t_dirs"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:45: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_inodes_data_pkey" for table "t_inodes_data"
<lineannotation>many more like this...</lineannotation>
INSERT 0 1
<lineannotation>many more like this...</lineannotation>
INSERT 0 1
CREATE INDEX
CREATE INDEX
psql:/opt/d-cache/libexec/chimera/sql/create.sql:256: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_storageinfo_pkey" for table "t_storageinfo"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:263: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_access_latency_pkey" for table "t_access_latency"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:270: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_retention_policy_pkey" for table "t_retention_policy"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:295: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_locationinfo_pkey" for table "t_locationinfo"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:311: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_locationinfo_trash_pkey" for table "t_locationinfo_trash"
CREATE TABLE
CREATE INDEX
psql:/opt/d-cache/libexec/chimera/sql/create.sql:332: NOTICE:  CREATE TABLE / PRIMARY KEY will create
implicit index "t_acl_pkey" for table "t_acl"
CREATE TABLE
CREATE INDEX

&prompt-root; <userinput>createlang -U postgres plpgsql chimera</userinput>
&prompt-root; <userinput>psql -U chimera chimera -f /opt/d-cache/libexec/chimera/sql/pgsql-procedures.sql</userinput>
CREATE FUNCTION
CREATE FUNCTION
CREATE FUNCTION
CREATE TRIGGER
CREATE FUNCTION
CREATE TRIGGER
CREATE SEQUENCE
CREATE FUNCTION
CREATE TRIGGER
</screen>

          <para>
            Database connection settings can be customized in
            <filename>/opt/d-cache/config/chimera-config.xml</filename>
            Specifically you should change the user to
            <quote>chimera</quote>.
          </para>

<screen>&lt;?xml version="1.0"?&gt;
&lt;config&gt;
        &lt;db fsid="0" url="jdbc:postgresql://localhost/chimera?prepareThreshold=3" drv="org.postgresql
.Driver" user="chimera" pass="" dialect="PgSQL" /&gt;
        &lt;nfs&gt;
                &lt;port&gt;2049&lt;/port&gt;
                &lt;logLevel&gt;0&lt;/logLevel&gt;
                &lt;logFile&gt;/tmp/himera.log&lt;/logFile&gt;
        &lt;/nfs&gt;
&lt;/config&gt;</screen>
        </section>

        <section>
          <title>Mounting Chimera through NFS</title>

          <para>
            Although most components in &dcache; access the &chimera;
            database directly, some rely on a mounted file system for
            access. The mounted file system is also nice for
            administrative access. This offers the opportunity to use
            OS-level tools like ls and mkdir for Chimera. However,
            direct I/O-operations like cp are not possible, since the
            NFSV3 interface provides the namespace part only. This
            section describes how to start the &chimera; NFS3 server
            and mount the name space.
          </para>

          <para>Chimera NFS server uses
          <filename>/etc/exports</filename> file to manage
          exports. So it has to exist or be created.
          The typical <filename>exports</filename> file looks like this:</para>

<programlisting>/ localhost(rw)
/pnfs
# or
# /pnfs *.my.domain(rw)</programlisting>
          
          <para>
            Since &chimera; is coupled with &dcache; it uses the same configuration file
            and won't start without it. So copy the <filename>/opt/d-cache/etc/dCacheSetup.template</filename> to
            <filename>/opt/d-cache/config/dCacheSetup</filename>. 
          </para>

          <note>
            <para>
              On some linux distributions you might have to switch the portmap
              daemon off before starting chimera:
            </para>
            <screen>&prompt-root; <userinput>/etc/init.d/portmap stop</userinput>
Stopping portmap: portmap</screen>
          </note>
          
          <para>
            Start it via script:
          </para>

<screen>&prompt-root; <userinput>/opt/d-cache/libexec/chimera/chimera-nfs-run.sh start</userinput></screen>

          <para>
            To automate the launching of that script at startup time,
            link to it from <filename>/etc/init.d/</filename>. Then
            announce it to chkconfig:
          </para>

<screen>&prompt-root; <userinput>chkconfig --add chimera-nfs-run.sh</userinput>
&prompt-root; <userinput>chkconfig chimera-nfs-run.sh on</userinput></screen>

          <para>
            First we create the root of the Chimera namespace, called
            'pnfs' for legacy reasons.
          </para>

<screen>&prompt-root; <userinput>/opt/d-cache/libexec/chimera/chimera-cli.sh Mkdir /pnfs</userinput></screen>

          <para>
            Now we need to add directory tags.
            For more information on tags see <xref linkend="cf-pnfs-tags"/>.:
          </para>

<screen>&prompt-root; <userinput>/opt/d-cache/libexec/chimera/chimera-cli.sh Mkdir /pnfs/<replaceable>your domain</replaceable></userinput>
&prompt-root; <userinput>/opt/d-cache/libexec/chimera/chimera-cli.sh Mkdir /pnfs/<replaceable>your domain</replaceable>/data</userinput>
&prompt-root; <userinput>echo "chimera" | /opt/d-cache/libexec/chimera/chimera-cli.sh Writetag /pnfs/<replaceable>your domain</replaceable>
/data sGroup</userinput>
&prompt-root; <userinput>echo "StoreName sql" | /opt/d-cache/libexec/chimera/chimera-cli.sh Writetag /pnfs/<replaceable>your do
main</replaceable>/data OSMTemplate</userinput>
</screen>

          <para>If you plan to use &dcap; with mounted file system
          instead of the URL-syntax (e.g. &prog-dccp;
          <filename>/pnfs/desy.de/data/file1</filename>
          <filename>/tmp/file1</filename>), we need to mount the
            root of &chimera; locally
            (remote mounts are not allowed yet). This will allow us to 
            establish wormhole files so &dcap; clients can discover the &dcap; doors.
          </para>

<screen>&prompt-root; <userinput>mount localhost:/ /mnt</userinput>
&prompt-root; <userinput>mkdir /mnt/admin/etc/config/dCache</userinput>
&prompt-root; <userinput>touch /mnt/admin/etc/config/dCache/dcache.conf</userinput>
&prompt-root; <userinput>touch /mnt/admin/etc/config/dCache/'.(fset)(dcache.conf)(io)(on)'</userinput>
&prompt-root; <userinput>echo "<replaceable>door host</replaceable>:<replaceable>port</replaceable>" > /mnt/admin/etc/config/dCache/dcache.conf</userinput></screen>
          <para>
            The default values for ports can be found in <xref linkend="rf-ports"/>. They can be altered in 
            <filename>/opt/d-cache/config/dCacheSetup</filename> 
          </para>

          <para>
            The configuration is done now, so unmount &chimera;:
          </para>

<screen>&prompt-root; <userinput>umount /mnt</userinput></screen>

          <para>
            Please note that whenever you need to change the
            configuration, you have to remount the root
            <literal>localhost:/</literal> to a temporary location like <filename class="directory">/mnt</filename>.
          </para>

          <para>The <quote>user's view</quote> of Chimera is automatically mounted
          by the &dcache; init script. You have to make sure that the mountpoint is
          created on the machine (<filename class="directory">/pnfs</filename>). 
          &chimera; can be mounted manually with:</para>

<screen>&prompt-root; <userinput>mkdir /pnfs</userinput>
&prompt-root; <userinput>mount localhost:/pnfs /pnfs</userinput></screen>
        </section>
      </section>

      <section>
        <title>Creating users and databases for &dcache;</title>

        <para>
          The &dcache; components will access the database server with
          the user <database class="user">srmdcache</database> which can
          be created with the <command>createuser</command>; for
          example:
        </para>

        <screen>&prompt-root; <userinput>createuser -U postgres --no-superuser --no-createrole --createdb --pwprompt srmdcache</userinput></screen>

        <para>
          Several management components running on the head node as well
          as the &srm; will use the database
          <database>dcache</database> for state information:
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache dcache</userinput></screen>

        <para>
          There might be several of these on several hosts. Each is
          used by the &dcache; components running on the respective
          host.
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache companion</userinput>
&prompt-root; <userinput>psql -U srmdcache companion -f /opt/d-cache/etc/psql_install_companion.sql</userinput></screen>

        <para>
          If the resilience feature provided by the <glossterm
          linkend="gl-replicamanager">replica manager</glossterm> is
          used, the database <quote>replicas</quote> has to be prepared
          on the head node with the command:
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache replicas</userinput>
&prompt-root; <userinput>psql -U srmdcache replicas -f /opt/d-cache/etc/psql_install_replicas.sql</userinput></screen>

        <note>
          <para>
            Note that the disk space will at least be cut in half if the
            replica manager is used.
          </para>
        </note>

        <para>
          If the billing information should also be stored in a
          database (in addition to files) the database
          <database>billing</database> has to be created:
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache billing</userinput></screen>

        <para>
          However, we strongly advise against using the same database
          server for &chimera; and the billing information.  For how
          to configure the &cell-billing; to write into this database,
          see below.
        </para>
      </section>
    </section>

    <section>
      <title>Installing &dcache; Components</title>

      <para>
        The main configuration file of a &dcache; instance is
        <filename>/opt/d-cache/config/dCacheSetup</filename>. Set the
        variable <varname>java</varname> to the binary of the Java VM
        and the variable <varname>serviceLocatorHost</varname> to the
        hostname of the single node running &dcache;.
      </para>

      <para>
        Use the templates of the configuration files found in
        <filename class="directory">/opt/d-cache/etc/</filename> to
        create the following files.
      </para>
          
      <para>
        The installation and start-up scripts use the information in
        <filename>/opt/d-cache/etc/node_config</filename>. First copy it from the template.
        For a setup with a single node, set <varname>NODE_TYPE</varname> to
        <quote>admin</quote>. To enable doors on this node, add the
        respective doors to <varname>SERVICES</varname>, for instance
        <quote>dcap</quote> or <quote>gridftp</quote>. Set
        <varname>NAMESPACE</varname> to <quote>chimera</quote>.
      </para>

      <para>
        For authorization of grid users the file
        <filename>/opt/d-cache/etc/dcache.kpwd</filename> is
        needed. You can simply copy the template that is in the same directory. Note that it may be generated from the standard
        <filename>/etc/grid-security/grid-mapfile</filename> with the
        tool <filename>grid-mapfile2dcache-kpwd</filename> which is
        distributed with the WLCG software.
      </para>

      <para>
        We proceed by finalising the initial configuration by
        executing
        <command>/opt/d-cache/install/install.sh</command>, for
        example:
      </para>

      <screen>&prompt-root; <userinput>/opt/d-cache/install/install.sh</userinput>
INFO:Skipping ssh key generation

 Checking MasterSetup  ./config/dCacheSetup O.k.

   Sanning dCache batch files

    Processing adminDoor
    Processing chimera
    Processing dCache
    Processing dir
    Processing door
    Processing gPlazma
    Processing gridftpdoor
    Processing gsidcapdoor
    Processing httpd
    Processing info
    Processing infoProvider
    Processing lm
    Processing maintenance
    Processing chimera
    Processing pool
    Processing replica
    Processing srm
    Processing statistics
    Processing utility
    Processing xrootdDoor


 Checking Users database .... Ok
 Checking Security       .... Ok
 Checking JVM ........ Ok
 Checking Cells ...... Ok
 dCacheVersion ....... Version production-1-9-3-1
        </screen>

        <para>
          No pools have been created on the node yet. Adding pools to
          a node is a two step process:
        </para>

        <orderedlist>
          <listitem>
            <para>
              The directory layout of the pool is created and filled
              with a skeleton configuration using <command>dcache pool
              create <replaceable>poolSize</replaceable>
              <replaceable>poolDirectory</replaceable></command>,
              where <replaceable>poolDirectory</replaceable> is the
              full path to the directory which will contain the data
              files as well as some of the configuration of the pool,
              and <replaceable>poolSize</replaceable> is the size of
              the pool, specified in bytes or with a M, G, or T suffix
              (for mibibytes, gibibytes and tibibytes, respectively).
            </para>

            <para>
              Make sure that there is always enough space under
              <replaceable>poolDirectory</replaceable>. Be aware that
              only pure data content is counted by &dcache;. Leave enough
              room for configuration files and filesystem overhead.
            </para>

            <para>
              Creating a pool does not modify the &dcache;
              configuration.
            </para>
          </listitem>
          <listitem>
            <para>
              The pool is given a unique name and added to the
              &dcache; configuration using <command>dcache pool add
              <replaceable>poolName</replaceable>
              <replaceable>poolDirectory</replaceable></command>,
              where <replaceable>poolDirectory</replaceable> is the
              directory in which the pool was created and
              <replaceable>poolName</replaceable> is a name for the
              pool. The name must be unique throughout the whole
              &dcache; installation, not just on the node.
            </para>
            <para>
              Adding a pool to a configuration does not modify the
              pool or the data in it and can thus safely be undone or
              repeated.
            </para>
          </listitem>
        </orderedlist>

        <note>
          <para>The default gap for poolsizes is 4GiB. This means you should
          make a bigger pool than 4GiB otherwise you would have to change this gap
          in the &dcache; admin tool. See the example below.
          See also <xref linkend="intouch-admin"/>.
          </para>

          <screen>&dc-prompt-local; <userinput>cd <replaceable>poolName</replaceable></userinput>
&dc-prompt-pool; <userinput>set gap 2G</userinput>
&dc-prompt-pool; <userinput>save</userinput></screen>
        </note>
        
        <informalexample>
        
        <para>
          An example may help to clarify the use of these commands:         
        </para>

        <screen>&prompt-root; <userinput>/opt/d-cache/bin/dcache pool create 500G /q/pool1</userinput>
Created a 500 GiB pool in /q/pool1. The pool cannot be used until it has
been added to a domain. Use 'pool add' to do so.

Please note that this script does not set the owner of the pool directory.
You may need to adjust it.
&prompt-root; <userinput>/opt/d-cache/bin/dcache pool add myFirstPool <filename class="directory">/q/pool1/</filename></userinput>

Added pool myFirstPool in /q/pool1 to dcache-vmDomain.

The pool will not be operational until the domain has been started. Use
'start dcache-vmDomain' to start the pool domain.
&prompt-user; <userinput>/opt/d-cache/bin/dcache pool ls</userinput>
Pool        Domain                       Size   Free Path
myFirstPool dcache-vmDomain               500    550 /q/pool1
Disk space is measured in GiB.
        </screen>
        </informalexample>
        
        <para>
          All configured components can now be starting with
          <userinput>dcache start</userinput>, for example:
        </para>

        <screen>&prompt-root; <userinput>/opt/d-cache/bin/dcache start</userinput>
Starting lmDomain  Done (pid=7514)
Starting dCacheDomain  Done (pid=7574)
Starting pnfsDomain  Done (pid=7647)
Starting dirDomain  Done (pid=7709)
Starting adminDomain  Done (pid=7791)
Starting httpdDomain  Done (pid=7849)
Starting utilityDomain  Done (pid=7925)
Starting gPlazma-dcache-vmDomain  Done (pid=8002)
Starting infoProviderDomain  Done (pid=8081)
Starting dcap-dcache-vmDomain  Done (pid=8154)
Starting gridftp-dcache-vmDomain  Done (pid=8221)
Starting gsidcap-dcache-vmDomain  Done (pid=8296)
Starting dcache-vmDomain  Done (pid=8369)
        </screen>

      </section>
  </section>

  <section id="in-multinode">
    <title>Installing a Multi Node dCache Instance</title>

    <para>
      The previous section decsribed how to install a single node
      &dcache; installation. A typically medium-sized &dcache;
      installation will however have a single head node hosting the
      name space provider and other central components, and a number
      of pool nodes. It is common to also use pool nodes as FTP and
      DCAP doors.
    </para>

    <para>
      The &chimera; file system must be mounted on all nodes running
      either the &srm; or &gridftp;. Client nodes relying on &dcap;
      access without using URLs also need to mount &chimera;. Pools do
      not need a mount anymore. Having a mount on the
      Chimera/NFS3-server node itself is always a good idea as it
      eases maintenance.
    </para>

    <para>
      To mount the &chimera; file system, either modify
      <filename>config/chimera-config.xml</filename> such that it
      points towards the correct &psql; host and start a local Chimera
      NFSv3 server locally, or mount the NFS file system exported from
      the head node. In the latter case, set
      <varname>NAMESPACE_NODE</varname> in
      <filename>etc/node_config</filename> to the host running the
      Chimera NFSv3 server.
    </para>

    <para>
      For the head node, follow the description of the previous
      chapter, but do not create any pools. For pools and for &dcap;
      or &gridftp; doors, &psql; is not needed and installation of
      &psql; can be skipped on nodes that only hosts these
      services. Proceed by creating
      <filename>config/dCacheSetup</filename>;
      <varname>serviceLocatorHost</varname> has to be set to the name
      of the head node. In <filename>etc/node_config</filename> leave
      <varname>NODE_TYPE</varname> empty. Add any doors you want to
      start on this node to <varname>SERVICES</varname> and set
      <varname>NAMESPAE</varname> to <quote>chimera</quote>. Run
      <filename>install/install.sh</filename> to finish the
      installation. Finally, use <command>dcache pool create</command>
      and <command>dcache pool add</command> to create and add pools
      on this node.
    </para>

  </section>

  <section id="in-upgrade">
    <title>Upgrading a &dcache; Instance</title>

    <para>
      Upgrading to bugfix releases within one version (e.g. from
      1.9.3-1 to 1.9.3-2) may be done by shutting down the server and
      upgrading the packages with

<screen>&prompt-root; <userinput>rpm -Uvh <replaceable>packageName</replaceable></userinput></screen>

      Follow this by rerunning <command>install/install.sh</command>.
      For details on the changes, please refer to the change log on
      the download page.
    </para>
  </section>

</chapter>
