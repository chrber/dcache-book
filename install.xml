<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % sharedents SYSTEM "shared-entities.xml" >
%sharedents;
<!ENTITY chimera-cli  "/opt/d-cache/bin/chimera-cli.sh">
]>

<chapter id="in">

  <title>Installing &dcache;</title>

  <chapterinfo>
    <author>
      <firstname>Michael</firstname>
      <surname>Ernst</surname>
      <affiliation>
        <shortaffil>FNAL</shortaffil>
      </affiliation>
    </author>

    <author>
      <firstname>Patrick</firstname>
      <surname>Fuhrmann</surname>
      <affiliation>
        <shortaffil>DESY</shortaffil>
      </affiliation>
    </author>

    <author>
      <firstname>Mathias</firstname>
      <surname>de Riese</surname>
      <affiliation>
        <shortaffil>DESY</shortaffil>
      </affiliation>
    </author>
  </chapterinfo>

  <para>
    The first section describes the installation of a fresh &dcache;
    instance using RPM files downloaded from <ulink
    url="http://www.dcache.org">the &dcache; home-page</ulink>.  It is
    followed by a guide to upgrading an existing installation. In both
    cases we assume standard requirements of a small to medium sized
    &dcache; instance without an attached <glossterm
    linkend="gl-tss">tertiary storage system</glossterm>. The third
    section contains some pointers on extended features.
  </para>

  <section id="in-install">
    <title>Installing a &dcache; instance</title>

    <para>
      In the following the installation of a &dcache;
      instance will be described.  The &chimera; name space provider,
      some management components, and the &srm; need a &psql; server
      installed. We recommend running this &psql; on the local
      node. The first section describes the configuration of a &psql;
      server. After that the installation of &chimera; and of the
      &dcache; components will follow. During the whole installation
      process root access is required.
    </para>

    <section>
      <title>Prerequisites</title>

      <para>
        In order to install &dcache; the following requirements must be met:
      </para>

      <itemizedlist>
        <listitem>
          <para>
            An RPM-based Linux distribution is required for the
            following procedure. For Debian derived systems the RPM
            may be converted to a DEB using alien. Solaris is
            supported using either the Solaris package or the tarball.
          </para>
        </listitem>

        <listitem>
          <para>
           &dcache; requires Java 1.6 JRE. It is recommended to use
	   the newest Java release
           available within the release series used.
          </para>
        </listitem>

        <listitem>
          <para>
            &psql; must be installed and running. See <xref
            linkend="cb-postgres-install"/> for more details. It is
            strongly recommended to use version 8 or higher.
          </para>
        </listitem>
      </itemizedlist>
    </section>

    <section>
      <title>Installation of the &dcache; Software</title>

      <para>
        The RPM packages may be installed right away, for example
        using the command:
      </para>

      <screen>&prompt-root; <userinput>rpm -ivh dcache-server-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm</userinput>
&prompt-root; <userinput>rpm -ivh dcache-client-<replaceable>version</replaceable>-<replaceable>release</replaceable>.i386.rpm</userinput></screen>
       <para>
       The actual sources lie at <ulink url="http://www.dcache.org/downloads.shtml"/>.
       To install for example Version &dcache-version;-1 of the server you would use this:
        <screen>&prompt-root; <userinput>rpm -ivh http://www.dcache.org/downloads/1.9/dcache-server-&dcache-version;-1.noarch.rpm</userinput></screen>
        The client can be found in the download-section of the above url, too.
        </para>

    </section>

    <section>
      <title>Readying the &psql; server for the use with &dcache;</title>

      <para>
	Using a &psql; server with &dcache; places a number of requirements on the database.
        You must configure &psql; for use by &dcache; and create the
        necessary &psql; user accounts and database structure.  This
        section describes how to do this.
      </para>

      <section>
        <title>Enabling local trust</title>

          <para>
            Perhaps the simplest configuration is to allow
            password-less access to the database and the following
            documentation assumes this is so.
          </para>

          <note>
            <para>
              Please note it is also possible to run &dcache; with all
              &psql; accounts requiring passwords. See <xref
              linkend="cb-postgres-configure"/> for
              more advice on the configuration of &psql;.
            </para>
          </note>

          <para>
            To allow local users to access &psql; without requiring a
            password, ensure the file
            <filename>pg_hba.conf</filename>, usually located in
            <filename class="directory">/var/lib/pgsql/data</filename>,
            contains the following lines.
          </para>

          <programlisting>local   all         all                        trust
host    all         all         127.0.0.1/32   trust
host    all         all         ::1/128        trust</programlisting>

          <important>
	    <title>Restarting &psql;</title>

	    <para>
            If you have edited &psql; configuration files, you
            <emphasis>must</emphasis> restart &psql; for those changes
            to take effect.  On many systems, this can be done with
            the following command:
	    </para>

	    <screen>&prompt-root; <userinput>/etc/init.d/postgresql restart</userinput>
Stopping postgresql service:                               [  OK  ]
Starting postgresql service:                               [  OK  ]</screen>
          </important>
        </section>

      <section>
        <title>Configuring &chimera;</title>

        <para>
          &chimera; is a library providing a hierarchical name space
          with associated meta data. Where pools in &dcache; store the
          content of files, &chimera; stores the names and meta data
          of those files. &chimera; itself stores the data in a
          relational database. We will use &psql; in this tutorial. Database connection settings can be customized in
            <filename>/opt/d-cache/config/chimera-config.xml</filename>
        </para>

        <note>
          <para> &dcache; used to use another name space
          implementation called &pnfs;. &pnfs; is still available, we
          do however recommend that new installations use &chimera;.
          </para>
        </note>

        <section>
          <title>Initialize the database</title>

          <para>
            Create the <database>Chimera</database> user and database and add the
            Chimera-specific tables and stored procedures:
          </para>

          <screen>&prompt-root; <userinput>createdb -U postgres chimera</userinput>
CREATE DATABASE

&prompt-root; <userinput>createuser -U postgres --no-superuser --no-createrole --createdb --pwprompt chimera</userinput>
Enter password for new role:
Enter it again:
CREATE ROLE

&prompt-root; <userinput>psql -U chimera chimera -f /opt/d-cache/libexec/chimera/sql/create.sql</userinput>
psql:/opt/d-cache/libexec/chimera/sql/create.sql:23: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_inodes_pkey" for table "t_inodes"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:35: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_dirs_pkey" for table "t_dirs"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:45: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_inodes_data_pkey" for table "t_inodes_data"
<lineannotation>many more like this...</lineannotation>
INSERT 0 1
<lineannotation>many more like this...</lineannotation>
INSERT 0 1
CREATE INDEX
CREATE INDEX
psql:/opt/d-cache/libexec/chimera/sql/create.sql:256: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_storageinfo_pkey" for table "t_storageinfo"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:263: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_access_latency_pkey" for table "t_access_latency"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:270: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_retention_policy_pkey" for table "t_retention_policy"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:295: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_locationinfo_pkey" for table "t_locationinfo"
CREATE TABLE
psql:/opt/d-cache/libexec/chimera/sql/create.sql:311: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_locationinfo_trash_pkey" for table "t_locationinfo_trash"
CREATE TABLE
CREATE INDEX
psql:/opt/d-cache/libexec/chimera/sql/create.sql:332: NOTICE:  CREATE TABLE / PRIMARY KEY will create implicit index "t_acl_pkey" for table "t_acl"
CREATE TABLE
CREATE INDEX

&prompt-root; <userinput>createlang -U postgres plpgsql chimera</userinput>
&prompt-root; <userinput>psql -U chimera chimera -f /opt/d-cache/libexec/chimera/sql/pgsql-procedures.sql</userinput>
CREATE FUNCTION
CREATE FUNCTION
CREATE FUNCTION
CREATE TRIGGER
CREATE FUNCTION
CREATE TRIGGER
CREATE SEQUENCE
CREATE FUNCTION
CREATE TRIGGER
</screen>

          <para>
            Since &dcache; version 1.9.7, &chimera; does not need to be started
            with an init script any more. &chimera; is now available as the
            &cell-nfsv3; service, which can be added by editing the &dcache;
            <firstterm>layout</firstterm> file    (see <xref linkend="in-install-layout" />) and will be started automatically as &dcache; is started.
          </para>

          <note>
            <para>
              In general you should switch the
              <systemitem class='service'>portmap</systemitem> daemon off
              before starting &chimera;:
            </para>
            <screen>&prompt-root; <userinput>/etc/init.d/portmap stop</userinput>
Stopping portmap: portmap</screen>
          </note>

        </section>
      </section>

        <section>
         <title>Creating users and databases for &dcache;</title>

         <para>
          The &dcache; components will access the database server with
          the user <database class="user">srmdcache</database> which can
          be created with the <command>createuser</command>; for
          example:
         </para>

        <screen>&prompt-root; <userinput>createuser -U postgres --no-superuser --no-createrole --createdb --pwprompt srmdcache</userinput></screen>

        <para>
          Several management components running on the head node as well
          as the &srm; will use the database
          <database>dcache</database> for state information:
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache dcache</userinput></screen>

        <para>
          There might be several of these on several hosts. Each is
          used by the &dcache; components running on the respective
          host.
        </para>


        <para>
	Now the configuration of &psql; is done and you can start &dcache;.
	</para>
	</section>

<screen>&prompt-root; <userinput>/opt/d-cache/bin/dcache start</userinput>
Starting dCacheDomain done</screen>
        </section>

      <section>
	<title>Mounting &chimera;</title>
      <!-- TODO: move this section to appendix. -->
        <section>
          <title>Mounting &chimera; through &nfs;</title>

          <para>
            Since release 1.9.7, &dcache; does not need the &chimera;
            filesystem to be mounted any longer. But a mounted file
            system can be nice for administrative access.
            This offers the opportunity to use
            OS-level tools like <command>ls</command> and <command>mkdir</command> for &chimera;. However,
            direct I/O-operations like <command>cp</command> are not possible,
            since the &nfs3; interface provides the namespace part only. This
            section describes how to mount &chimera; through &nfs; and how to create directory tags.
          </para>

          <para>The &chimera; &nfs; server uses the
          <filename>/etc/exports</filename> file to manage
          exports. If this file doesn't exist it must be created.
          The typical <filename>exports</filename> file looks like this:</para>

<programlisting>/ localhost(rw)
/pnfs
# or
# /pnfs *.my.domain(rw)</programlisting>

	  <para>
	    First you create the root of the &chimera; namespace, called
	    <filename class="directory">pnfs</filename> for legacy reasons and the directories where your data will be accessible.
	    <screen>&prompt-root; <userinput>&chimera-cli; Mkdir /pnfs</userinput>
&prompt-root; <userinput>&chimera-cli; Mkdir /pnfs/<replaceable>your domain</replaceable></userinput>
&prompt-root; <userinput>&chimera-cli; Mkdir /pnfs/<replaceable>your domain</replaceable>/data</userinput>
</screen>
	  </para>
          <para>
            Now you need to add directory tags.
            For more information on tags see <xref linkend="cf-pnfs-tags"/>.
          </para>

<screen>
&prompt-root; <userinput>echo "chimera" | &chimera-cli; Writetag /pnfs/<replaceable>your domain</replaceable>/data sGroup</userinput>
&prompt-root; <userinput>echo "StoreName sql" | &chimera-cli; Writetag /pnfs/<replaceable>your domain</replaceable>/data OSMTemplate</userinput>
</screen>
	</section>

	<section>
	  <title>Using &dcap; with a mounted file system</title>

          <para>
	    If you plan to use &dcap; with mounted file system
	    instead of the URL-syntax (e.g. &prog-dccp;
	    <filename>/pnfs/example.org/data/file1</filename>
	    <filename>/tmp/file1</filename>), you need to mount the
            root of &chimera; locally
            (remote mounts are not allowed yet). This will allow us to
            establish wormhole files so &dcap; clients can discover the &dcap; doors.
          </para>

<screen>&prompt-root; <userinput>mount localhost:/ /mnt</userinput>
&prompt-root; <userinput>mkdir /mnt/admin/etc/config/dCache</userinput>
&prompt-root; <userinput>touch /mnt/admin/etc/config/dCache/dcache.conf</userinput>
&prompt-root; <userinput>touch /mnt/admin/etc/config/dCache/'.(fset)(dcache.conf)(io)(on)'</userinput>
&prompt-root; <userinput>echo "<replaceable>door host</replaceable>:<replaceable>port</replaceable>" > /mnt/admin/etc/config/dCache/dcache.conf</userinput></screen>
          <para>
            The default values for ports can be found in <xref linkend="rf-ports"/> (for &dcap; the default port is <literal>22125</literal>)  and in the
            file
            <filename>/opt/d-cache/share/defaults/dcache.properties</filename>. They can be altered in
            <filename>/opt/d-cache/etc/dcache.conf</filename>
          </para>

          <para>
            When the configuration is complete you can unmount &chimera;:
          </para>

<screen>&prompt-root; <userinput>umount /mnt</userinput></screen>

          <note>
	    <para>
            Please note that whenever you need to change the
            configuration, you have to remount the root
            <literal>localhost:/</literal> to a temporary location like <filename class="directory">/mnt</filename>.
	    </para>
          </note>
      </section>
      </section>
      <section>
	<title>Replica manager and billing</title>
      <!-- TODO:  move this section to the appropriate chapters, which need to be rewritten. -->
              <para>
          If the resilience feature provided by the <glossterm
          linkend="gl-replicamanager">replica manager</glossterm> is
          used, the database <database>replicas</database> has to be prepared
          on the head node with the command:
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache replicas</userinput>
&prompt-root; <userinput>psql -U srmdcache replicas -f /opt/d-cache/etc/psql_install_replicas.sql</userinput></screen>

        <note>
          <para>
            Note that the disk space will at least be cut in half if the
            replica manager is used.
          </para>
        </note>

        <para>
          If the billing information should also be stored in a
          database (in addition to files) the database
          <database>billing</database> has to be created:
        </para>

        <screen>&prompt-root; <userinput>createdb -U srmdcache billing</userinput></screen>

        <para>
          However, we strongly advise against using the same database
          server for &chimera; and the billing information.  For how
          to configure the &cell-billing; to write into this database,
          see below.
        </para>
      </section>

    <section>
      <title>Configuring &dcache;</title>

      <para>
        In the setup of &dcache; 1.9.7 and newer, there is one main
        configuration file:
        <filename>/opt/d-cache/etc/dcache.conf</filename>. Initially, it
        contains only a reference to the <firstterm>layout</firstterm> of the
        instance, but is otherwise empty. That is because &dcache;
        stores default settings separately. These default configuration values
        can be viewed in the files in
        <filename class="directory">/opt/d-cache/share/default/</filename>.
      </para>
      <para>
        If any of the default configurations values needs to be changed, copy it
        to <filename>/opt/d-cache/etc/dcache.conf</filename> and update the
        value.
      </para>

      <important>
      <para>
        Do not update configuration values in the files in the default folders,
        since changes to these files will be overwritten by updates.
      </para>
      </important>

      <note>
        <para>
            As &dcache; no longer parses any of the configuration files from
            the shell scripts, the location of the &java; binary cannot be
            defined in the configuration files. &dcache; will search for &java;
            on the system path first; if it is found there, no further action
            is needed. If &java; is not on the system path, the environment
            variable <envar>JAVA_HOME</envar> to define the location of the
            &java; installation directory. Alternatively, the environment
            variable <envar>JAVA</envar> can be used to point to the &java;
            executable directly.
        </para>

        <para>
            If <envar>JAVA_HOME</envar> or <envar>JAVA</envar> cannot be
            defined as global environment variables
            in the operating system, then they can be defined in either
            <filename>/etc/default/dcache</filename> or <filename>/etc/dcache.env</filename>.
            These two files are sourced by the init script and allow
            <envar>JAVA_HOME</envar>, <envar>JAVA</envar> and
            <envar>DCACHE_HOME</envar> to be defined.
        </para>
      </note>
   </section>

    <section>
        <title>Grid user authorization</title>

        <para>
        For authorization of grid users the file
        <filename>/opt/d-cache/etc/dcache.kpwd</filename> is
        needed. You can simply copy the template that is in the same directory. Note that it may be generated from the standard
        <filename>/etc/grid-security/grid-mapfile</filename> with the
        tool <filename>grid-mapfile2dcache-kpwd</filename> which is
        distributed with the WLCG software.
      </para>
    </section>

    <section id="in-install-layout">
      <title>Defining services and domains</title>
      <para>
        Depending on your site, you may have requirements upon the doors that
        you want to configure and domains within which you want to organise
        them. Since release 1.9.7, &dcache; keeps lists of the domains and the
        services which are to be run within them in special files, so
        called <firstterm>layout-files</firstterm>. Layout files may also
        contain domain- and
        service-specific configuration values.
      </para>

      <para>
        There are several layout files in the layout directory, but only one
        of them is read by &dcache; when starting up. This layout file is
        configured in <filename>/opt/d-cache/etc/dcache.conf</filename> using
        the <varname>dcache.layout</varname> property. Keep in mind that the
        property value has to point to a file with the same name, suffixed by
        <literal>.conf</literal>. That means, if you set the
        <varname>dcache.layout</varname> property to <literal>mylayout</literal>,
        make sure that the file
        <filename>/opt/d-cache/etc/layouts/mylayout.conf</filename> exists.
      </para>

      <informalexample>
      <para>
        For a single node &dcache;
        setup, specify <varname>dcache.layout=single</varname> in
        <filename>/opt/d-cache/etc/dcache.conf</filename>. This
        will instruct dCache to read the layout file
        <filename>/opt/d-cache/etc/layouts/single.conf</filename> when starting
        up.
      </para>
      </informalexample>

      <para>
        Besides defining domains and the services to run in a domain,
        a layout can define domain- and service- specific configuration
        parameters.
        Any property that can be assigned in
        <filename>/opt/d-cache/etc/dcache.conf</filename> can be assigned a
        new value per domain or per service in the layout file.
      </para>

      <informalexample>
      <para>
        Consider the following segment of the <filename>head.conf</filename>
        layout file:
      </para>
      <programlisting>[gPlazmaDomain]
[gPlazmaDomain/gplazma]

[utilityDomain]
[utilityDomain/gsi-pam]
[utilityDomain/pinmanager]</programlisting>

      <para>
        A name in square brackets, without a forward-slash
        (<literal>/</literal>) defines a domain.
        For example, <literal>[gPlazmaDomain]</literal> defines a domain
        called &domain-gplazma;. A name in square brackets with
        a forward slash defines a service that is to run in a domain.
        For example, <literal>[gPlazmaDomain/gplazma]</literal> declares that
        the &cell-gplazma; service is to be run in the &domain-gplazma; domain.
        A domain must be defined if services are to run in that domain.
        The order that services are defined determines in which order the
        services are started; however, this order doesn't matter.
      </para>
      <para>
        Lines starting with a hash-symbol (<literal>#</literal>) are comments
        and will be ignored by &dcache;.
      </para>
      </informalexample>
    </section>

    <section>
      <title>Configuring pools</title>

      <para>
	&dcache; will need to write the files it keeps in pools.
	These pools are defined as services within &dcache;. Hence, they are added
	to the layout file of your &dcache; instance, like all other
	services.
      </para>

      <para>
	Consider the following example for adding a pool to a
	&dcache;-layout file:
      </para>

      <informalexample>
	<programlisting>[myPoolDomain/pool]
name=pool1
path=/san/pool1
waitForFiles=${path}/data
maxDiskSpace=2T</programlisting>

        <para>
	  The only required properties are <varname>name</varname> and
	  <varname>path</varname>.  The <varname>path</varname>
	  property defines in which directory files are to be stored.
	  The <varname>name</varname> defines the name of the
	  pool. With this name, the pool can also be addressed from
	  administrative components (see <xref
	  linkend="intouch-admin"/> or <xref linkend="intouch-web"
	  />).  The property <varname>waitForFiles</varname> instructs
	  the pool not to start up until the specified file or
	  directory is available.  This is recommended as it prevents
	  problems should the underlying storage be unavailable (e.g.,
	  if a RAID device is offline).  The property
	  <varname>maxDiskSpace</varname> defines the maximum disk
	  usage for the pool.
	 </para>

	 <para>
	  In the example above the name of the pool is
	  <literal>pool1</literal> and the pool has 2 Tebibytes.  The
	  example pool waits for the data directory (<filename
	  class='directory'>${path}/data</filename>) to become
	  available.  The <literal>${path}</literal> will expand to
	  the value of the <varname>path</varname> property, so the
	  example <varname>waitForFiles</varname> value is always
	  correct.
	</para>
      </informalexample>

      <para>
	Adding a pool to a configuration does not modify the pool or
	the data in it and can thus safely be undone or repeated.
      </para>

       <note>

        <title>Mind the Gap!</title>
	<para>
	  The default gap for poolsizes is 4GiB. This means you should
	  make a bigger pool than 4GiB otherwise you would have to
	  change this gap in the &dcache; admin tool. See the example
	  below.  See also <xref linkend="intouch-admin"/>.
	</para>

	<screen>&dc-prompt-local; <userinput>cd <replaceable>poolName</replaceable></userinput>
&dc-prompt-pool; <userinput>set gap 2G</userinput>
&dc-prompt-pool; <userinput>save</userinput></screen>
	  </note>

	  <para>
	    All configured components can now be started with
	    <userinput>dcache start</userinput>, for example:
	  </para>

        <screen>&prompt-root; <userinput>/opt/d-cache/bin/dcache start</userinput>
Starting lmDomain  Done (pid=7514)
Starting dCacheDomain  Done (pid=7574)
Starting pnfsDomain  Done (pid=7647)
Starting dirDomain  Done (pid=7709)
Starting adminDomain  Done (pid=7791)
Starting httpdDomain  Done (pid=7849)
Starting utilityDomain  Done (pid=7925)
Starting gPlazma-dcache-vmDomain  Done (pid=8002)
Starting infoProviderDomain  Done (pid=8081)
Starting dcap-dcache-vmDomain  Done (pid=8154)
Starting gridftp-dcache-vmDomain  Done (pid=8221)
Starting gsidcap-dcache-vmDomain  Done (pid=8296)
Starting dcache-vmDomain  Done (pid=8369)</screen>

      </section>

      <section>
    <title>Installing &dcache; on several nodes</title>

    <para>
        Installing &dcache; on several nodes is not much more complicated
        than installing it on a single node.
        Think about how &dcache; should be organised regarding services and
        domains. Then adapt the layout files, as
        described in <xref linkend="in-install-layout" />, to the layout that
        you have in mind. The files <filename>/opt/d-cache/etc/layouts/head.conf</filename>
        and <filename>/opt/d-cache/etc/layouts/pool.conf</filename> contain
        examples for a &dcache; head-node and a &dcache; pool respectively.
    </para>

    <tip>
        <para>
            On &dcache; nodes running only pool services you do not need to
            install &psql;. If your current node hosts only these services,
            the installation of &psql; can be skipped.
        </para>
    </tip>

    <note>
        <para>
          Neither &chimera; nor the pools need to be mounted anymore.
          Having a mount on the
          &chimera;/&nfs3;-server node itself is always a good idea as it
          eases maintenance.
        </para>
    </note>

    <para>
      On any other nodes than the head node, <varname>broker.host</varname>
      has to be added to
      <filename>/opt/d-cache/etc/dcache.conf</filename>.
      <varname>broker.host</varname> should point to the host running your
      &dcache; broker. Usually that is the host containing the special
      domain &domain-dcache;, because that domain acts implicitly as a
      broker.
    </para>

  </section>
  </section>

  <section id="in-upgrade">
    <title>Upgrading a &dcache; Instance</title>

    <para>
      Upgrading to bugfix releases within one version (e.g. from
      1.9.3-1 to 1.9.3-2) may be done by shutting down the server and
      upgrading the packages with

<screen>&prompt-root; <userinput>rpm -Uvh <replaceable>packageName</replaceable></userinput></screen>
    </para>
  </section>

</chapter>
