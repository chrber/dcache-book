<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN" "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd">

<chapter id="intouch">
  <!-- <?dbhtml dir="intouch"?> -->
  <title>Getting in Touch with dCache</title>
  
  <para>
    This section is a guide for exploring a newly installed <dcache/>
    system. The confidence obtained by this exploration will prove
    very helpful when encountering problems in the running
    system. This forms the basis for the more detailed stuff in the
    later parts of this book.
  </para>
  
  <para>
    The starting point is a fresh installation according to the
    installation instructions shipped with any distribution of
    <dcache/>. All components (<pnfs/>, <dcache/>-core,
    <dcache/>-pool, and <dcache/>-opt) are started on the same
    host. Additional pools on other hosts may also run.
  </para>

  <section id="intouch-client">
    <title>Checking the Functionality</title>

    <para>
      First, we will get used to the client tools. On the <dcache/>
      admin host, change into the <pnfs/> directory, where the users
      are going to store their data:

<screen><userprompt/><command>cd</command> <filename class="directory">/pnfs/<replaceable>site.de</replaceable>/data/</filename>
<userprompt/>
</screen>

      Note that on the <dcache/> admin node this directory is a link
      to <filename class="directory">/pnfs/fs/usr/</filename> and the
      NFS export <literal>localhost:/fs</literal> is mounted to
      <filename class="directory">/pnfs/fs</filename>. The NFS server
      running on the <dcache/> admin node is part of the <pnfs/> system
      which is not part of <dcache/> but heavily used by it.
    </para>

    <para>
      The <pnfs/> filesystem is not intended for reading or writing
      actual data with regular file operations via the NFS
      protocol. However, <literal>localhost:/fs</literal> is a
      special, privileged NFS export that allows reading and writing
      for administrative tasks. It should only be mounted by the admin
      node.
    </para>

    <para>
      Reading and writing data to and from a <dcache/> instance can be
      done with a number of protocols. After a standard installation,
      these protocols are <productname>dCap</productname>,
      <productname>gsiDCap</productname>, and
      <productname>GridFTP</productname>. In addition <dcache/> comes
      with an implementation of the <productname>SRM</productname>
      which negotiates the actual data transfer protocol.
    </para>

    <para>
      We will first try <productname>dCap</productname> with the
      <command>dccp</command> command:

<screen><userprompt/>PATH=/opt/d-cache/dcap/bin/:$PATH
<userprompt/><command>dccp</command> /bin/sh my-test-file
541096 bytes in 0 seconds</screen>

      This command succeeds if the user <literal>user</literal> has
      the unix rights to write to the current directory <filename
      class="directory">/pnfs/<replaceable>site.de</replaceable>/data/</filename>.
      The <productname>dCap</productname> protocol also hat a URL syntax:

<screen><userprompt/><command>dccp</command> dcap://<replaceable>adminNode</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test.tmp
541096 bytes in 0 seconds</screen>

      However, this command only succeeded because the file is world
      readable:

<screen><userprompt/><command>chmod</command> o-r my-test-file
<userprompt/><command>dccp</command> dcap://<replaceable>adminNode</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test2.tmp
Command failed!
Server error message for [1]: "Permission denied" (errno 2).
Failed open file in the dCache.
Can't open source file : "Permission denied"
System error: Input/output error</screen>

      This command did not succeed, because the dcap protocol is
      unauthenticated and the user is mapped to a non-existent user in
      order to determine the access rights. However, you should be
      able to access the file with the NFS mount:

<screen><userprompt/><command>dccp</command> my-test-file /tmp/test2.tmp
541096 bytes in 0 seconds</screen>
    </para>

    <para>
      If you have a valid grid proxy with a certificate subject which
      is properly mapped in the configuration file
      <filename>/opt/d-cache/etc/dcache.kpwd</filename> you can also
      try grid-authenticated access via GSI authenticated
      <productname>dCap</productname>:

<screen><userprompt/><command>chgrp</command> <replaceable>yourVO</replaceable> my-test-file
<userprompt/>LD_LIBRARY_PATH=/opt/d-cache/dcap/lib/:$LD_LIBRARY_PATH
<userprompt/><command>dccp</command> gsidcap://<replaceable>adminNode</replaceable>:22128/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test3.tmp
541096 bytes in 0 seconds</screen>

      Or we let the <productname>SRM</productname> negotiate the
      protocol:

<screen><userprompt/>PATH=/opt/d-cache/srm/bin/:$PATH
<userprompt/><command>srmcp</command> srm://<replaceable>adminNode</replaceable>:8443/pnfs/desy.de/data/my-test-file file:////tmp/test4.tmp
configuration file not found, configuring srmcp
created configuration file in ~/.srmconfig/config.xml</screen>

      If the <dcache/> instance is registered as a storage element in
      the LCG/EGEE grid and the LCG user interface software is
      available the file can be accessed via
      <productname>SRM</productname>:

<screen><userprompt/><command>lcg-cp</command> -v --vo <replaceable>yourVO</replaceable> \
srm://<replaceable>dCacheAdminFQN</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file \
file:///tmp/test5.tmp
Source URL: srm://<replaceable>dCacheAdminFQN</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
File size: 541096
Source URL for copy: gsiftp://<replaceable>dCacheAdminFQN</replaceable>:2811//pnfs/site.de/data/my-test-file
Destination URL: file:///tmp/test5.tmp
# streams: 1
Transfer took 770 ms</screen>

      and it can be deleted with the help of the <productname>SRM</productname>:

<screen><userprompt/><command>srm-advisory-delete</command> srm://<replaceable>dCacheAdminFQN</replaceable>:8443/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
 srmcp error :  advisoryDelete(User [name=...],pnfs/<replaceable>site.de</replaceable>/data/my-test-file) 
Error user User [name=...] has no permission to delete 000100000000000000BAF0C0</screen>

      This works only if the grid certificate subject is mapped to a
      user which has permissions to delete the file:

<screen><userprompt/><command>chown</command> <replaceable>yourVO</replaceable>001 my-test-file
<userprompt/><command>srm-advisory-delete</command> srm://<replaceable>dCacheAdminFQN</replaceable>:8443/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
</screen>
    </para>

    <para>
      If the grid functionality is not required the file can be
      deleted with the NFS mount of the <pnfs/> filesystem:

<screen><userprompt/><command>rm</command> <filename>my-test-file</filename>
</screen>
    </para>

  </section>

  <section id="intouch-web">
    <title>The Web Interface for Monitoring <dcache/></title>

    <para>
      In the standard configuration the dCache web interface is
      started on the admin node and can be reached via port
      <literal>2288</literal>. Point a web browser to <ulink
      url="http://adminNode:2288/">http://<replaceable>adminNode</replaceable>:2288/</ulink>
      to get to the main menue of the dCache web interface.  The
      contents of the web interface are self-explanatory and are the
      primary source for most monitoring and trouble-shooting tasks.
    </para>

    <para>
      The <quote>Cell Services</quote> page displays the status of
      some important <glossterm linkend="gl-cell">cells</glossterm> of
      the <dcache/> instance. You might observe that some cells are
      marked <quote>OFFLINE</quote> even though you know that they are
      running and fine. These might be the cells <quote>SRM</quote>,
      <quote>GFTP</quote>, and <quote>DCap-gsi</quote>. The reason is
      that the names of the cells monitored by the web interface are
      explicitly configured in the file
      <filename>/opt/d-cache/config/httpd.batch</filename>. However,
      the cells have other names. If you change the following section
      near the end of the file

<programlisting>#
create diskCacheV111.cells.WebCollectorV3 collector \
    "PnfsManager \
     PoolManager \
     GFTP \
     SRM \
     DCap-gsi \
     -replyObject"
#</programlisting>

      such that it reads

<programlisting>#
create diskCacheV111.cells.WebCollectorV3 collector \
    "PnfsManager \
     PoolManager \
     GFTP-<replaceable>adminNode</replaceable> \
     SRM-<replaceable>adminNode</replaceable> \
     DCap-gsi-<replaceable>adminNode</replaceable> \
     -replyObject"
#</programlisting>

      and restart the <literal>httpd</literal> <glossterm
      linkend="gl-domain">domain</glossterm> by executing

<screen><rootprompt/><command>/opt/d-cache/jobs/httpd</command> stop
<rootprompt/><command>/opt/d-cache/jobs/httpd</command> -logfile=/opt/d-cache/log/http.log start
</screen>

      More information about the
      <filename><replaceable>domainName</replaceable>.batch</filename>
      will follow in the next section.
    </para>

    <para>
      The <quote>Pool Usage</quote> page gives an good overview of the
      current space usage of the whole <dcache/> instance. In the
      graphs, free space is marked yellow, space occupied by
      <glossterm linkend="gl-cached_file">cached files</glossterm>
      which may be deleted when space is needed is marked green, and
      space occupied by <glossterm linkend="gl-precious_file">precious
      files</glossterm> which cannot be deleted. Other states
      (e.g. files which are currently written) are marked purple.
    </para>

    <para>
      The page <quote>Pool Request Queues</quote> (or <quote>Pool
      Transfer Queues</quote>) gives information about the number
      current requests handled by each pool. <quote>Actions
      Log</quote> keeps track of all the transfers performed by the
      pools up to now.
    </para>

    <para>
      The remaining pages are only relevant with more advanced
      configurations: The page <quote>Pools</quote> (or <quote>Pool
      Attraction Configuration</quote>) can be used to analyze the
      current configuration of the <glossterm
      linkend="gl-pm-comp-psu">pool selection unit</glossterm> in the
      pool manager. The remaining pages are relevant only if a
      <glossterm linkend="gl-tss">tertiary storage system (HSM)</glossterm>
      is connected to the <dcache/> instance.
    </para>

  </section>

  <section id="intouch-files">
    <title>Files</title>

    <para>
      In this section we will have a look at the configuration and log
      files of <pnfs/> and <dcache/>. 
    </para>

    <para>
      The primary configuration file for <pnfs/> is
      <filename>/usr/etc/pnfsSetup</filename>. For administrative
      tasks in <pnfs/> it has to be sourced and the path to the
      admninistration tools should be added to <varname>$PATH</varname>:

<screen><rootprompt/>. /usr/etc/pnfsSetup
<rootprompt/>PATH=$PATH:$pnfs/tools
</screen>

      Stopping and starting the <pnfs/> server can now be done with

<screen><rootprompt/><command>pnfs</command> stop
<rootprompt/><command>pnfs</command> start
</screen>

      The log files for the three <pnfs/> server daemons are
      <filename>/var/log/pmountd.log</filename>,
      <filename>/var/log/dbserver.log</filename>, and
      <filename>/var/log/pnfsd.log</filename>. The
      <command>pnfsd</command> is the NFS server implementation. Its
      log file contains one line for each NFS filesystem
      operation. The result code of that operation can be found at the
      end of the line.
    </para>

    <para>
      The <command>pnfsd</command> daemons communicate with the
      <command>dbserver</command> daemons via a shared memory
      area. These perform the actual operations on the database files
      which can be found in <filename
      class="directory">/opt/pnfsdb/pnfs/databases/</filename>. The
      <pnfs/> system finds them via the information in the directory
      <filename
      class="directory">/opt/pnfsdb/pnfs/info/</filename>. 
    </para>

    <para>
      Each database file is handled by one <command>dbserver</command>
      daemon and each access will lock the database file. Each file
      stores the filesystem for one directory sub-tree similar to
      mount points in the UNIX filesystem. The
      <literal>admin</literal> database contains the root (the one
      mounted on <filename class="directory">/pnfs/fs/</filename>) and
      the <literal>data1</literal> database the subdirectory <filename
      class="directory">usr/data/</filename>. <xref
      linkend="cf-pnfs-db"/> describes how to create new databases.
    </para>

    <para>
      The <dcache/> software is installed in one directory, normally
      <filename class="directory">/opt/d-cache/</filename>.  All
      configuration and log files can be found here. In the following
      filenames will always be relative to this directory.
    </para>
    
    <para>
      In the previous section we have already seen how a <glossterm
      linkend="gl-domain">domain</glossterm> is restarted:
      
<screen><rootprompt/><command>/opt/d-cache/jobs/<replaceable>domainName</replaceable></command> stop
<rootprompt/><command>/opt/d-cache/jobs/<replaceable>domainName</replaceable></command> \
-logfile=/opt/d-cache/log/<replaceable>domainName</replaceable>.log start
</screen>

      Listing the contents of <filename
      class="directory">jobs/</filename> you will see that these
      scripts are all in fact links to
      <filename>jobs/wrapper2.sh</filename>. This script will
      determine the domain to start by its basename. From the above
      commands you can already read of the standard location for the
      log files.
    </para>

    <para>
      The files
      <filename>config/<replaceable>domainName</replaceable>Setup</filename>
      will be used by the wrapper script and by the domains at
      start-up. These files are also links to
      <filename>config/dCacheSetup</filename>. This is the primary
      configuration file of the <dcache/> system. The only files which
      are different for each domain are
      <filename>config/<replaceable>domainName</replaceable>.batch</filename>.
      They describe which cells are started in the domains. Normally,
      changes in these files should not be necessary. Since they will
      be overwritten when updating to a newer version (e.g. with RPM),
      you should make a copy of it and modify that. By that you give
      the domain a new name. The necessary links can be created with

<screen><rootprompt/><command>cd</command> /opt/d-cache/config/
<rootprompt/><command>../jobs/initPackage.sh</command></screen>

      Then the old domain can be stopped and the new one started:

<screen><rootprompt/><command>/opt/d-cache/jobs/<replaceable>domainName</replaceable></command> stop
<rootprompt/><command>/opt/d-cache/jobs/<replaceable>newDomainName</replaceable></command> \
-logfile=/opt/d-cache/log/<replaceable>newDomainName</replaceable>.log start</screen>
    </para>

    <para>
      The most central component of a <dcache/> instance is the
      <literal>PoolManager</literal> cell. It reads additional
      configuration information from the file
      <filename>config/PoolManager.config</filename> at
      start-up. However, in contrast to the
      <filename>config/<replaceable>domainName</replaceable>Setup</filename>
      files, it is not necessary to restart the domain when changing
      the file. We will see an example of this below.
    </para>

    <para>
      The domains containing the pools are handled a little
      differently from the other domains. They are started by

<screen><rootprompt/><command>/opt/d-cache/jobs/pool</command> -pool=<replaceable>pool</replaceable> \
-logfile=log/<replaceable>pool</replaceable>Domain.log start</screen>

      This will start the domain
      <literal><replaceable>pool</replaceable>Domain</literal> which
      contains a pool cell for each line in the file
      <filename>config/<replaceable>pool</replaceable>.poollist</filename>. When
      only one pool domain is started on a host, it is conventional to
      use the hostname for <replaceable>pool</replaceable>. A line of
      the
      <filename><replaceable>pool</replaceable>.poollist</filename>
      has the form

<programlisting><replaceable>poolName</replaceable> <replaceable>poolDir</replaceable> <replaceable>options</replaceable></programlisting>

      For the pool names it is conventional to use
      <literal><replaceable>hostname</replaceable>_<replaceable>n</replaceable></literal>
      (where <replaceable>n</replaceable> starts at 1).
    </para>

    <para>
      Similar to <filename>config/PoolManager.conf</filename>, the
      pools read their configuration from
      <filename><replaceable>poolDir</replaceable>/pool/setup</filename>
      at startup.
    </para>

  </section>


  <unfinished>
    <para>
      <filename>certificate</filename>
      <filename>CAs</filename>
      <filename>kpwd</filename>
      chmod 600 srm.batch utility.batch
    </para>
    
    <para>
      admin interface, change passwd, info, pinboard, rc ls, rep ls, cm ls -r, unwatch, watch, create pool group, save, restore
    </para>

    <para>
      <pnfs/> IDs,  check wormholes and tags, wormholes have to be rewritten after fset io
    </para>

    <para>
      Check pnfs mount permissions and restrict to localhost.
    </para>

    <para>
      When a file in the <pnfs/> filesystem is deleted the server
      stores information about is in the subdirectories of <filename
      class="directory">/opt/pnfsdb/pnfs/trash/</filename>. The
      <literal>cleaner</literal> cell in the
      <literal>pnfsDomain</literal> is responsible for deleting the
      actual files from the pools asyncronously. It uses the files in
      the directory <filename
      class="directory">/opt/pnfsdb/pnfs/trash/2/</filename>. It
      contains a file with the <pnfs/> ID of the deleted file as
      name. If a pool containing that file is down at the time the
      cleaner tries to remove it, it will retry for a while. After
      that the file
      <filename>/opt/pnfsdb/pnfs/trash/2/current/failed.<replaceable>poolName</replaceable></filename>
      will contain the <pnfs/> IDs which have not been removed from
      that pool. The cleaner will still retry the removal with a lower
      frequency.
    </para>


  </unfinished>
  
</chapter>
