<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % sharedents SYSTEM "shared-entities.xml" >
<!ENTITY new-user   "<systemitem class='username'>&simple-new-user;</systemitem>">
<!ENTITY simple-new-user   "<replaceable>new-user</replaceable>">
<!ENTITY admin   "<systemitem class='admin'>admin</systemitem>">
<!ENTITY root   "<systemitem class='root'>root</systemitem>">

%sharedents;
]>

<chapter id="intouch">

  <title>Getting in Touch with &dcache;</title>

  <para>
    This section is a guide for exploring a newly installed &dcache;
    system. The confidence obtained by this exploration will prove
    very helpful when encountering problems in the running
    system. This forms the basis for the more detailed stuff in the
    later parts of this book. The starting point is a fresh
    installation according to the <xref
    linkend="in-install">installation instructions</xref>.
  </para>

  <section id="intouch-client">
    <title>Checking the Functionality</title>

    <para>
      First, we will get used to the client tools. On the &dcache;
      head node, change into the &pnfs; directory, where the users
      are going to store their data:
    </para>

    <screen>&prompt-user; <userinput>cd /pnfs/<replaceable>site.de</replaceable>/data/</userinput>
&prompt-user;</screen>

    <para>
      The mounted &chimera; filesystem is not intended for reading or
      writing actual data with regular file operations via the NFS
      protocol.
    </para>

    <para>
      Reading and writing data to and from a &dcache; instance can be
      done with a number of protocols. After a standard installation,
      these protocols are &dcap;, &gsidcap;, and &gridftp;. In
      addition &dcache; comes with an implementation of the &srm;
      protocol which negotiates the actual data transfer protocol.
    </para>

    <para>
      We will first try &dcap; with the <command>dccp</command>
      command:
    </para>

    <screen>&prompt-user; <userinput>export PATH=/opt/d-cache/dcap/bin/:$PATH</userinput>
&prompt-user; <userinput>cd /pnfs/<replaceable>site.de</replaceable>/data/</userinput>
&prompt-user; <userinput>dccp /bin/sh my-test-file</userinput>
541096 bytes in 0 seconds</screen>

    <para>
      This command succeeds if the user <systemitem
      class="username">user</systemitem> has the Unix rights to write
      to the current directory <filename
      class="directory">/pnfs/<replaceable>site.de</replaceable>/data/</filename>.
    </para>

    <para>
      The dccp command also accepts URLs.  We can copy the data back
      using the dccp command and the &dcap; protocol but this time
      describing the location of the file using a URL.
    </para>

    <screen>&prompt-user; <userinput>dccp dcap://<replaceable>adminNode</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test.tmp</userinput>
541096 bytes in 0 seconds</screen>

    <para>
      However, this command only succeeds if the file is world
      readable.  The following shows how to ensure the file is
      <emphasis>not</emphasis> world readable and illustrates dccp
      consequently failing to copy the file.
    </para>

    <screen>&prompt-user; <userinput>chmod o-r my-test-file</userinput>
&prompt-user; <userinput>dccp dcap://<replaceable>adminNode</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test2.tmp</userinput>
Command failed!
Server error message for [1]: "Permission denied" (errno 2).
Failed open file in the dCache.
Can't open source file : "Permission denied"
System error: Input/output error</screen>

    <para>
      This command did not succeed, because &dcap; access is
      unauthenticated and the user is mapped to a non-existent user in
      order to determine the access rights. However, you should be
      able to access the file with the NFS mount:
    </para>

    <screen>&prompt-user; <userinput>dccp my-test-file /tmp/test2.tmp</userinput>
541096 bytes in 0 seconds</screen>

    <para>
      If you have a valid grid proxy with a certificate subject which
      is properly mapped in the configuration file
      <filename>/opt/d-cache/etc/dcache.kpwd</filename> you can also
      try grid-authenticated access via the GSI-authenticated version
      of &dcap;:
    </para>

    <screen>&prompt-user; <userinput>chgrp <replaceable>yourVO</replaceable> my-test-file</userinput>
&prompt-user; <userinput>export LD_LIBRARY_PATH=/opt/d-cache/dcap/lib/:$LD_LIBRARY_PATH</userinput>
&prompt-user; <userinput>dccp gsidcap://<replaceable>adminNode</replaceable>:22128/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test3.tmp</userinput>
541096 bytes in 0 seconds</screen>

    <para>
      Or we let the &srm; negotiate the protocol:
    </para>

    <screen>&prompt-user; <userinput>export PATH=/opt/d-cache/srm/bin/:$PATH</userinput>
&prompt-user; <userinput>srmcp srm://<replaceable>adminNode</replaceable>:8443/pnfs/desy.de/data/my-test-file file:////tmp/test4.tmp</userinput>
configuration file not found, configuring srmcp
created configuration file in ~/.srmconfig/config.xml</screen>

    <para>
      If the &dcache; instance is registered as a storage element in
      the LCG/EGEE grid and the LCG user interface software is
      available the file can be accessed via &srm;:
    </para>

    <screen>&prompt-user; <userinput>lcg-cp -v --vo <replaceable>yourVO</replaceable> \
srm://<replaceable>dCacheAdminFQN</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file \
file:///tmp/test5.tmp</userinput>
Source URL: srm://<replaceable>dCacheAdminFQN</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
File size: 541096
Source URL for copy: gsiftp://<replaceable>dCacheAdminFQN</replaceable>:2811//pnfs/site.de/data/my-test-file
Destination URL: file:///tmp/test5.tmp
# streams: 1
Transfer took 770 ms</screen>

    <para>
      and it can be deleted with the help of the &srm; interface:
    </para>

    <screen>&prompt-user; <userinput>srm-advisory-delete srm://<replaceable>dCacheAdminFQN</replaceable>:8443/pnfs/<replaceable>site.de</replaceable>/data/my-test-file</userinput>
 srmcp error :  advisoryDelete(User [name=...],pnfs/<replaceable>site.de</replaceable>/data/my-test-file)
Error user User [name=...] has no permission to delete 000100000000000000BAF0C0</screen>

    <para>
      This works only if the grid certificate subject is mapped to a
      user which has permissions to delete the file:
    </para>

    <screen>&prompt-user; <userinput>chown <replaceable>yourVO</replaceable>001 my-test-file</userinput>
&prompt-user; <userinput>srm-advisory-delete srm://<replaceable>dCacheAdminFQN</replaceable>:8443/pnfs/<replaceable>site.de</replaceable>/data/my-test-file</userinput></screen>

    <para>
      If the grid functionality is not required the file can be
      deleted with the NFS mount of the &pnfs; filesystem:
    </para>

    <screen>&prompt-user; <userinput>rm <filename>my-test-file</filename></userinput></screen>

  </section>



  <section id="intouch-web">
    <title>The Web Interface for Monitoring &dcache;</title>

    <para>
      In the standard configuration the &dcache; web interface is
      started on the head node and can be reached via port
      <systemitem class="resource">2288</systemitem>. Point a web
      browser to <ulink
      url="http://headNode:2288/">http://<replaceable>adminNode</replaceable>:2288/</ulink>
      to get to the main menue of the dCache web interface.  The
      contents of the web interface are self-explanatory and are the
      primary source for most monitoring and trouble-shooting tasks.
    </para>

    <para>
      The <quote>Cell Services</quote> page displays the status of
      some important <glossterm linkend="gl-cell">cells</glossterm> of
      the &dcache; instance. You might observe that some cells are
      marked <quote>OFFLINE</quote>. In general &dcache; has no
      knowledge about which cells are supposed to be online, but for
      purposes of monitoring, some cells may be hard coded in the file
      <filename>/opt/d-cache/config/httpd.batch</filename>:
    </para>

    <programlisting>#
create diskCacheV111.cells.WebCollectorV3 collector \
    "PnfsManager \
     PoolManager \
     -loginBroker=LoginBroker,srm-LoginBroker \
     -replyObject"
#</programlisting>

    <para>
      Additional cells may be added. To take effect, the &domain-http;
      <glossterm linkend="gl-domain">domain</glossterm> must be
      restarted by executing
    </para>

    <screen>&prompt-root; <userinput>/opt/d-cache/bin/dcache restart httpd</userinput></screen>

    <para>
      More information about the
      <filename><replaceable>domainName</replaceable>.batch</filename>
      will follow in the next section.
    </para>

    <para>
      The <quote>Pool Usage</quote> page gives a good overview of the
      current space usage of the whole &dcache; instance. In the
      graphs, free space is marked yellow, space occupied by
      <glossterm linkend="gl-cached">cached files</glossterm> (which
      may be deleted when space is needed) is marked green, and space
      occupied by <glossterm linkend="gl-precious">precious
      files</glossterm>, which cannot be deleted. Other states
      (e.g., files which are currently written) are marked purple.
    </para>

    <para>
      The page <quote>Pool Request Queues</quote> (or <quote>Pool
      Transfer Queues</quote>) gives information about the number
      current requests handled by each pool. <quote>Actions
      Log</quote> keeps track of all the transfers performed by the
      pools up to now.
    </para>

    <para>
      The remaining pages are only relevant with more advanced
      configurations: The page <quote>Pools</quote> (or <quote>Pool
      Attraction Configuration</quote>) can be used to analyze the
      current configuration of the <glossterm
      linkend="gl-pm-comp-psu">pool selection unit</glossterm> in the
      pool manager. The remaining pages are relevant only if a
      <glossterm linkend="gl-tss">tertiary storage system
      (HSM)</glossterm> is connected to the &dcache; instance.
    </para>
  </section>


  <section id="intouch-files">
    <title>Files</title>

    <para>
      In this section we will have a look at the configuration and log
      files of &dcache;.
    </para>

    <para>
      The &dcache; software is installed in one directory, normally
      <filename class="directory">/opt/d-cache/</filename>.  All
      configuration files can be found here. In the following relative
      filenames will always be relative to this directory.
    </para>

    <para>
      In the previous section we have already seen how a <glossterm
      linkend="gl-domain">domain</glossterm> is restarted:
    </para>

    <screen>&prompt-root; <userinput>/opt/d-cache/bin/dcache restart <replaceable>domainName</replaceable></userinput></screen>

    <para>
      Log files of domains are by default stored in
      <filename>/var/log/<replaceable>domainName</replaceable>.log</filename>. We
      strongly encourage to configure logrotate to rotate the &dcache;
      log files to avoid filling up the log file system. This can
      typically be achieved by creating the file
      <filename>/etc/logrotate.d/dcache</filename> with the following
      content:
    </para>

    <programlisting>/var/log/*Domain.log {
    compress
    rotate 100
    missingok
    copytruncate
}</programlisting>

    <para>
      The files
      <filename>config/<replaceable>domainName</replaceable>Setup</filename>
      contain configuration parameters for the domain. These files are
      typically symbolic links to
      <filename>config/dCacheSetup</filename>. This is the primary
      configuration file of &dcache;.
    </para>

    <para>
      The only files which are different for each domain are
      <filename>config/<replaceable>domainName</replaceable>.batch</filename>.
      They describe which <glossterm
      linkend="gl-cell">cells</glossterm> are started in the
      <glossterm linkend="gl-domain">domains</glossterm>. Normally,
      changes in these files should not be necessary. However, if you
      need to change something, consider the following:
    </para>

    <para>
      Since the standard
      <filename>config/<replaceable>domainName</replaceable>.batch</filename>
      files will be overwritten when updating to a newer version of
      &dcache; (e.g. with RPM), it is a good idea to modify only
      private copies of them.  When choosing a name like
      <filename>config/<replaceable>newDomainName</replaceable>.batch</filename>
      you give the domain the name
      <replaceable>newDomainName</replaceable>. The necessary links
      can be created with
    </para>

    <screen>&prompt-root; <userinput>cd /opt/d-cache/config/</userinput>
&prompt-root; <userinput>../jobs/initPackage.sh</userinput></screen>

    <para>
      Then the old domain can be stopped and the new one started:
    </para>

    <screen>&prompt-root; <userinput>/opt/d-cache/bin/dache stop <replaceable>domainName</replaceable></userinput>
&prompt-root; <userinput>/opt/d-cache/bin/dcache start <replaceable>newDomainName</replaceable></userinput></screen>

    <para>
      More details about domains and cells can be found in <xref
        linkend="cf-cellpackage"/>.
    </para>

    <para>
      The most central component of a &dcache; instance is the
      &cell-poolmngr; cell. It reads additional configuration
      information from the file
      <filename>config/PoolManager.conf</filename> at
      start-up. However, in contrast to the
      <filename>config/<replaceable>domainName</replaceable>Setup</filename>
      files, it is not necessary to restart the domain when changing
      the file. We will see an example of this below.
    </para>

    <para>
      Similar to <filename>config/PoolManager.conf</filename>,
      pools read their configuration from
      <filename><replaceable>poolDir</replaceable>/pool/setup</filename>
      at startup.
    </para>

    <!--
        TODO:
        <para>
        <filename>certificate</filename>
        <filename>CAs</filename>
        <filename>kpwd</filename>
        </para>
      -->
  </section>



  <section id="intouch-admin">
    <title>The Admin Interface</title>

    <note>
      <para>
	If you attempt to log into the admin interface without
	generating the &ssh;-keys you will get an error message.
      </para>
      <screen>&prompt-user; <userinput>ssh -c blowfish -p 22223 -l admin headnode.example.org</userinput>
Connection closed by 192.0.2.11</screen>
      <para>
	See <xref linkend="in-install-ssh-keys" />.
      </para>
    </note>


    <para>
      &dcache; has a powerful administration interface. It is accessed
      with the &ssh; protocol. The server is part of the
      &domain-adminDoor; domain. Connect to it with
    </para>

    <screen>&prompt-user; <userinput>ssh -c blowfish -p 22223 -l admin headnode.example.org</userinput></screen>

    <para>
      The initial password is
      <quote><literal>dickerelch</literal></quote> (which is German
      for <quote>fat elk</quote>) and you will be greeted by the
      prompt
    </para>

    <screen>   dCache Admin (VII) (user=admin)


&dc-prompt-local;</screen>


    <para>
      The password can now be changed with
    </para>

    <screen>&dc-prompt-local; <userinput>cd acm</userinput>
&dc-prompt-acm; <userinput>create user admin</userinput>
&dc-prompt-acm; <userinput>set passwd -user=admin <replaceable>newPasswd</replaceable> <replaceable>newPasswd</replaceable></userinput>
&dc-prompt-acm; <userinput>..</userinput>
&dc-prompt-local; <userinput>logoff</userinput></screen>

    <para>
      This already illustrates how to navigate within the
      administration interface: Starting from the local prompt
      (&dc-prompt-local;) the command <command>cd</command> takes you
      to the specified <glossterm linkend="gl-cell">cell</glossterm>
      (here &cell-acm;, the access control manager). There
      two commands are executed. The escape sequence
      <command>..</command> takes you back to the local prompt and
      <command>logoff</command> exits the admin shell.
    </para>

    <para>
      Note that <command>cd</command> only works from the local
      prompt. If the cell you are trying to access does not exist, the
      <command>cd</command> command will not complain.  However,
      trying to execute any command subsequently will result in an
      error message <quote>No Route to cell...</quote>.  Type
      <literal>..</literal> to return to the &dc-prompt-local; prompt.
    </para>

     <para>
      To create a new user, &new-user;, set a new password and to give him/her an access to a particular cell (for example to the &cell-poolmngr;) run following command sequence:
     </para>

     <screen>&dc-prompt-local; <userinput>cd acm</userinput>
&dc-prompt-acm; <userinput>create user &simple-new-user;</userinput>
&dc-prompt-acm; <userinput>create acl cell.PoolManager.execute</userinput>
&dc-prompt-acm; <userinput>add access -allowed cell.PnfsManager.execute </userinput></screen>


     <para>
      Now you can check the permissions by:
     </para>

     <screen>&dc-prompt-acm; <userinput>check cell.PnfsManager.execute &simple-new-user;</userinput>
Allowed
&dc-prompt-acm; <userinput>show acl cell.PnfsManager.execute &simple-new-user;</userinput>
&lt;noinheritance&gt;
&lt;new-user&gt; -&gt; true</screen>


     <para>
      Following commands allow to a particular user an access to every cell:
     </para>

     <screen>&dc-prompt-acm; <userinput> create acl cell.*.execute</userinput>
&dc-prompt-acm; <userinput>add access -allowed cell.*.execute &simple-new-user;</userinput></screen>

     <para>
      To make an user as powerful as &admin; (&dcache;'s equivalent to the &root; user):
     </para>

     <screen>&dc-prompt-acm; <userinput>create acl *.*.*</userinput>
&dc-prompt-acm; <userinput>add access -allowed *.*.* &simple-new-user;</userinput></screen>


    <para>
      All cells know the commands <command>info</command> for general
      information about the cell and <command>show pinboard</command>
      for listing the last lines of the <glossterm
      linkend="gl-pinboard">pinboard</glossterm> of the cell. The
      output of these commands contains useful information for solving
      problems. It is a good idea to get aquainted with the normal
      output in the following cells: &cell-poolmngr;, &cell-pnfsmngr;,
      and the pool cells (e.g., &cell-pool-eg;).
    </para>
    
    <para>
       If you want to find out which cells are running on a certain domain,
       you can issue the command <command>ps</command> in the &cell-system;
       cell of the domain. For example, if you want to list the cells running
       on the &domain-adminDoor;, <command>cd</command> to its &cell-system;
       cell and issue the <command>ps</command> command.
    </para>

    <screen>&dc-prompt-local; <userinput>cd System@adminDoorDomain</userinput>
&dc-prompt-admindoor; <userinput>ps</userinput>
  Cell List
------------------
acm
alm
skm
c-dCacheDomain-101-102
System
c-dCacheDomain-101
RoutingMgr
alm-admin-103
pam
lm</screen>

    <para>
    The cells in the domain can be accessed using <command>cd</command>
    together with the cell-name scoped by the domain-name. So first, one has to
    get back to the local prompt, as the <command>cd</command> command will not
    work otherwise.
    </para>

    <screen>&dc-prompt-admindoor; <userinput>..</userinput>
&dc-prompt-local; <userinput>cd skm@adminDoorDomain</userinput>
&dc-prompt-skmadmin;</screen>

    <note>
    <para>
    If the cells are <firstterm>well-known</firstterm>, they can be accessed
    without adding the domain-scope. See <xref linkend="cf-cellpackage" /> for
    more information.
    </para>
    </note>

    <para>
        The domains that are running on the &dcache;-instance, can be viewed
        in the layout-configuration (see <xref linkend="in" />). Additionally,
        there is the &cell-topo; cell, which keeps track of the instance's
        domain topology. If it is running, it can be used to obtain the list of
        domains the following way:
    </para>

    <screen>&dc-prompt-local; <userinput>cd topo</userinput>
&dc-prompt-topo; <userinput>ls</userinput>
dirDomain
infoDomain
adminDoorDomain
spacemanagerDomain
utilityDomain
gPlazmaDomain
nfsDomain
dCacheDomain
httpdDomain
statisticsDomain
namespaceDomain</screen>

    <note>
    <para>
	The &cell-topo; cell rescans periodically which domains are running, so
       it can take some time until <command>ls</command> displays the full
       domain list.
    </para>
    </note>

    <para>
      There also is the command <command>help</command> for listing all
      commands the cell knows and their parameters. However, many of the
      commands are only used for debugging and
      development purposes. Only commands described in this documentation
      should be used for the administration of a &dcache; system.
    </para>

    <para>
      The most useful command of the pool cells is <xref
        linkend="cmd-rep_ls"/>. It lists the files which are stored in the pool
      by their &pnfs; IDs:
    </para>

    <screen>000100000000000000001120 &lt;-P---------(0)[0]&gt; 485212 si={myStore:STRING}
000100000000000000001230 &lt;C----------(0)[0]&gt; 1222287360 si={myStore:STRING}</screen>

    <para>
      Each file in a pool has one of the 4 primary states:
      <quote>cached</quote> (<literal>&lt;C---</literal>),
      <quote>precious</quote> (<literal>&lt;-P--</literal>),
      <quote>from client</quote> (<literal>&lt;--C-</literal>), and
      <quote>from store</quote> (<literal>&lt;---S</literal>).
    </para>

    <para>
      Two commands in the pool manager are quite useful: <xref
      linkend="cmd-rc_ls"/> lists the requests currently handled by
      the pool manager. A typical line of output for a read request
      with an error condition is (all in one line):
    </para>

    <screen>000100000000000000001230@0.0.0.0/0.0.0.0 m=1 r=1 [&lt;unknown&gt;]
  [Waiting 08.28 19:14:16]
  {149,No pool candidates available or configured for 'staging'}</screen>

    <para>
      As the error message at the end of the line indicates, no pool
      was found containing the file and no pool could be used for
      staging the file from a tertiary storage system.
    </para>

    <para>
      Finally, <xref linkend="cmd-cm_ls"/> with the option
      <option>-r</option> gives the information about the pools
      currently stored in the cost module of the pool manager. A
      typical output is:
    </para>

    <!-- TODO: following is too long, needs breaking up -->
    <screen>&dc-prompt-pm; <userinput>cm ls <option>-r</option></userinput>
<replaceable>poolName1</replaceable>={R={a=0;m=2;q=0};S={a=0;m=2;q=0};M={a=0;m=100;q=0};PS={a=0;m=20;q=0};PC={a=0;m=20;q=0};
    <lineannotation>(...continues...)</lineannotation>   SP={t=2147483648;f=924711076;p=1222772572;r=0;lru=0;{g=20000000;b=0.5}}}
<replaceable>poolName1</replaceable>={Tag={{hostname=<replaceable>hostname</replaceable>}};size=0;SC=0.16221282938326134;CC=0.0;}
<replaceable>poolName2</replaceable>={R={a=0;m=2;q=0};S={a=0;m=2;q=0};M={a=0;m=100;q=0};PS={a=0;m=20;q=0};PC={a=0;m=20;q=0};
    <lineannotation>(...continues...)</lineannotation>   SP={t=2147483648;f=2147483648;p=0;r=0;lru=0;{g=4294967296;b=250.0}}}
<replaceable>poolName2</replaceable>microcebus_2={Tag={{hostname=<replaceable>hostname</replaceable>}};size=0;SC=2.7939677238464355E-4;CC=0.0;}</screen>

    <para>
      While the first line for each pool gives the information stored
      in the cache of the cost module, the second line gives the costs
      (SC: <glossterm linkend="gl-space_cost">space cost</glossterm>,
      CC: <glossterm linkend="gl-performance_cost">performance
      cost</glossterm>) calculated for a (hypothetical) file of zero
      size. For details on how these are calculated and their meaning,
      see <xref linkend="cf-pm-cm"/>.
    </para>

    <para>
      The &ssh; admin interface can be used non-interactively by
      scripts. For this the &dcache;-internal &ssh; server uses
      public/private key pairs.
    </para>

    <para>
      The file <filename>config/authorized_keys</filename> contains
      one line per user. The file has the same format as
      <filename>~/.ssh/authorized_keys</filename> which is used by
      <command>sshd</command>. The keys in
      <filename>config/authorized_keys</filename> have to be of type
      RSA1 as &dcache; only supports SSH protocol 1. Such a key is
      generated with
    </para>

    <screen>&prompt-user; <userinput>ssh-keygen -t rsa1 -C 'SSH1 key of <replaceable>user</replaceable>'</userinput>
Generating public/private rsa1 key pair.
Enter file in which to save the key (/home/<replaceable>user</replaceable>/.ssh/identity):
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/<replaceable>user</replaceable>/.ssh/identity.
Your public key has been saved in /home/<replaceable>user</replaceable>/.ssh/identity.pub.
The key fingerprint is:
c1:95:03:6a:66:21:3c:f3:ee:1b:8d:cb:46:f4:29:6a SSH1 key of <replaceable>user</replaceable></screen>

    <para>
      The passphrase is used to encrypt the private key (now stored in
      <filename>/home/<replaceable>user</replaceable>/.ssh/identity</filename>). If
      you do not want to enter the passphrase every time the private
      key is used, you can use <command>ssh-add</command> to add it to
      a running <command>ssh-agent</command>. If no agent is running
      start it with
    </para>

    <screen>&prompt-user; <userinput>if [ -S $SSH_AUTH_SOCK ] ; then echo "Already running" ; else eval `ssh-agent` ; fi</userinput></screen>

    <para>
      and add the key to it with
    </para>

    <screen>&prompt-user; <userinput>ssh-add</userinput>
Enter passphrase for SSH1 key of <replaceable>user</replaceable>:
Identity added: /home/<replaceable>user</replaceable>/.ssh/identity (SSH1 key of <replaceable>user</replaceable>)</screen>

    <para>
      Now, insert the public key
      <filename>~/.ssh/identity.pub</filename> as a separate line into
      <filename>config/authorized_keys</filename>. The comment field
      in this line <quote>SSH1 key of
      <replaceable>user</replaceable></quote> has to be changed to the
      &dcache; user name. An example file is:
    </para>

    <programlisting>1024 35 141939124<lineannotation>(... many more numbers ...)</lineannotation>15331 admin</programlisting>

    <para>
      The key manager within &dcache; will read this file every minute.
    </para>

    <para>
      Now, the ssh program should not ask for a password anymore. This
      is still quite secure, since the unencrypted private key is only
      held in the memory of the <command>ssh-agent</command>. It can
      be removed from it with
    </para>

    <screen>&prompt-user; <userinput>ssh-add -d</userinput>
Identity removed: /home/<replaceable>user</replaceable>/.ssh/identity (RSA1 key of <replaceable>user</replaceable>)</screen>

    <para>
      In scripts, one can use a <quote>Here Document</quote> to list
      the commands, or supply them to <command>ssh</command> as
      standard-input (stdin).  The following demonstrates using a Here
      Document:
    </para>

      <programlisting>#!/bin/sh
#
#  Script to automate dCache administrative activity

outfile=/tmp/$(basename $0).$$.out

ssh -c blowfish -p 22223 admin@<replaceable>adminNode</replaceable> &gt; $outfile &lt;&lt; EOF
cd PoolManager
cm ls -r
<lineannotation>(more commands here)</lineannotation>
logoff
EOF</programlisting>

    <para>
      or, the equivalient as stdin.
    </para>

    <programlisting>#!/bin/bash
#
#   Script to automate dCache administrative activity.

echo -e 'cd <replaceable>pool_1</replaceable>\nrep ls\n<lineannotation>(more commands here)</lineannotation>\nlogoff' \
  | ssh -c blowfish -p 22223 admin@<replaceable>adminNode</replaceable> \
  | tr -d '\r' &gt; rep_ls.out</programlisting>

  </section>

  <section id="intouch-gui">
  <title>The Graphical User Interface</title>

    <para>
      Instead of using <command>ssh</command> to access the admin
      interface, the &dcache; graphical user interface can be used. If
      it is not included in the &dcache; distribution, it can be
      downloaded from the <ulink url="http://www.dcache.org/">&dcache;
      homepage</ulink>. It is started by
    </para>

    <screen>&prompt-user; <userinput>java -jar org.pcells.jar</userinput></screen>

    <para>
      First, a new session has to be created with

      <menuchoice>
        <guimenu>Session</guimenu>
        <guimenuitem>New...</guimenuitem>
      </menuchoice>.

      After giving the session a name of your choice, a login mask
      appears. The session is configured with the
      <guibutton>Setup</guibutton> button. The only thing that needs
      to be configured is the hostname. After clicking
      <guibutton>Apply</guibutton> and <guibutton>Quit</guibutton> you
      are ready to log in. Pressing the right mouse button clicking
      <guibutton>Login</guibutton> will scan the &dcache; instance for
      domains. Cells can be reached by clicking on their name and the
      same commands can be entered as in the SSH login.
    </para>

    <para>
      The other tabs of the GUI are very useful for monitoring the
      &dcache; system.
    </para>

   </section>


  <!--
  TODO:
    <para>
      admin interface, unwatch, watch, create pool group, save, restore - intouch is getting too big
    </para>

    <para>
      &pnfs; IDs,  check wormholes and tags, wormholes have to be rewritten after fset io - see pnfs
    </para>

    <para>
      Check pnfs mount permissions and restrict to localhost - see pnfs
    </para>

    -->


  <!-- TODO:
    <para>
      Trash moved to pnfs

      -

      When a file in the &pnfs; filesystem is deleted the server
      stores information about is in the subdirectories of <filename
      class="directory">/opt/pnfsdb/pnfs/trash/</filename>. The
      <literal>cleaner</literal> cell in the
      <literal>pnfsDomain</literal> is responsible for deleting the
      actual files from the pools asyncronously. It uses the files in
      the directory <filename
      class="directory">/opt/pnfsdb/pnfs/trash/2/</filename>. It
      contains a file with the &pnfs; ID of the deleted file as
      name. If a pool containing that file is down at the time the
      cleaner tries to remove it, it will retry for a while. After
      that the file
      <filename>/opt/pnfsdb/pnfs/trash/2/current/failed.<replaceable>poolName</replaceable></filename>
      will contain the &pnfs; IDs which have not been removed from
      that pool. The cleaner will still retry the removal with a lower
      frequency.
    </para>
    -->


  <!-- TODO:
    <para>
      garbage - not useful

      -

<screen>storageinfoof &lt;PNFSID&gt;</screen>

      gives the desired size and - maybe, depending on the method the
      file was written - the ALDER checksum in
      <quote>flag-c=1:&lt;alder32(hex)&gt;</quote>. You can compare it
      with the file on disk.
    </para>

    <para>
      My guess would be:

      If the state is "from store", the file is not complete and you
      have to stage it again. If it is complete, you can set it to
      cached with no harm. It should not be in any other state.
    </para>

    <para>
      If "ls rep" does not list it at all: I know that there is a
      mechanism to register files, which are on disk - including
      checking the checksums and generating the local control
      data. Unfortunately, i do not know of a way to trigger it other
      than to restart the whole pool. It will then do this
      registration for all files it finds which have incomplete
      controll information. This might take a while if there are a lot
      of them.
    </para>
    -->

</chapter>
