<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN" "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd">

<chapter id="intouch">
  <!-- <?dbhtml dir="intouch"?> -->
  <title>Getting in Touch with dCache</title>
  
  <para>
    This section is a guide for exploring a newly installed <dcache/>
    system. The confidence obtained by this exploration will prove
    very helpful when encountering problems in the running
    system. This forms the basis for the more detailed stuff in the
    later parts of this book.
  </para>
  
  <para>
    The starting point is a fresh installation according to the
    installation instructions shipped with any distribution of
    <dcache/>. All components (<pnfs/>, <dcache/>-core,
    <dcache/>-pool, and <dcache/>-opt) are started on the same
    host. Additional pools on other hosts may also run.
  </para>

  <section id="intouch-client">
    <title>Checking the Functionality</title>

    <para>
      First, we will get used to the client tools. On the <dcache/>
      admin host, change into the <pnfs/> directory, where the users
      are going to store their data:

<screen><userprompt/><command>cd</command> <filename class="directory">/pnfs/<replaceable>site.de</replaceable>/data/</filename>
<userprompt/>
</screen>

      Note that on the <dcache/> admin node this directory is a link
      to <filename class="directory">/pnfs/fs/usr/</filename> and the
      NFS export <literal>localhost:/fs</literal> is mounted to
      <filename class="directory">/pnfs/fs</filename>. The NFS server
      running on the <dcache/> admin node is part of the <pnfs/> system
      which is not part of <dcache/> but heavily used by it.
    </para>

    <para>
      The <pnfs/> filesystem is not intended for reading or writing
      actual data with regular file operations via the NFS
      protocol. However, <literal>localhost:/fs</literal> is a
      special, privileged NFS export that allows reading and writing
      for administrative tasks. It should only be mounted by the admin
      node.
    </para>

    <para>
      Reading and writing data to and from a <dcache/> instance can be
      done with a number of protocols. After a standard installation,
      these protocols are <productname>dCap</productname>,
      <productname>gsiDCap</productname>, and
      <productname>GridFTP</productname>. In addition <dcache/> comes
      with an implementation of the <productname>SRM</productname>
      which negotiates the actual data transfer protocol.
    </para>

    <para>
      We will first try <productname>dCap</productname> with the
      <command>dccp</command> command:

<screen><userprompt/><command>cd</command> <filename class="directory">/pnfs/<replaceable>site.de</replaceable>/data/</filename>
<userprompt/>PATH=/opt/d-cache/dcap/bin/:$PATH
<userprompt/><command>dccp</command> /bin/sh my-test-file
541096 bytes in 0 seconds</screen>

      This command succeeds if the user <literal>user</literal> has
      the unix rights to write to the current directory <filename
      class="directory">/pnfs/<replaceable>site.de</replaceable>/data/</filename>.
      The <productname>dCap</productname> protocol also hat a URL syntax:

<screen><userprompt/><command>dccp</command> dcap://<replaceable>adminNode</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test.tmp
541096 bytes in 0 seconds</screen>

      However, this command only succeeded because the file is world
      readable:

<screen><userprompt/><command>chmod</command> o-r my-test-file
<userprompt/><command>dccp</command> dcap://<replaceable>adminNode</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test2.tmp
Command failed!
Server error message for [1]: "Permission denied" (errno 2).
Failed open file in the dCache.
Can't open source file : "Permission denied"
System error: Input/output error</screen>

      This command did not succeed, because the dcap protocol is
      unauthenticated and the user is mapped to a non-existent user in
      order to determine the access rights. However, you should be
      able to access the file with the NFS mount:

<screen><userprompt/><command>dccp</command> my-test-file /tmp/test2.tmp
541096 bytes in 0 seconds</screen>
    </para>

    <para>
      If you have a valid grid proxy with a certificate subject which
      is properly mapped in the configuration file
      <filename>/opt/d-cache/etc/dcache.kpwd</filename> you can also
      try grid-authenticated access via GSI authenticated
      <productname>dCap</productname>:

<screen><userprompt/><command>chgrp</command> <replaceable>yourVO</replaceable> my-test-file
<userprompt/>LD_LIBRARY_PATH=/opt/d-cache/dcap/lib/:$LD_LIBRARY_PATH
<userprompt/><command>dccp</command> gsidcap://<replaceable>adminNode</replaceable>:22128/pnfs/<replaceable>site.de</replaceable>/data/my-test-file /tmp/test3.tmp
541096 bytes in 0 seconds</screen>

      Or we let the <productname>SRM</productname> negotiate the
      protocol:

<screen><userprompt/>PATH=/opt/d-cache/srm/bin/:$PATH
<userprompt/><command>srmcp</command> srm://<replaceable>adminNode</replaceable>:8443/pnfs/desy.de/data/my-test-file file:////tmp/test4.tmp
configuration file not found, configuring srmcp
created configuration file in ~/.srmconfig/config.xml</screen>

      If the <dcache/> instance is registered as a storage element in
      the LCG/EGEE grid and the LCG user interface software is
      available the file can be accessed via
      <productname>SRM</productname>:

<screen><userprompt/><command>lcg-cp</command> -v --vo <replaceable>yourVO</replaceable> \
srm://<replaceable>dCacheAdminFQN</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file \
file:///tmp/test5.tmp
Source URL: srm://<replaceable>dCacheAdminFQN</replaceable>/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
File size: 541096
Source URL for copy: gsiftp://<replaceable>dCacheAdminFQN</replaceable>:2811//pnfs/site.de/data/my-test-file
Destination URL: file:///tmp/test5.tmp
# streams: 1
Transfer took 770 ms</screen>

      and it can be deleted with the help of the <productname>SRM</productname>:

<screen><userprompt/><command>srm-advisory-delete</command> srm://<replaceable>dCacheAdminFQN</replaceable>:8443/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
 srmcp error :  advisoryDelete(User [name=...],pnfs/<replaceable>site.de</replaceable>/data/my-test-file) 
Error user User [name=...] has no permission to delete 000100000000000000BAF0C0</screen>

      This works only if the grid certificate subject is mapped to a
      user which has permissions to delete the file:

<screen><userprompt/><command>chown</command> <replaceable>yourVO</replaceable>001 my-test-file
<userprompt/><command>srm-advisory-delete</command> srm://<replaceable>dCacheAdminFQN</replaceable>:8443/pnfs/<replaceable>site.de</replaceable>/data/my-test-file
</screen>
    </para>

    <para>
      If the grid functionality is not required the file can be
      deleted with the NFS mount of the <pnfs/> filesystem:

<screen><userprompt/><command>rm</command> <filename>my-test-file</filename>
</screen>
    </para>

  </section>

  <section id="intouch-web">
    <title>The Web Interface for Monitoring <dcache/></title>

    <para>
      In the standard configuration the dCache web interface is
      started on the admin node and can be reached via port
      <literal>2288</literal>. Point a web browser to <ulink
      url="http://adminNode:2288/">http://<replaceable>adminNode</replaceable>:2288/</ulink>
      to get to the main menue of the dCache web interface.  The
      contents of the web interface are self-explanatory and are the
      primary source for most monitoring and trouble-shooting tasks.
    </para>

    <para>
      The <quote>Cell Services</quote> page displays the status of
      some important <glossterm linkend="gl-cell">cells</glossterm> of
      the <dcache/> instance. You might observe that some cells are
      marked <quote>OFFLINE</quote> even though you know that they are
      running and fine. These might be the cells <quote>SRM</quote>,
      <quote>GFTP</quote>, and <quote>DCap-gsi</quote>. The reason is
      that the names of the cells monitored by the web interface are
      explicitly configured in the file
      <filename>/opt/d-cache/config/httpd.batch</filename>. However,
      the cells have other names. If you change the following section
      near the end of the file

<programlisting>#
create diskCacheV111.cells.WebCollectorV3 collector \
    "PnfsManager \
     PoolManager \
     GFTP \
     SRM \
     DCap-gsi \
     -replyObject"
#</programlisting>

      such that it reads

<programlisting>#
create diskCacheV111.cells.WebCollectorV3 collector \
    "PnfsManager \
     PoolManager \
     GFTP-<replaceable>adminNode</replaceable> \
     SRM-<replaceable>adminNode</replaceable> \
     DCap-gsi-<replaceable>adminNode</replaceable> \
     -replyObject"
#</programlisting>

      and restart the <literal>httpd</literal> <glossterm
      linkend="gl-domain">domain</glossterm> by executing

<screen><rootprompt/><command>/opt/d-cache/jobs/httpd</command> stop
<rootprompt/><command>/opt/d-cache/jobs/httpd</command> -logfile=/opt/d-cache/log/http.log start
</screen>

      More information about the
      <filename><replaceable>domainName</replaceable>.batch</filename>
      will follow in the next section.
    </para>

    <para>
      The <quote>Pool Usage</quote> page gives an good overview of the
      current space usage of the whole <dcache/> instance. In the
      graphs, free space is marked yellow, space occupied by
      <glossterm linkend="gl-cached">cached files</glossterm>
      which may be deleted when space is needed is marked green, and
      space occupied by <glossterm linkend="gl-precious">precious
      files</glossterm> which cannot be deleted. Other states
      (e.g. files which are currently written) are marked purple.
    </para>

    <para>
      The page <quote>Pool Request Queues</quote> (or <quote>Pool
      Transfer Queues</quote>) gives information about the number
      current requests handled by each pool. <quote>Actions
      Log</quote> keeps track of all the transfers performed by the
      pools up to now.
    </para>

    <para>
      The remaining pages are only relevant with more advanced
      configurations: The page <quote>Pools</quote> (or <quote>Pool
      Attraction Configuration</quote>) can be used to analyze the
      current configuration of the <glossterm
      linkend="gl-pm-comp-psu">pool selection unit</glossterm> in the
      pool manager. The remaining pages are relevant only if a
      <glossterm linkend="gl-tss">tertiary storage system (HSM)</glossterm>
      is connected to the <dcache/> instance.
    </para>

  </section>

  <section id="intouch-files">
    <title>Files</title>

    <para>
      In this section we will have a look at the configuration and log
      files of <pnfs/> and <dcache/>. 
    </para>

    <para>
      The primary configuration file for <pnfs/> is
      <filename>/usr/etc/pnfsSetup</filename>. For administrative
      tasks in <pnfs/> it has to be sourced and the path to the
      admninistration tools should be added to <varname>$PATH</varname>:

<screen><rootprompt/>. /usr/etc/pnfsSetup
<rootprompt/>PATH=$PATH:$pnfs/tools
</screen>

      Stopping and starting the <pnfs/> server can now be done with

<screen><rootprompt/><command>pnfs</command> stop
<rootprompt/><command>pnfs</command> start
</screen>

      The log files for the three <pnfs/> server daemons are
      <filename>/var/log/pmountd.log</filename>,
      <filename>/var/log/dbserver.log</filename>, and
      <filename>/var/log/pnfsd.log</filename>. The
      <command>pnfsd</command> is the NFS server implementation. Its
      log file contains one line for each NFS filesystem
      operation. The result code of that operation can be found at the
      end of the line.
    </para>

    <para>
      The <command>pnfsd</command> daemons communicate with the
      <command>dbserver</command> daemons via a shared memory
      area. These perform the actual operations on the database files
      which can be found in <filename
      class="directory">/opt/pnfsdb/pnfs/databases/</filename>. The
      <pnfs/> system finds them via the information in the directory
      <filename
      class="directory">/opt/pnfsdb/pnfs/info/</filename>. 
    </para>

    <para>
      Each database file is handled by one <command>dbserver</command>
      daemon and each access will lock the database file. Each
      database file/server is the container for one directory
      sub-tree. Similar to mounts in the UNIX filesystem, the
      filesystem tree is stitched together from several such
      sub-trees. In the standard configuration the
      <literal>admin</literal> database contains the root of the
      filesystem (the one mounted on <filename
      class="directory">/pnfs/fs/</filename>) and the
      <literal>data1</literal> database the sub-tree starting at
      <filename class="directory">/pnfs/fs/usr/data/</filename>. <xref
      linkend="cf-pnfs-db"/> describes how to create new databases.
    </para>

    <para>
      The <dcache/> software is installed in one directory, normally
      <filename class="directory">/opt/d-cache/</filename>.  All
      configuration and log files can be found here. In the following
      filenames will always be relative to this directory.
    </para>
    
    <para>
      In the previous section we have already seen how a <glossterm
      linkend="gl-domain">domain</glossterm> is restarted:
      
<screen><rootprompt/><command>/opt/d-cache/jobs/<replaceable>domainName</replaceable></command> stop
<rootprompt/><command>/opt/d-cache/jobs/<replaceable>domainName</replaceable></command> \
-logfile=/opt/d-cache/log/<replaceable>domainName</replaceable>.log start
</screen>

      Listing the contents of <filename
      class="directory">jobs/</filename> you will see that these
      scripts are all in fact links to
      <filename>jobs/wrapper2.sh</filename>. This script will
      determine the domain to start by its basename. From the above
      commands you can already read of the standard location for the
      log files.
    </para>

    <para>
      The files
      <filename>config/<replaceable>domainName</replaceable>Setup</filename>
      will be used by the wrapper script and by the domains at
      start-up. These files are also links to
      <filename>config/dCacheSetup</filename>. This is the primary
      configuration file of the <dcache/> system. 
    </para>

    <para>
      The only files which are different for each domain are
      <filename>config/<replaceable>domainName</replaceable>.batch</filename>.
      They describe which <glossterm linkend="gl-cell">cells</glossterm> are 
      started in the <glossterm linkend="gl-domain">domains</glossterm>. Normally,
      changes in these files should not be necessary. However, if you need to
      change something, consider the following:
    </para>

    <para>
      Since the standard
      <filename>config/<replaceable>domainName</replaceable>.batch</filename>
      files will be overwritten when updating to a newer version of <dcache/>
      (e.g. with RPM), it is a good idea to modify only private copies of them.
      When choosing a name like
      <filename>config/<replaceable>newDomainName</replaceable>.batch</filename> 
      you give the domain the name
      <replaceable>newDomainName</replaceable>. The necessary links can be
      created with

<screen><rootprompt/><command>cd</command> /opt/d-cache/config/
<rootprompt/><command>../jobs/initPackage.sh</command></screen>

      Then the old domain can be stopped and the new one started:

<screen><rootprompt/><command>/opt/d-cache/jobs/<replaceable>domainName</replaceable></command> stop
<rootprompt/><command>/opt/d-cache/jobs/<replaceable>newDomainName</replaceable></command> \
-logfile=/opt/d-cache/log/<replaceable>newDomainName</replaceable>.log start</screen>

    </para>

    <para>
      Recall, that we did change <filename>config/httpd.batch</filename> in
      order to adjust the names of the cells which should show up in the
      <quote>Cell Services</quote> page. Since this was a minor change not
      influencing the functionality of the <dcache/> instance, it is preferable
      not to change the name of the domain. This way, changes to
      <filename>config/http.batch</filename> in the distribution will be
      included in your <filename>config/httpd.batch</filename>. Such a future
      change will correct the problem with the <quote>OFFLINE</quote> cells, anyway.
    </para>

    <para>
      An example, where you might want to create new
      <filename>.batch</filename> files is the following:
    </para>

    <para>
      Changing the postgresql password in
      <filename>config/srm.batch</filename>,
      <filename>config/utility.batch</filename>, and
      <filename>config/replica.batch</filename> to a secure one would be done
      best in copies of these <filename>.batch</filename> files. You should
      also do e.g.

<screen><rootprompt/><command>chmod</command> 600 <filename>config/mysrm.batch</filename></screen>

      to secure the password further. Note that future versions of <dcache/>
      will not require a plain text postgresql password in these files
      anymore. 
    </para>

    <para>
      More details about domains and cells can be found in <xref
        linkend="cf-cellpackage"/>.
    </para>

    <para>
      The most central component of a <dcache/> instance is the
      <literal>PoolManager</literal> cell. It reads additional
      configuration information from the file
      <filename>config/PoolManager.config</filename> at
      start-up. However, in contrast to the
      <filename>config/<replaceable>domainName</replaceable>Setup</filename>
      files, it is not necessary to restart the domain when changing
      the file. We will see an example of this below.
    </para>

    <para>
      The domains containing the pools are handled a little
      differently from the other domains. They are started by

<screen><rootprompt/><command>/opt/d-cache/jobs/pool</command> -pool=<replaceable>pool</replaceable> \
-logfile=log/<replaceable>pool</replaceable>Domain.log start</screen>

      This will start the domain
      <literal><replaceable>pool</replaceable>Domain</literal> which
      contains a pool cell for each line in the file
      <filename>config/<replaceable>pool</replaceable>.poollist</filename>. When
      only one pool domain is started on a host, it is conventional to
      use the hostname for <replaceable>pool</replaceable>. A line of
      the
      <filename><replaceable>pool</replaceable>.poollist</filename>
      has the form

<programlisting><replaceable>poolName</replaceable> <replaceable>poolDir</replaceable> <replaceable>options</replaceable></programlisting>

      For the pool names it is conventional to use
      <literal><replaceable>hostname</replaceable>_<replaceable>n</replaceable></literal>
      (where <replaceable>n</replaceable> starts at 1).
    </para>

    <para>
      Similar to <filename>config/PoolManager.conf</filename>, the
      pools read their configuration from
      <filename><replaceable>poolDir</replaceable>/pool/setup</filename>
      at startup.
    </para>

    <unfinished>
      <para>
        <filename>certificate</filename>
        <filename>CAs</filename>
        <filename>kpwd</filename>
      </para>
    </unfinished>

  </section>

  <section id="intouch-admin">
    <title>The Admin Interface</title>

    <para>
      <dcache/> has a powerful administration interface. It is accessed with
      the SSH protocol. The server is part of the <literal>adminDoor</literal>
      domain, which is started on the admin node. Connect to it with

<screen><userprompt/><command>ssh</command> -c blowfish -p 22223 -l admin <replaceable>adminNode</replaceable></screen>

      The initial password is <quote>dickerelch</quote> (German for <quote>fat
        elk</quote>) and you will be greeted by the prompt

<screen>
    dCache Admin (VII) (user=admin)


<dcprompt select="local"/></screen>

    </para>
    
    <para>
      The password can now be changed with

<screen><dcprompt>local</dcprompt><command>cd</command> acm
<dcprompt select="acm"/><command>create user</command> admin
<dcprompt select="acm"/><command>set passwd</command> -user=admin <replaceable>newPasswd</replaceable> <replaceable>newPasswd</replaceable>
<dcprompt>acm</dcprompt><command>..</command>
<dcprompt>local</dcprompt><command>logoff</command>
</screen>

      This already illustrates how to navigate within the administration
      interface: Starting from the local prompt (<dcprompt select="local"/>)
      the command <command>cd</command> takes you to the specified 
      <glossterm linkend="gl-cell">cell</glossterm> (here
      <literal>acm</literal>, the access control manager). There two commands
      are executed. The escape sequence <command>..</command> takes you back
      to the local prompt and <command>logoff</command> exits the admin
      shell. 
    </para>

    <para>
      Note that <command>cd</command> only works from the local
      prompt. If the cell you are trying to access does not exist, the
      <command>cd</command> command will not recognise this. Trying to execute
      any command after the <command>cd</command> will then result in an error
      message <quote>No Route to cell...</quote>.
    </para>

    <para>
      All cells know the commands <command>info</command> for general information
      about the cell and <command>show pinboard</command> for listing the last
      lines of the <glossterm linkend="gl-pinboard">pinboard</glossterm> of the
      cell. The output of these commands contains useful information for
      solving problems. It is a good idea to get aquainted with the normal
      output in the following cells: <literal>PoolManager</literal>,
      <literal>PnfsManager</literal>, and the pool cells
      (e.g. <literal><replaceable>poolHostname</replaceable>_1</literal>).
    </para>

    <para>
      There also is the command <command>help</command> for listing all
      commands the cell knows and their parameters. However, many of the
      commands are only used for debuging and 
      development purposes. Only commands described in this documentation
      should be used for the administration of a <dcache/> system.
    </para>

    <para>
      Instead of using <command>ssh</command> to access the admin interface,
      the <dcache/> graphical user interface can be used. If it is not included
      in the <dcache/> distribution, it can be downloaded from the <ulink
        url="http://www.dcache.org/"><dcache/> homepage</ulink>. It is started
      by 

<screen><userprompt/><command>java</command> -jar org.pcells.jar</screen>

      First, a new session has to be created with
      <menuchoice>
        <guimenu>Session</guimenu>
        <guimenuitem>New...</guimenuitem>
      </menuchoice>. 
      After giving the session a name of your choice, a login
      mask appears. The session is configured with the
      <guibutton>Setup</guibutton> button. The only thing that needs to be
      configured is the hostname. After clicking <guibutton>Apply</guibutton>
      and <guibutton>Quit</guibutton> you are ready to log in. Pressing the
      right mouse button clicking <guibutton>Login</guibutton> will scan the 
      <dcache/> instance for domains. Cells can be reached by clicking on their
      name and the same commands can be entered as in the SSH login. 
    </para>

    <para>
      The other tabs of the GUI are very useful for monitoring the <dcache/>
      system.
    </para>

    <para>
      The most useful command of the pool cells is <xref
        linkend="cmd-rep_ls"/>. It lists the files which are stored in the pool
      by their <pnfs/> IDs:

<screen>000100000000000000001120 &lt;-P---------(0)[0]&gt; 485212 si={myStore:STRING}
000100000000000000001230 &lt;C----------(0)[0]&gt; 1222287360 si={myStore:STRING}</screen>

      Each file in a pool has one of the 4 primary states:
      <quote>cached</quote> (<literal>&lt;C---</literal>),
      <quote>precious</quote> (<literal>&lt;-P--</literal>),
      <quote>from client</quote> (<literal>&lt;--C-</literal>), and
      <quote>from store</quote> (<literal>&lt;---S</literal>).
    </para>

    <para>
      Two commands in the pool manager are quite useful: <xref
        linkend="cmd-rc_ls"/> lists the requests currently handled by the pool
      manager. A typical line of output for a read request with an error
      condition is (all in one line):
    
<screen>000100000000000000001230@0.0.0.0/0.0.0.0 m=1 r=1 [&lt;unknown&gt;]
  [Waiting 08.28 19:14:16] 
  {149,No pool candidates available or configured for 'staging'}</screen>

      As the error message at the end of the line indicates, no pool was found
      containing the file and no pool could be used for staging the file from a
      tertiary storage system.
    </para>

    <para>
      Finally, <xref linkend="cmd-cm_ls"/> with the option <option>-r</option>
      gives the information about the pools currently stored in the cost module
      of the pool manager. A typical output is:

<screen><dcprompt select="PoolManager"/><command>cm ls</command> <option>-r</option>
<replaceable>poolName1</replaceable>={R={a=0;m=2;q=0};S={a=0;m=2;q=0};M={a=0;m=100;q=0};PS={a=0;m=20;q=0};PC={a=0;m=20;q=0};SP={t=2147483648;f=924711076;p=1222772572;r=0;lru=0;{g=20000000;b=0.5}}}
<replaceable>poolName1</replaceable>={Tag={{hostname=<replaceable>hostname</replaceable>}};size=0;SC=0.16221282938326134;CC=0.0;}
<replaceable>poolName2</replaceable>={R={a=0;m=2;q=0};S={a=0;m=2;q=0};M={a=0;m=100;q=0};PS={a=0;m=20;q=0};PC={a=0;m=20;q=0};SP={t=2147483648;f=2147483648;p=0;r=0;lru=0;{g=4294967296;b=250.0}}}
<replaceable>poolName2</replaceable>microcebus_2={Tag={{hostname=<replaceable>hostname</replaceable>}};size=0;SC=2.7939677238464355E-4;CC=0.0;}
</screen>

      While the first line for each pool gives the information stored in the
      cache of the cost module, the second line gives the costs (SC: <glossterm
        linkend="gl-space_cost">space cost</glossterm>, CC: <glossterm
        linkend="gl-performance_cost">performance cost</glossterm>) calculated
      for a (hypothetical) file of zero size. For details on how these are
      calculated and their meaning, see <xref linkend="cf-pm-cm"/>.
    </para>

    <para>
      The SSH admin interface can be used non-interactively by
      scripts. For this the <dcache/>-internal SSH server uses
      public/private key pairs.
    </para>

    <para>
      The file <filename>config/authorized_keys</filename> contains
      one line per user. The file has the same format as
      <filename>~/.ssh/authorized_keys</filename> which is used by
      <command>sshd</command>. The keys in
      <filename>config/authorized_keys</filename> have to be of type
      RSA1 as <dcache/> only supports SSH protocol 1. Such a key is generated with

      <screen><userprompt/><command>ssh-keygen</command> -t rsa1 -C 'SSH1 key of <replaceable>user</replaceable>'
Generating public/private rsa1 key pair.
Enter file in which to save the key (/home/<replaceable>user</replaceable>/.ssh/identity): 
Enter passphrase (empty for no passphrase):
Enter same passphrase again:
Your identification has been saved in /home/<replaceable>user</replaceable>/.ssh/identity.
Your public key has been saved in /home/<replaceable>user</replaceable>/.ssh/identity.pub.
The key fingerprint is:
c1:95:03:6a:66:21:3c:f3:ee:1b:8d:cb:46:f4:29:6a SSH1 key of <replaceable>user</replaceable></screen>

      The passphrase is used to encrypt the private key (now stored in
      <filename>/home/<replaceable>user</replaceable>/.ssh/identity</filename>). If
      you do not want to enter the passphrase every time the private
      key is used, you can use <command>ssh-add</command> to add it to
      a running <command>ssh-agent</command>. If no agent is running start it with

<screen><userprompt/>if [ -S $SSH_AUTH_SOCK ] ; then echo "Already running" ; else eval `ssh-agent` ; fi</screen>

      and add the key to it with
      
<screen><userprompt/><command>ssh-add</command>
Enter passphrase for SSH1 key of <replaceable>user</replaceable>:
Identity added: /home/<replaceable>user</replaceable>/.ssh/identity (SSH1 key of <replaceable>user</replaceable>)</screen>

      Now, insert the public key
      <filename>~/.ssh/identity.pub</filename> as a separate line into
      <filename>config/authorized_keys</filename>. The comment field
      in this line <quote>SSH1 key of
      <replaceable>user</replaceable></quote> has to be changed to the
     <dcache/>  user name. An example file is:

<programlisting>1024 35 141939124...15331 admin</programlisting> 
      
      The key manager within <dcache/> will read this file every minute.
    </para>

    <para>
      Now, the ssh program should not ask for a password anymore. This
      is still quite secure, since the unencrypted private key is only
      held in the memory of the <command>ssh-agent</command>. It can
      be removed from it with

<screen><userprompt/><command>ssh-add</command> -d
Identity removed: /home/<replaceable>user</replaceable>/.ssh/identity (RSA1 key of <replaceable>user</replaceable>)</screen>
    </para>

    <para>
      In scripts, constructs like

<programlisting>ssh -c blowfish -p 22223 admin@<replaceable>adminNode</replaceable> &lt;&lt; EOF
cd PoolManager
cm ls -r
..
logoff
EOF</programlisting>

      or

<programlisting>echo -e 'cd <replaceable>pool_1</replaceable>\nrep ls\n..\nlogoff' \
  | ssh -c blowfish -p 22223 admin@<replaceable>adminNode</replaceable> \
  | tr -d '\r' &gt; rep_ls.out</programlisting>

      are quite useful. The last example also illustrates how output
      of the commands should be treated.
    </para>
    
  </section>


  <unfinished>
    
    <para>
      admin interface, unwatch, watch, create pool group, save, restore - intouch is getting too big
    </para>

    <para>
      <pnfs/> IDs,  check wormholes and tags, wormholes have to be rewritten after fset io - see pnfs
    </para>

    <para>
      Check pnfs mount permissions and restrict to localhost - see pnfs
    </para>

  </unfinished>


  <unfinished>

    <para>
      Trash moved to pnfs

      -

      When a file in the <pnfs/> filesystem is deleted the server
      stores information about is in the subdirectories of <filename
      class="directory">/opt/pnfsdb/pnfs/trash/</filename>. The
      <literal>cleaner</literal> cell in the
      <literal>pnfsDomain</literal> is responsible for deleting the
      actual files from the pools asyncronously. It uses the files in
      the directory <filename
      class="directory">/opt/pnfsdb/pnfs/trash/2/</filename>. It
      contains a file with the <pnfs/> ID of the deleted file as
      name. If a pool containing that file is down at the time the
      cleaner tries to remove it, it will retry for a while. After
      that the file
      <filename>/opt/pnfsdb/pnfs/trash/2/current/failed.<replaceable>poolName</replaceable></filename>
      will contain the <pnfs/> IDs which have not been removed from
      that pool. The cleaner will still retry the removal with a lower
      frequency.
    </para>

  </unfinished>

  <unfinished>

    <para>
      garbage - not useful

      -

<screen>storageinfoof &lt;PNFSID&gt;</screen>

      gives the desired size and - maybe, depending on the method the
      file was written - the ALDER checksum in
      <quote>flag-c=1:&lt;alder32(hex)&gt;</quote>. You can compare it
      with the file on disk.
    </para>

    <para>
      My guess would be:

      If the state is "from store", the file is not complete and you
      have to stage it again. If it is complete, you can set it to
      cached with no harm. It should not be in any other state.
    </para>

    <para>
      If "ls rep" does not list it at all: I know that there is a
      mechanism to register files, which are on disk - including
      checking the checksums and generating the local control
      data. Unfortunately, i do not know of a way to trigger it other
      than to restart the whole pool. It will then do this
      registration for all files it finds which have incomplete
      controll information. This might take a while if there are a lot
      of them.
    </para>

  </unfinished>
  
</chapter>
