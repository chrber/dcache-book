<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
<!ENTITY % sharedents SYSTEM "shared-entities.xml" >
%sharedents;
]>

<chapter id="cb-pool">

  <title>Pool Operations</title>

  <!-- TODO:  The following should be completed
  <section id="cb-pool-remove">
    <title>Removing a Pool</title>

    <section id="cb-pool-remove-precious">
      <title>Removing a Pool with Precious Files on it</title>

      <para>
        TODO
      </para>
    </section>

    <section id="cb-pool-remove-cached">
      <title>Removing a Pool with only cached Files</title>

      <para>
        TODO
      </para>
    </section>

  </section>
  -->

  <section id="cb-pool-checksumming">
    <title>Checksums</title>

    <para>
      In &dcache; the storage of a checksum is part of a successful
      transfer.
      <itemizedlist>
	<listitem>
	  <para>
	    For an incoming transfer a checksum can be sent by the
	    client (<firstterm>Client Checksum</firstterm>, it can be
	    calculated during the transfer (<firstterm>Transfer
	    Checksum</firstterm>) or it can be calculated on the
	    server after the file has been written to disk
	    (<firstterm>Server File Checksum</firstterm>).
	  </para>
	</listitem>
	<listitem>
	  <para>
	    For a pool to pool transfer a Transfer Checksum or a
	    Server File Checksum can be calculated.
	  </para>
	</listitem>
	<listitem>
	  <para>
	    For data that is flushed to or restored from tape a
	    checksum can be calculated before flushed to tape or after
	    restored from tape, respectively.
	  </para>
	</listitem>
      </itemizedlist>
    </para>

      <variablelist>
        <varlistentry>
          <term>Client Checksum</term>
          <listitem>
            <para>
              The client calculates the checksum before or while the
              data is sent to &dcache;. The checksum value, depending
              on when it has been calculated, may be sent together
              with the open request to the door and stored into
              &chimera; before the data transfer begins or it may be
              sent with the close operation after the data has been
              transferred.
            </para>

            <para>
              The &dcap; protocol provides both methods, but the
              &dcap; clients use the latter by default.
            </para>

            <para>
              The &ftp; protocol does not provide a mechanism to send
              a checksum.  Nevertheless, some &ftp; clients can
              (mis-)use the <quote><literal>site</literal></quote>
              command to send the checksum prior to the actual data
              transfer.
            </para>
          </listitem>
        </varlistentry>

        <varlistentry>
          <term>Transfer Checksum</term>

          <listitem>
            <para>
              While data is coming in, the server data mover may
              calculate the checksum on the fly.
            </para>
          </listitem>
        </varlistentry>


        <varlistentry>
          <term>Server File Checksum</term>

          <listitem>
            <para>
              After all the file data has been received by the
              &dcache; server and the file has been fully written to
              disk, the server may calculate the checksum, based on
              the disk file.
            </para>
          </listitem>
        </varlistentry>
      </variablelist>

    <para>
      The default configuration is that a checksum is calculated on
      write, i.e. a Server File Checksum.
    </para>

    <section id="cb-pool-checksumming-config">
      <title>How to configure checksum calculation</title>

      <para>
	Configure the calculation of checksums in the <link
	linkend='intouch-admin'>admin interface</link>. The
	configuration has to be done for each pool separately.
      </para>

      <screen>&dc-prompt-local; <userinput>cd <replaceable>poolname</replaceable></userinput>
&dc-prompt-pool; <userinput>csm set policy -<replaceable>option</replaceable>=<replaceable>on/off</replaceable></userinput>
&dc-prompt-pool; <userinput>save</userinput></screen>

      <para>
	The configuration will be saved in the file
	<filename><replaceable>path/to/pool</replaceable>/<replaceable>nameOfPooldirectory</replaceable>/setup</filename>.
      </para>

      <para>
	Use the command <command>csm info</command> to see the
	checksum policy of the pool.
      </para>

      <screen>&dc-prompt-pool; <userinput>csm info</userinput>
 Policies :
        on read : false
       on write : true
       on flush : false
     on restore : false
    on transfer : false
    enforce crc : true
  getcrcfromhsm : false
          scrub : false
      </screen>

      <para>
	The default configuration is to check checksums on write.
      </para>

      <para>
	Use the command <command>help csm set policy</command> to see
	the configuration options.
      </para>

      <para>
	The syntax of the command <command>csm set policy</command> is

      <cmdsynopsis>
	<command>csm set policy</command>
	  <arg><parameter>-<replaceable>option</replaceable></parameter>=on<arg>|off</arg></arg>
      </cmdsynopsis>

	where <replaceable>option</replaceable> can be replaced by
      </para>


      <variablelist>
	<title>OPTIONS</title>
	<varlistentry>
	  <term><varname>ontransfer</varname></term>
	  <listitem>
	    <para>
	      If supported by the protocol, the checksum is
	      calculated during file transfer.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><varname>onwrite</varname></term>
	  <listitem>
	    <para>
	      The checksum is calculated after the file has been
	      written to disk.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><varname>onrestore</varname></term>
	  <listitem>
	    <para>
	      The checksum is calculated after data has been
	      restored from tape.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><varname>onflush</varname></term>
	  <listitem>
	    <para>
	      The checksum is calculated before data is flushed to
	      tape.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><varname>getcrcfromhsm</varname></term>
	  <listitem>
	    <para>
	      If the &hsm; script supports it, the
	      <filename><replaceable>pnfsid</replaceable>.crcval</filename>
	      file is read and stored in &chimera;.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry>
	  <term><varname>scrub</varname></term>
	  <listitem>
	    <para>
	      Pool data will periodically be veryfied against
	      checksums. Use the command <command>help csm set
	      policy</command> to see the configuration options.
	    </para>
	  </listitem>
	</varlistentry>
	<varlistentry><term><varname>enforcecrc</varname></term>
	<listitem>
	  <para>
	    If no checksum has been calculated after or during
	    the transfer, this option ensures that a checksum is
	    calculated and stored in &chimera;.
	  </para>
	</listitem>
	</varlistentry>
      </variablelist>

      <para>
	The option <varname>onread</varname> has not yet been
	implemented.
      </para>

      <para>
	If an option is enabled a checksum is calculated as described.
	If there is already another checksum, the checksums are
	compared and if they match stored in &chimera;.
      </para>

      <important>
	<para>
	  Do not change the default configuration for the option
	  <varname>enforcecrc</varname>. This option should always be
	  enabled as this ensures that there will always be a checksum
	  stored with a file.
	</para>
      </important>

    </section>

  </section>








  <section id="cb-pool-migration" status="draft">
    <title>Migration Module</title>

    <para>
      The migration module is a component of &dcache; pools introduced
      in version 1.9.1. The purpose of the component is essentially to
      copy or move the content of a pool to one or more other
      pools. The migration module replaces the copy manager found in
      previous releases of &dcache;. We advice against using the old
      copy manager, as it known to have problems.
    </para>

    <para>
      Typical use cases for the migration module include:
    </para>

    <itemizedlist>
      <listitem>
        <para>
          Vacating pools, that is, moving all files to other pools
          before decomisioning the pool.
        </para>
      </listitem>

      <listitem>
        <para>
          Caching data on other pools, thus distributing the load and
          increasing availability.
        </para>
      </listitem>

      <listitem>
        <para>
          As an alternative to the hopping manager.
        </para>
      </listitem>
    </itemizedlist>

    <section>
      <title>Overview and Terminology</title>

      <para>
        The migration module runs inside pools and hosts a number of
        migration jobs.  Each job operates on a set of files on the
        pool on which it is executed and can copy or move those files
        to other pools. The migration module provides filters for
        defining the set of files on which a job operates.
      </para>

      <para>
        The act of copying or moving a single file is called a
        migration task. A task selects a target pool and asks it to
        perform a pool to pool transfer from the source pool. The
        actual transfer is performed by the same component performing
        other pool to pool transfers. The migration module does not
        perform the transfer; it only orchestrates it.
      </para>

      <para>
        The state of the target copy (the target state) as well as the
        source copy (the source state) can be explicitly defined. For
        instance, for vacating a pool the target state is set to be
        the same as the original source state, and the source state is
        changed to removed; for caching files, the target state is set
        to cached, and the source state is unmodified.
      </para>

      <para>
        Sticky flags owned by the pin manager are never touched by a
        migration job, however the migration module can ask the pin
        manager to move the pin to the target pool. Care has been
        taken that unless the pin is moved by the pin manager, the
        source file is not deleted by a migration job, even if asked
        to do so. To illustrate this, assume a source file marked
        precious and with two sticky flags, one owned by foobar and
        the other by the pin manager. If a migration job is configured
        to delete the source file, but not to move the pin, the result
        will be that the file is marked cached, and the sticky flag
        owned by foobar is removed. The pin remains. Once it expires,
        the file is eligible for garbage collection.
      </para>

      <para>
        All operations are idempotent. This means that a migration job
        can be safely rerun, and as long as everything else is
        unchanged, files will not be transferred again. Because jobs
        are idempotent they do not need to maintain persistent state,
        which in turns means the migration module becomes simpler and
        more robust. Should a pool crash during a migration job, the
        job can be rerun and the remaining files will be transfered.
      </para>

      <para>
        It is safe to run migration jobs while pools are in use. Once
        started, migration jobs run to completion and do only operate
        on those files that matched the selection filters at the time
        the migration job started. New files that arrive on the pool
        are not touched. Neither are files that change state after a
        migration job has been initialized, even though the selection
        filters would match the new state of the file. The exception
        to the rule is when files are deleted from the pool or change
        state so that they no longer match the selection filter. Such
        files will be excluded from the migration job, unless the file
        was already processed. Rerunning a migration job will force it
        to pick up any new files. Because the job is idempotent, any
        files copied before are not copied again.
      </para>

      <para>
        Permanent migration jobs behave differently. Rather than
        running to completion, permanent jobs keep running until
        explicitly cancelled. They monitor the pool for any new files
        or state changes, and dynamically add or remove files from the
        transfer queue. Permanent jobs are made persistent when the
        <command>save</command> command is executed and will be
        recreated on pool restart. The main use case for permanent
        jobs is as an alternative to using a central hopping manager.
      </para>

      <para>
        Idempotence is achieved by locating existing copies of a file
        on any of the target pools. If an existing copy is found,
        rather than creating a new copy, the state of the existing
        copy is updated to reflect the target state specified for the
        migration job. Care is taken to never make a file more
        volatile than it already is: Sticky flags are added, or
        existing sticky flags are extended, but never removed or
        shortened; cached files may be marked precious, but not vice
        versa. One caveat is when the target pool containing the
        existing copy is offline. In that case the existence of the
        copy cannot be verified. Rather than creating a new copy, the
        task fails and the file is put back into the transfer
        queue. This behaviour can be modified by marking a migration
        job as eager. Eager jobs create new copies if an existing copy
        cannot be immediately verified. As a rule of thumb, permanent
        jobs should never be marked eager. This is to avoid that a
        large number of unnecessary copies are created when several
        pools are restarted simultaneously.
      </para>

      <para>
        A migration task aborts whenever it runs into a problem. The
        file will be reinserted at the end of the transfer
        queue. Consequently, once a migration job terminates, all
        files have been successfully transferred. If for some reason
        tasks for particular files keep failing, then the migration
        job will never terminate by itself as it retries indefinitely.
      </para>

    </section>

    <section>
      <title>Command Summary</title>

      <para>
        All commands begin with the string
        <command>migration</command>, e.g. <command>migration
        copy</command>. The commands <command>migration
        copy</command>, <command>migration cache</command> and
        <command>migration move</command> create new migration
        jobs. These commands take the same options and only differ in
        default values. Except for the number of concurrent tasks,
        transfer parameters of existing jobs cannot be changed. This
        is by design to ensure idempotency of jobs. The concurrency
        can be altered through the <command>migration
        concurrency</command> command.
      </para>

      <para>
        Jobs are assinged a job ID and are executed in the
        background. The status of a job may be queried through the
        <command>migration info</command> command. A list of all jobs
        can be obtained through <command>migration ls</command>. Jobs
        stay in the list even after they have terminated. Terminated
        jobs can be cleared from the list through the
        <command>migration clear</command> command.
      </para>

      <para>
        Jobs can be suspended, resumed and cancelled through the
        <command>migration suspend</command>, <command>migration
        resume</command> and <command>migration cancel</command>
        commands. Existing tasks are allowed to finish before a job is
        suspended or cancelled.
      </para>
    </section>

    <section>
      <title>Examples</title>

      <section>
        <title>Vacating a pool</title>

        <para>
          To vacate <replaceable>sourcePool</replaceable>, we first
          mark the pool read-only to avoid that more files are added
          to the pool, and then move all files to
          <replaceable>targetPool</replaceable>. It is not strictly
          necessary to mark the pool read-only, however if not done
          there is no guarantee that the pool is empty when the
          migration job terminates. The job can be rerun to move
          remaining files.
        </para>

<screen>&dc-prompt-srcpool; <userinput>pool disable -rdonly</userinput>
&dc-prompt-srcpool; <userinput>migration move <replaceable>targetPool</replaceable></userinput>
[1] RUNNING      migration move <replaceable>targetPool</replaceable>
&dc-prompt-srcpool; <userinput>migration info 1</userinput>
Command    : migration move <replaceable>targetPool</replaceable>
State      : RUNNING
Queued     : 0
Attempts   : 1
Targets    : <replaceable>targetPool</replaceable>
Completed  : 0 files; 0 bytes; 0%
Total      : 830424 bytes
Concurrency: 1
Running tasks:
[0] 0001000000000000000BFAE0: TASK.Copying -> [<replaceable>targetPool</replaceable>@local]
&dc-prompt-srcpool; <userinput>migration info 1</userinput>
Command    : migration move <replaceable>targetPool</replaceable>
State      : FINISHED
Queued     : 0
Attempts   : 1
Targets    : <replaceable>targetPool</replaceable>
Completed  : 1 files; 830424 bytes
Total      : 830424 bytes
Concurrency: 1
Running tasks:
&dc-prompt-srcpool; <userinput>rep ls</userinput>
&dc-prompt-srcpool;</screen>
      </section>

      <section>
        <title>Caching recently accessed files</title>

        <para>
          Say we want to cache all files belonging to the storage
          group atlas:default and accessed within the last month on a
          set of low-cost cache pools defined by pool group
          cache_pools. We can achieve this through the following
          command.
        </para>

<screen>&dc-prompt-srcpool; <userinput>migration cache -target=pgroup -accessed=0..2592000 -storage=atlas:default cache_pools</userinput>
[1] INITIALIZING migration cache -target=pgroup -accessed=0..2592000 -storage=atlas:default cache_pools
&dc-prompt-srcpool; <userinput>migration info 1</userinput>
Command    : migration cache -target=pgroup -accessed=0..2592000 -storage=atlas:default cache_pools
State      : RUNNING
Queued     : 2577
Attempts   : 2
Targets    : pool group cache_pools, 5 pools
Completed  : 1 files; 830424 bytes; 0%
Total      : 2143621320 bytes
Concurrency: 1
Running tasks:
[72] 00010000000000000000BE10: TASK.Copying -> [pool_2@local]</screen>

        <para>
          The files on the source pool will not be altered. Any file
          copied to one of the target pools will be marked cached.
        </para>
      </section>
    </section>
  </section>

  <section id="cb-pool-rename">
    <title>Renaming a Pool</title>

    <para>
      A pool may be renamed with the following procedure,
      regardless of the type of files stored on it.
    </para>

    <para>
      Disable file transfers from and to the pool with

<screen>&dc-prompt-pool; <userinput>pool disable <option>-strict</option></userinput></screen>

      Then make sure, no transfers are being processed anymore.
      All the following commands should give no output:

<screen>&dc-prompt-pool; <userinput>queue ls queue</userinput>
&dc-prompt-pool; <userinput>mover ls</userinput>
&dc-prompt-pool; <userinput>p2p ls</userinput>
&dc-prompt-pool; <userinput>pp ls</userinput>
&dc-prompt-pool; <userinput>st jobs ls</userinput>
&dc-prompt-pool; <userinput>rh jobs ls</userinput></screen>

      Now the files on the pools have to be unregistered on the
      namespace server with

<screen>&dc-prompt-pool; <userinput>pnfs unregister</userinput></screen>

      <note>
	Do not get confused that the commands <command>pnfs
	unregister</command> and <command>pnfs register</command>
	contain <literal>pnfs</literal> in their names. They also
	apply to &dcache; instances with &chimera; and are named like
	that for legacy reasons.
      </note>

      Even if the pool contains precious files, this is no problem, since
      we will register them again in a moment. The files might not be available
      for a short moment, though.
      Log out of the pool, and stop the domain running the pool:

<screen>&prompt-root; <userinput>&path-odb-n-s;dcache stop <replaceable>poolDomain</replaceable></userinput>
Stopping <replaceable>poolDomain</replaceable> (pid=6070) 0 1 2 3 done
&prompt-root;</screen>

      <para>
      Adapt the name of the pool in the layout files of your dCache installation
      to include your new pool-name. For a general overview of layout-files see
      <xref linkend="in-install-layout" />.
      </para>
      <informalexample>
      <para>
      For example, to rename a pool from
      <literal>swimmingPool</literal> to <literal>carPool</literal>, change
      your layout file from
      </para>

      <programlisting>[<replaceable>poolDomain</replaceable>]
[<replaceable>poolDomain</replaceable>/pool]
name=swimmingPool
path=/pool/</programlisting>

      <para>
      to
      </para>

      <programlisting>[<replaceable>poolDomain</replaceable>]
[<replaceable>poolDomain</replaceable>/pool]
name=carPool
path=/pool/</programlisting>
      </informalexample>

      <warning>
      <para>
        Be careful about renaming pools in the layout after users have already
        been writing to them. This can cause
        inconsistencies in other components of &dcache;, if they are relying
        on pool names to provide their functionality. An example of such a
        component is the &chimera; cache info.
      </para>
      </warning>

      Start the domain running the pool:
<screen>&prompt-root; <userinput>&path-odb-n-s;dcache start <replaceable>poolDomain</replaceable></userinput>
Starting poolDomain done
&prompt-root;</screen>


      Register the files on the pool with
      <screen>&dc-prompt-pool; <userinput>pnfs register</userinput></screen>
    </para>

  </section>

  <section id="cb-pool-pin">
    <title>Pinning Files to a Pool</title>

    <para>
      You may pin a file locally within the private pool repository:

<screen>&dc-prompt-pool; <userinput>rep set sticky <replaceable>pnfsid</replaceable> <option>on|off</option></userinput></screen>

      the 'sticky' mode will stay with the file as long as the file
      is in the pool.  If the file is removed from the pool and
      recreated afterwards this information gets lost.
    </para>

    <para>
      You may use the same mechanism globally:  in the command line
      interface (local mode) there is the command

<screen>&dc-prompt-pool; <userinput>set sticky <replaceable>pnfsid</replaceable></userinput></screen>

      This command does:
      <orderedlist>
        <listitem>
          <para>
            Flags the file as sticky in the name space database
            (pnfs). So from now the filename is globally set sticky.
          </para>
        </listitem>
        <listitem>
          <para>
            Will go to all pools where it finds the file and will flag
            it sticky in the pools.
          </para>
        </listitem>
        <listitem>
          <para>
            All new copies of the file will become sticky.
          </para>
        </listitem>
      </orderedlist>
    </para>

  </section>

</chapter>


