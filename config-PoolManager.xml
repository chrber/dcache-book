<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE chapter PUBLIC "-//OASIS//DTD DocBook XML V4.3//EN"
                         "http://www.oasis-open.org/docbook/xml/4.3/docbookx.dtd" [
  <!ENTITY % sharedents SYSTEM "shared-entities.xml" >
  <!ENTITY psu    "<abbrev>PSU</abbrev>">
  <!ENTITY read   "<literal>read</literal>">
  <!ENTITY write  "<literal>write</literal>">
  <!ENTITY p2p    "<literal>p2p</literal>">
  <!ENTITY cache  "<literal>cache</literal>">

  %sharedents;
]>

<chapter id="cf-pm">

  <title>The &serv-poolmngr; service</title>

  <!--
  <para>(We assume a basic knowledge of the functionality and terminology of
  dCache. Pool, domain, cell, how to log into a cell, and execute
  commands)</para>
  -->

  <para>
    The heart of a &dcache; system is the &serv-poolmngr;. When a user
    performs an action on a file - reading or writing - a
    <firstterm>transfer request</firstterm> is sent to the &dcache;
    system. The &serv-poolmngr; then decides how to handle this
    request.
  </para>

  <para>
    If a file the user wishes to read resides on one of the
    storage-pools within the &dcache; system, it will be transferred
    from that pool to the user. If it resides on several pools, the
    file will be retrieved from the pool which is least busy. If all
    pools the file is stored on are busy, a new copy of the file on an
    idle pool will be created and this pool will answer the request.
  </para>

  <para>
    A new copy can either be created by a <firstterm>pool to pool
    transfer</firstterm> (<abbrev>p2p</abbrev>) or by fetching it from
    a connected <firstterm>tertiary storage system</firstterm>
    (sometimes called <abbrev>HSM</abbrev> - hierarchical storage
    manager). Fetching a file from a tertiary storage system is called
    <firstterm>staging</firstterm>. It is also performed if the file
    is not present on any of the pools in the &dcache; system. The pool
    manager has to decide on which pool the new copy will be
    created, i.e. staged or p2p-copied.
  </para>

  <para>
    The behaviour of the &serv-poolmngr; service is highly
    configurable. In order to exploit the full potential of the
    software it is essential to understand the mechanisms used and how
    they are configured. The &serv-poolmngr; service creates the
    &cell-poolmngr; cell, which is a unique cell in &dcache; and
    consists of several sub-modules: The important ones are the
    <firstterm>pool selection unit</firstterm> (<abbrev>&psu;</abbrev>)
    and the <firstterm>cost manager</firstterm> (<abbrev>CM</abbrev>).
  </para>

  <para>
    The &serv-poolmngr; can be configured by either directly editing
    the file <filename>&file-poolmanager;</filename>
    or via the <link linkend="intouch-admin">Admin
    Interface</link>. Changes made via the Admin Interface will be
    saved in the file
    <filename>&file-poolmanager;</filename> by the
    <command>save</command> command. This file will be parsed,
    whenever the &dcache; starts up. It is a simple text file
    containing the corresponding Admin Interface commands. It can
    therefore also be edited before the system is started. It can also
    be loaded into a running system with the <command>reload</command>
    command.  In this chapter we will describe this file.
<!-- TODO:
This needs to be explained in the section on the Admin Interface:
        The syntax of the commands for configuring the &psu; will be
	explained with the examples below. These commands can be
	issued within the &cell-poolmngr; to change the configuration
	while the system is running. The
	<command>save</command>-command can then be used to save the
	current configuration to the file
	<filename>&file-poolmanager;</filename>.
-->
  </para>

  <para>
    The &psu; is responsible for finding the pool which will be used for
    a specific transfer-request based on the information from the
    CM. By telling the &psu; which pools are permitted for which type of
    transfer-request, the administrator of the &dcache; system can
    adjust the system to any kind of scenario: Separate organizations
    served by separate pools, special pools for writing the data to a
    tertiary storage system, pools in a DMZ which serves only a
    certain kind of data (e.g., for the grid). The following section
    explains the mechanism employed by the &psu; and shows how to
    configure it with several examples.
  </para>

  <section id="cf-pm-psu">
    <title>The Pool Selection Mechanism</title>

    <para>
      The &psu; generates a list of allowed storage-pools for each
      incoming transfer-request. The &psu; configuration described below
      tells the &psu; which combinations of transfer-request and
      storage-pool are allowed. Imagine a two-dimensional table with
      a row for each possible transfer-request and a column for each
      pool - each field in the table containing either
      <quote>yes</quote> or <quote>no</quote>. For an incoming
      transfer-request the &psu; will return a list of all pools with
      <quote>yes</quote> in the corresponding row.
    </para>

    <para>
      Instead of <quote>yes</quote> and <quote>no</quote> the table
      really contains a <firstterm>preference</firstterm> - a
      non-negative integer.  However, the &psu; configuration is easier
      to understand if this is ignored.
    </para>

    <para>
      Actually maintaining such a table in memory (and as user in a
      configuration file) would be quite inefficient, because there are
      many possibilities for the transfer-requests. Instead, the &psu;
      consults a set of rules in order to generate the list of allowed
      pools. Each such rule is called a <firstterm>link</firstterm>
      because it links a set of transfer-requests to a group of pools.
    </para>

    <section id='cf-pm-links'>
      <title>Links</title>
      <para>
	A link consists of a
	set of unit groups and a list of pools. If all the unit groups
	are matched, the pools belonging to the link are added to
	the list of allowable pools.
      </para>

      <para>
	A link is defined in the file
	<filename>&file-poolmanager;</filename> by
      </para>

	<cmdsynopsis>
	  <command>psu create link</command>
	  <arg choice='plain'><replaceable>link</replaceable></arg>
	  <arg choice='plain'><replaceable>unitgroup</replaceable></arg>

	  <command>psu set link</command>
	  <arg choice='plain'><replaceable>link</replaceable></arg>
	  <arg choice='plain'><parameter>-readpref</parameter>=<replaceable>rpref</replaceable></arg>
	  <arg choice='plain'><parameter>-writepref</parameter>=<replaceable>wpref</replaceable></arg>
	  <arg choice='plain'><parameter>-cachepref</parameter>=<replaceable>cpref</replaceable></arg>
	  <arg choice='plain'><parameter>-p2ppref</parameter>=<replaceable>ppref</replaceable></arg>

	  <command>psu add link</command>
	  <arg choice='plain'><replaceable>link</replaceable></arg>
	  <arg choice='plain'><replaceable>poolgroup</replaceable></arg>
	</cmdsynopsis>

      <para>
	For the preference values see <xref linkend="cf-pm-psu-pref"/>.
      </para>

      <para>
	The main task is to understand how the unit groups in a link
	are defined. After we have dealt with that, the preference
	values will be discussed and a few examples will follow.
      </para>

      <para>
	The four properties of a transfer request, which are relevant for
	the &psu;, are the following:
<!--      </para>
-->
	<variablelist>
          <varlistentry>
            <term>Location of the File</term>
            <listitem>
	      <para>
		The location of the file in the file system is not
		used directly. Each file has the following two
		properties which can be set per directory:
	      </para>

		<itemizedlist>
		  <listitem>
		    <formalpara>
		    <title>Storage Class</title>

			The storage class is a string. It is used by a
			tertiary storage system to decide where to
			store the file (i.e. on which set of tapes)
			and &dcache; can use the storage class for a
			similar purpose (i.e. on which pools the file
			can be stored.). A detailed description of the
			syntax and how to set the storage class of a
			directory in the namespace is given in <xref
			linkend="secStorageClass" />.
		    </formalpara>
		    </listitem>

		    <listitem>
		    <formalpara>
		      <title>Cache Class</title>
			The cache class is a string with essentially
			the same functionality as the storage class,
			except that it is not used by a tertiary
			storage system. It is used in cases, where the
			storage class does not provide enough
			flexibility. It should only be used, if an
			existing configuration using storage classes
			does not provide sufficient flexibility.
		      </formalpara>

		    </listitem>
		  </itemizedlist>

	    </listitem>
	  </varlistentry>

          <varlistentry>
            <term>IP Address</term>
            <listitem>
	      <para>
		The IP address of the requesting host.
	      </para>

            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Protocol / Type of Door</term>
            <listitem>
	      <para>
		The protocol respectively the type of door used by the transfer.
	      </para>
            </listitem>
          </varlistentry>

          <varlistentry>
            <term>Type of Transfer</term>
            <listitem>
              <para>
		The type of transfer is either &read;, &write;, &p2p;
		request or &cache;.
	      </para>
	      <para>
		A request for reading a file which is not on a read
		pool will trigger a &p2p; request and a subsequent
		&read; request. These will be treated as two separate
		requests.
	      </para>
	      <para>
		A request for reading a file which is not stored on
		disk, but has to be staged from a connected tertiary
		storage system will trigger a &cache; request to fetch
		the file from the tertiary storage system and a
		subsequent &read; request. These will be treated as
		two separate requests.
	      </para>
            </listitem>
          </varlistentry>
        </variablelist>
      </para>

     <para>
       Each link contains one or more <firstterm>unit
       groups</firstterm>, all of which have to be matched by the
       transfer request. Each unit group in turn contains several
       <firstterm>units</firstterm>. The unit group is matched if at
       least one of the units is matched.
      </para>

      <section id='cf-pm-links-units'>
	<title>Types of Units</title>
	<para>
	  There are four types of units: network
	  (<literal>-net</literal>), protocol
	  (<literal>-protocol</literal>), storage class
	  (<literal>-store</literal>) and cache class
	  (<literal>-dcache</literal>) units. Each type imposes a
	  condition on the IP address, the protocol, the storage class
	  and the cache class respectively.
	</para>
	<para>
	  For each transfer at most one of each of the four unit types
	  will match.  If more than one unit of the same type could
	  match the request then the most restrictive unit matches.
	</para>
	<para>
	  The unit that matches is selected from all units defined in
	  &dcache;, not just those for a particular unit group.  This
	  means that, if a unit group has a unit that could match a
	  request but this request also matches a more restrictive
	  unit defined elsewhere then the less restrictive unit will
	  not match.
	</para>

	<variablelist>

          <varlistentry>
            <term>Network Unit</term>
            <listitem>
	      <para>
		A <emphasis>network unit</emphasis> consists of an IP
		address and a net mask written as
		<literal><replaceable>IP-address</replaceable>/<replaceable>net
		mask</replaceable></literal>, say
		<literal>111.111.111.0/255.255.255.0</literal>. It
		is satisfied, if the request is coming from a host
		with IP address within the subnet given by the
		address/netmask pair.
	      </para>

	      <screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command><parameter> -net</parameter> <replaceable>IP-address</replaceable>/<replaceable>net mask</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>IP-address</replaceable>/<replaceable>net mask</replaceable></screen>
	    </listitem>
          </varlistentry>

	  <varlistentry>
	    <term>Protocol Unit</term>
	    <listitem>
	      <para>
		A <emphasis>protocol unit</emphasis> consists of the
		name of the protocol and the version number written as
		<replaceable>protocol-name</replaceable>/<replaceable>version-number</replaceable>,
		e.g., <literal>xrootd/3</literal>.
	      </para>
		<screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command><parameter> -protocol</parameter> <replaceable>protocol-name</replaceable>/<replaceable>version-number</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>protocol-name</replaceable>/<replaceable>version-number</replaceable></screen>

	    </listitem>
	  </varlistentry>

	  <varlistentry>
	    <term>Storage Class Unit</term>
	    <listitem>
	      <para>
		A <emphasis>storage class unit</emphasis> is given by
		a storage class. It is satisfied if the requested file
		has this storage class. Simple wild cards are allowed:
		for this it is important to know that a storage class
		must always contain exactly one
		<literal>@</literal>-symbol as will be explained in
		<xref linkend="secStorageClass" />. In a storage class
		unit, either the part before the
		<literal>@</literal>-symbol or both parts may be
		replaced by a <literal>*</literal>-symbol; for
		example, <literal>*@osm</literal> and
		<literal>*@*</literal> are both valid storage class
		units whereas <literal>something@*</literal> is
		invalid. The <literal>*</literal>-symbol represents a
		limited wildcard: any string that doesn't contain an
		<literal>@</literal>-symbol will match.
	      </para>
	      <screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command> <parameter>-store</parameter> <replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>@<replaceable>type-of-storage-system</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>@<replaceable>type-of-storage-system</replaceable></screen>
	    </listitem>
	  </varlistentry>

	  <varlistentry>
	    <term>Cache Class Unit</term>
	    <listitem>
	      <para>
		A <emphasis>cache class unit</emphasis> is given by a
		cache class. It is satisfied, if the cache class of
		the requested file agrees with it.
	      </para>
	      <screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command> <parameter>-dcache</parameter> <replaceable>name-of-cache-class</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>name-of-cache-class</replaceable></screen>
	    </listitem>
	  </varlistentry>

	</variablelist>

      </section>

      <section id="cf-pm-psu-pref">
	<title>Preference Values for Type of Transfer</title>
      <para>
	The conditions for the <emphasis>type of transfer</emphasis>
	are not specified with units. Instead, each link contains four
	attributes <literal>-readpref</literal>,
	<literal>-writepref</literal>,
	<literal>-p2ppref</literal> and
	<literal>-cachepref</literal>, which specify a
	preference value for the respective types of transfer. If all
	the unit groups in the link are matched, the corresponding
	preference is assigned to each pool the link points to. Since
	we are ignoring different preference values at the moment, a
	preference of <literal>0</literal> stands for
	<literal>no</literal> and a non-zero preference stands for
	<literal>yes</literal>. A negative value for <literal>-p2ppref</literal> means, that the value for <literal>-p2ppref</literal> should equal the one for the <literal>-readpref</literal>.
      </para>

      <section>
	<title>Multiple non-zero Preference Values</title>

	<note>
	  <para>
	    This explanation of the preference values can be skipped
	    at first reading. It will not be relevant, if all non-zero
	    preference values are the same. If you want to try
	    configuring the pool manager right now without bothering
	    about the preferences, you should only use
	    <literal>0</literal> (for <literal>no</literal>) and, say,
	    <literal>10</literal> (for <literal>yes</literal>) as
	    preferences. You can choose <literal>-p2ppref=-1</literal> if it should match the value for <literal>-readpref</literal>. The first examples below are of this type.
	  </para>
	</note>

	<para>
	  If several different non-zero preference values are used,
	  the &psu; will not generate a single list but a set of
	  lists, each containing pools with the same preference. The
	  Cost Manager will use the list of pools with highest
	  preference and select the one with the lowest cost for the
	  transfer. Only if all pools with the highest preference are
	  offline, the next list will be considered by the Cost
	  Manager. This can be used to configure a set of fall-back
	  pools which are used if none of the other pools are
	  available.
	</para>
      </section>
    </section>

      <section>
        <title>Pool Groups</title>

        <para>
	  Pools can be grouped together to pool groups.
	</para>
	<screen><command>psu create pgroup</command> <replaceable>name-of-poolgroup</replaceable>
<command>psu create pool</command> <replaceable>name-of-pool</replaceable>
<command>psu addto pgroup</command> <replaceable>name-of-poolgroup</replaceable> <replaceable>name-of-pool</replaceable></screen>

	<informalexample>
	  <para>
	    Consider a host <literal>pool1</literal> with two pools,
	    <literal>pool1_1</literal> and <literal>pool1_2</literal>,
	    and a host <literal>pool2</literal> with one pool
	    <literal>pool2_1</literal>. If you want to treat them in
	    the same way, you would create a pool group and put all of
	    them in it:
	  </para>
	  <programlisting><command>psu create pgroup</command> normal-pools
<command>psu create pool</command> pool1_1
<command>psu addto pgroup</command> normal-pools pool1_1
<command>psu create pool</command> pool1_2
<command>psu addto pgroup</command> normal-pools pool1_2
<command>psu create pool</command> pool2_1
<command>psu addto pgroup</command> normal-pools pool2_1</programlisting>

          <para>
	    If you later want to treat <literal>pool1_2</literal>
	    differently from the others, you would remove it from this
	    pool group and add it to a new one:
	  </para>
	  <programlisting><command>psu removefrom pgroup</command> normal-pools pool1_2
<command>psu create pgroup</command> special-pools
<command>psu addto pgroup</command> special-pools pool1_2</programlisting>

	</informalexample>
	<para>
	  In the following, we will assume that the necessary pool
	  groups already exist. All names ending with
	  <literal>-pools</literal> will denote pool
	  groups.
	</para>

        <para>
	  Note that a pool-node will register itself with the
	  &cell-poolmngr;: The pool will be created within the &psu; and
	  added to the pool group
	  <literal>default</literal>, if that
	  exists. This is why the &dcache; system will automatically use
	  any new pool-nodes in the standard configuration: All pools
	  are in <literal>default</literal> and can
	  therefore handle any request.
	</para>
      </section>

      <section>
	<title>Define a link</title>
	<para>
	  Now we have everything we need to define a link.
	</para>
	<screen><command>psu create ugroup</command> <replaceable>name-of-unitgroup</replaceable>
<command>psu create unit</command><parameter> - <replaceable>type</replaceable></parameter> <replaceable>unit</replaceable>
<command>psu addto ugroup</command> <replaceable>name-of-unitgroup</replaceable> <replaceable>unit</replaceable>

<command>psu create pgroup</command> <replaceable>poolgroup</replaceable>
<command>psu create pool</command> <replaceable>pool</replaceable>
<command>psu addto pgroup</command> <replaceable>poolgroup</replaceable> <replaceable>pool</replaceable>

<command>psu create link</command> <replaceable>link</replaceable> <replaceable>name-of-unitgroup</replaceable>
<command>psu set link</command> <replaceable>link</replaceable> <parameter>-readpref=</parameter><replaceable>10</replaceable> <parameter>-writepref=</parameter><replaceable>0</replaceable> <parameter>-cachepref=</parameter><replaceable>10</replaceable><parameter>-p2ppref=</parameter><replaceable>-1</replaceable>
<command>psu add link</command> <replaceable>link</replaceable>  <replaceable>poolgroup</replaceable>
</screen>
      </section>

    </section>

    <section>
      <title>Examples</title>
      <para>
	Find some examples for the configuration of the &psu; below.
      </para>

      <section id="secExReadWrite">
	<title>Separate Write and Read Pools</title>

	<para>
	  The &dcache; we are going to configure receives data from a
	  running experiment, stores the data onto a tertiary storage
	  system, and serves as a read cache for users who want to
	  analyze the data.  While the new data from the experiment
	  should be stored on highly reliable and therefore expensive
	  systems, the cache functionality may be provided by
	  inexpensive hardware. It is therefore desirable to have a
	  set of pools dedicated for writing the new data and a
	  separate set for reading.
	</para>

	<informalexample>
	  <para>
	    The simplest configuration for such a setup would consist
	    of two links <quote>write-link</quote> and
	    <quote>read-link</quote>. The configuration is as follows:
	</para>
	<programlisting><command>psu create pgroup</command> read-pools
<command>psu create pool</command> pool1
<command>psu addto pgroup</command> read-pools pool1
<command>psu create pgroup</command> write-pools
<command>psu create pool</command> pool2
<command>psu addto pgroup</command> write-pools pool2

<command>psu create unit</command><parameter> -net</parameter> 0.0.0.0/0.0.0.0
<command>psu create ugroup</command> allnet-cond
<command>psu addto ugroup</command> allnet-cond 0.0.0.0/0.0.0.0

<command>psu create link</command> read-link allnet-cond
<command>psu set link</command> read-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>0 <parameter>-cachepref=</parameter>10
<command>psu add link</command> read-link read-pools

<command>psu create link</command> write-link allnet-cond
<command>psu set link</command> write-link <parameter>-readpref=</parameter>0 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>0
<command>psu add link</command> write-link write-pools</programlisting>
        <para>
	  Why is the unit group <literal>allnet-cond</literal>
	  necessary? It is used as a condition which is always true in
	  both links. This is needed, because each link must contain
	  at least one unit group.
	</para>
	</informalexample>
      </section>

      <section>
        <title>Restricted Access by IP Address</title>

        <para>
	  You might not want to give access to the pools for the whole
	  network, as in the previous example (<xref
	  linkend="secExReadWrite" />), though.
	</para>
	<informalexample>
	  <para>
	    Assume, the experiment data is copied into the cache from
	    the hosts with IP <literal>111.111.111.201</literal>,
	    <literal>111.111.111.202</literal>, and
	    <literal>111.111.111.203</literal>. As you might guess,
	    the subnet of the site is
	    <literal>111.111.111.0/255.255.255.0</literal>. Access
	    from outside should be denied. Then you would modify the
	    above configuration as follows:
	</para>
<programlisting><command>psu create pgroup</command> read-pools
<command>psu create pool</command> pool1
<command>psu addto pgroup</command> read-pools pool1
<command>psu create pgroup</command> write-pools
<command>psu create pool</command> pool2
<command>psu addto pgroup</command> write-pools pool2

<command>psu create unit</command> <parameter>-store</parameter> *@*

<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.0/255.255.255.0
<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.201/255.255.255.255
<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.202/255.255.255.255
<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.203/255.255.255.255

<command>psu create ugroup</command> write-cond
<command>psu addto ugroup</command> write-cond 111.111.111.201/255.255.255.255
<command>psu addto ugroup</command> write-cond 111.111.111.202/255.255.255.255
<command>psu addto ugroup</command> write-cond 111.111.111.203/255.255.255.255

<command>psu create ugroup</command> read-cond
<command>psu addto ugroup</command> read-cond 111.111.111.0/255.255.255.0
<command>psu addto ugroup</command> read-cond 111.111.111.201/255.255.255.255
<command>psu addto ugroup</command> read-cond 111.111.111.202/255.255.255.255
<command>psu addto ugroup</command> read-cond 111.111.111.203/255.255.255.255

<command>psu create link</command> read-link read-cond
<command>psu set link</command> read-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>0 <parameter>-cachepref=</parameter>10
<command>psu add link</command> read-link read-pools

<command>psu create link</command> write-link write-cond
<command>psu set link</command> write-link <parameter>-readpref=</parameter>0 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>0
<command>psu add link</command> write-link write-pools</programlisting>

        <important>
	  <para>
	    For a given transfer exactly zero or one storage class
	    unit, cache class unit, net unit and protocol unit will
	    match. As always the most restrictive one will match, the
	    IP <literal>111.111.111.201</literal> will match the
	    <literal>111.111.111.201/255.255.255.255</literal> unit
	    and not the <literal>111.111.111.0/255.255.255.0</literal>
	    unit. Therefore if you only add
	    <literal>111.111.111.0/255.255.255.0</literal> to the unit
	    group <quote>read-cond</quote>, the transfer request
	    coming from the IP <literal>111.111.111.201</literal> will
	    only be allowed to write and not to read. The same is true
	    for transfer requests from <literal>111.111.111.202</literal> and
	    <literal>111.111.111.203</literal>.
	  </para>
	</important>
	</informalexample>
      </section>

      <section>
        <title>Reserving Pools for Storage and Cache Classes</title>

        <para>
	  If pools are financed by one experimental group, they
	  probably do not like it if they are also used by another
	  group. The best way to restrict data belonging to one
	  experiment to a set of pools is with the help of storage
	  class conditions. If more flexibility is needed, cache class
	  conditions can be used for the same purpose.
	</para>

	<informalexample>
	  <para>
	    Assume, data of experiment A obtained in 2010 is written
	    into subdirectories in the namespace tree which are tagged
	    with the storage class
	    <literal>exp-a:run2010@osm</literal>, and
	    similarly for the other years. (How this is done is
	    described in <xref linkend="secStorageClass" />.)
	    Experiment B uses the storage class
	    <literal>exp-b:alldata@osm</literal> for
	    all its data.  Especially important data is tagged with
	    the cache class
	    <literal>important</literal>. (This is
	    described in <xref linkend="secCacheClass" />.) A suitable
	    setup would be
	  </para>
	  <programlisting><command>psu create pgroup</command> exp-a-pools
<command>psu create pool</command> pool1
<command>psu addto pgroup</command> exp-a-pools pool1

<command>psu create pgroup</command> exp-b-pools
<command>psu create pool</command> pool2
<command>psu addto pgroup</command> exp-b-pools pool2

<command>psu create pgroup</command> exp-b-imp-pools
<command>psu create pool</command> pool3
<command>psu addto pgroup</command> exp-b-imp-pools pool3

<command>psu create unit</command> <parameter>-net</parameter> 111.111.111.0/255.255.255.0
<command>psu create ugroup</command> allnet-cond
<command>psu addto ugroup</command> allnet-cond 111.111.111.0/255.255.255.0

<command>psu create ugroup</command> exp-a-cond
<command>psu create unit</command> -store exp-a:run2011@osm
<command>psu addto ugroup</command> exp-a-cond exp-a:run2011@osm
<command>psu create unit</command> -store exp-a:run2010@osm
<command>psu addto ugroup</command> exp-a-cond exp-a:run2010@osm

<command>psu create link</command> exp-a-link allnet-cond exp-a-cond
<command>psu set link</command> exp-a-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>10
<command>psu add link</command> exp-a-link exp-a-pools

<command>psu create ugroup</command> exp-b-cond
<command>psu create unit</command> <parameter>-store</parameter> exp-b:alldata@osm
<command>psu addto ugroup</command> exp-b-cond exp-b:alldata@osm

<command>psu create ugroup</command> imp-cond
<command>psu create unit</command> <parameter>-dcache</parameter> important
<command>psu addto ugroup</command> imp-cond important

<command>psu create link</command> exp-b-link allnet-cond exp-b-cond
<command>psu set link</command> exp-b-link <parameter>-readpref=</parameter>10 <parameter>-writepref=</parameter>10 <parameter>-cachepref=</parameter>10
<command>psu add link</command> exp-b-link exp-b-pools

<command>psu create link</command> exp-b-imp-link allnet-cond exp-b-cond imp-cond
<command>psu set link</command> exp-b-imp-link <parameter>-readpref=</parameter>20 <parameter>-writepref=</parameter>20 <parameter>-cachepref=</parameter>20
<command>psu add link</command> exp-b-link exp-b-imp-pools</programlisting>

          <para>
	    Data tagged with cache class
	    <quote><literal>important</literal></quote> will always be
	    written and read from pools in the pool group
	    <literal>exp-b-imp-pools</literal>, except when all pools
	    in this group cannot be reached. Then the pools in
	    <literal>exp-a-pools</literal> will be used.
	  </para>

	  <para>
	    Note again that these will never be used otherwise. Not
	    even, if all pools in <literal>exp-b-imp-pools</literal>
	    are very busy and some pools in
	    <literal>exp-a-pools</literal> have nothing to do and lots
	    of free space.
	  </para>
        </informalexample>

        <para>
	  The central IT department might also want to set up a few
	  pools, which are used as fall-back, if none of the pools of
	  the experiments are functioning. These will also be used for
	  internal testing. The following would have to be added to
	  the previous setup:
	</para>

	<informalexample>

	  <programlisting><command>psu create pgroup</command> it-pools
<command>psu create pool</command> pool_it
<command>psu addto pgroup</command> it-pools pool_it

<command>psu create link</command> fallback-link allnet-cond
<command>psu set link</command> fallback-link <parameter>-readpref=</parameter>5 <parameter>-writepref=</parameter>5 <parameter>-cachepref=</parameter>5
<command>psu add link</command> fallback-link it-pools</programlisting>

          <para>
	    Note again that these will only be used, if none of the
	    experiments pools can be reached, or if the storage class
	    is not of the form <literal>exp-a:run2009@osm</literal>,
	    <literal>exp-a:run2010@osm</literal>, or
	    <literal>exp-b:alldata@osm</literal>. If the administrator
	    fails to create the unit
	    <literal>exp-a:run2005@osm</literal> and add it to the
	    unit group <literal>exp-a-cond</literal>, the fall-back
	    pools will be used eventually.
	  </para>

	</informalexample>
      </section>
    </section>
 <!--   </section>
-->
    <section id="secStorageClass">
      <title>Storage Classes</title>

      <para>
	The storage class is a string of the form
	<literal><replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>@<replaceable>type-of-storage-system</replaceable></literal>,
	where <replaceable>type-of-storage-system</replaceable>
	denotes the type of storage system in use, and
	<replaceable>StoreName</replaceable>:<replaceable>StorageGroup</replaceable>
	is a string describing the storage class in a syntax which
	depends on the storage system. In general use
	<literal><replaceable>type-of-storage-system</replaceable>=osm</literal>.
      </para>

      <para>
	Consider for example the following setup:
      </para>

      <informalexample>
	<screen>&prompt-root; <userinput>&chimera-cli; lstag /data/experiment-a</userinput>
Total: 2
OSMTemplate
sGroup
&prompt-root; <userinput>&chimera-cli; readtag /data/experiment-a OSMTemplate</userinput>
StoreName myStore
&prompt-root; <userinput>&chimera-cli; readtag /data/experiment-a sGroup</userinput>
STRING</screen>

       <para>
	 This is the setup after a fresh installation and it will lead
	 to the storage class
	 <literal>myStore:STRING@osm</literal>. An adjustment to more
	 sensible values will look like
       </para>
<screen>&prompt-root; <userinput>echo "StoreName exp-a" | &chimera-cli; Writetag /data/experiment-a OSMTemplate</userinput>
&prompt-root; <userinput>echo "run2010" | &chimera-cli; Writetag /data/experiment-a sGroup</userinput></screen>

      <para>
	and will result in the storage class
	<literal>exp-a:run2010@osm</literal>. To summarize: The
	storage class will depend on the directory the data is stored
	in and is configurable.
      </para>

      </informalexample>
    </section>

    <section id="secCacheClass">
      <title>Cache Class</title>

      <para>
	Storage classes might already be in use for the configuration
	of a tertiary storage system. In most cases they should be
	flexible enough to configure the &psu;. However, in rare cases
	the existing configuration and convention for storage classes
	might not be flexible enough.
      </para>

      <para>
	Consider for example a situation, where data produced by an
	experiment always has the same storage class
	<literal>exp-a:alldata@osm</literal>. This is good for the
	tertiary storage system, since all data is supposed to go to
	the same tape set sequentially. However, the data also
	contains a relatively small amount of meta-data, which is
	accessed much more often by analysis jobs than the rest of the
	data. You would like to keep the meta-data on a dedicated set
	of &dcache; pools. However, the storage class does not provide
	means to accomplish that.
      </para>

      <para>
	The cache class of a directory is set by the tag
	<literal>cacheClass</literal> as follows:
      </para>
      <screen>&prompt-root; <userinput>echo "metaData" &gt; "<filename>.(tag)(cacheClass)</filename>"</userinput></screen>
      <para>
	In the above example the meta-data is stored in directories
	which are tagged in this way.
      </para>

      <para>
	There is a nice trick for easy checking of the existing tags
	in one directory:
      </para>
      <screen>&prompt-root; <userinput>grep '' `cat '.(tags)()'`</userinput>
.(tag)(OSMTemplate):StoreName exp-a
.(tag)(sGroup):run2010
.(tag)(cacheClass):metaData</screen>
      <para>
	This only works, if the quote-symbols are used
	correctly. (tick, tick, back-tick, tick, tick,
	back-tick).
      </para>

      <para>
	Tags are inherited by sub-directories: Changing a tag of a
	directory will change the tag of each sub-directory, if the
	tag has never been changed for this sub-directory
	directly. Changing tags breaks these inheritance
	links. Directories in namespace should never be moved, since
	this will mess up the inheritance structure and eventually
	break the whole system.
      </para>

    </section>

    <section id='cf-pm-linkgroups'>
      <title>Link Groups</title>

	<para>
	  The &cell-poolmngr; supports a type of objects called
	  <firstterm>link groups</firstterm>. These link groups are
	  used by the <link linkend='cf-srm-space'>&srm;
	  &cell-spacemngr;</link> to make reservations against
	  space. Each link group corresponds to a number of &dcache;
	  pools in the following way: A link group is a collection of
	  <link linkend='cf-pm-links'>links</link> and each link
	  points to a set of pools. Each link group knows about the
	  size of its available space, which is the sum of all sizes
	  of available space in all the pools included in this link
	  group.
         </para>

	   <para>
	     To create a new link group login to the <link
	     linkend='intouch-admin'>Admin Interface</link> and
	     <command>cd</command> to the &cell-poolmngr;.
	   </para>
	   <screen>&dc-prompt-local; <userinput>cd PoolManager</userinput>
&dc-prompt-pm; <userinput>psu create linkGroup <replaceable>linkgroup</replaceable></userinput>
&dc-prompt-pm; <userinput>psu addto linkGroup <replaceable>linkgroup</replaceable> <replaceable>link</replaceable></userinput>
&dc-prompt-pm; <userinput>save</userinput></screen>


           <para>
	     With <command>save</command> the changes will be saved to
	     the file
	     <filename>&path-odc-ed;/PoolManager.conf</filename>.
	   </para>

	   <note>
	     <para>
	       You can also edit the file
	       <filename>&path-odc-ed;/PoolManager.conf</filename>
	       to create a new link group. Please make sure that it
	       already exists. Otherwise you will have to create it
	       first via the Admin Interface by
	     </para>
	     <screen>&dc-prompt-pm; <userinput>save</userinput></screen>

	     <para>
	       Edit the file
	       <filename>&path-odc-ed;/PoolManager.conf</filename>
	     </para>

	     <screen><command>psu create linkGroup</command> <replaceable>linkgroup</replaceable>
<command>psu addto linkGroup</command> <replaceable>linkgroup</replaceable> <replaceable>link</replaceable></screen>

             <para>
	       After editing this file you will have to restart the
	       domain which contains the &cell-poolmngr; cell to apply
	       the changes.
	     </para>
	   </note>

         <note>
	   <para>
	     Administrators will have to take care, that a pool is not
	     present in more than one link group.
	   </para>
	 </note>

      <formalpara>
	<title>Properties of <link
	linkend='cf-srm-intro-spaceReservation'>space
	reservation</link>.</title> The &dcache; administrator can specify a
	<firstterm><literal>RetentionPolicy</literal></firstterm> and
	an <firstterm><literal>AccessLatency</literal></firstterm> for
	the space reservation, where
	<literal>RetentionPolicy</literal> describes the quality of
	the storage service that will be provided for the data (files)
	stored in this space reservation and
	<literal>AccessLatency</literal> describes the availability of
	this data.
      </formalpara>
      <para>
	A link group has five boolean properties called
	<varname>replicaAllowed</varname>,
	<varname>outputAllowed</varname>,
	<varname>custodialAllowed</varname>,
	<varname>onlineAllowed</varname> and
	<varname>nearlineAllowed</varname>. The values of these
	properties (<literal>true</literal> or
	<literal>false</literal>) can be configured via the Admin
	Interface or directly in the file
	<filename>/opt/d-cache/config/PoolManager.conf</filename>.
      </para>
      <para>
	The link groups contained in a space reservation match the
	<literal>RetentionPolicy</literal> and the
	<literal>AccessLatency</literal> of the space reservation.
      </para>

      <screen>&dc-prompt-pm; <userinput>psu set linkGroup custodialAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup outputAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup replicaAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup onlineAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput>
&dc-prompt-pm; <userinput>psu set linkGroup nearlineAllowed <replaceable>linkgroup</replaceable> <replaceable>true|false</replaceable></userinput></screen>

       <para>
	 Moreover an attribute can be assigned to a link group.
       </para>
       <screen>&dc-prompt-pm; <userinput>psu set linkGroup attribute <replaceable>linkgroup</replaceable> <replaceable>key</replaceable>=<replaceable>value</replaceable></userinput></screen>

       <informalexample>
	 Possible assignments for attributes are:
	 <screen>&dc-prompt-pm; <userinput>psu set linkGroup attribute name-of-LinkGroup VO=dteam001</userinput>
&dc-prompt-pm; <userinput>psu set linkGroup attribute name-of-LinkGroup VO=cms001</userinput>
&dc-prompt-pm; <userinput>psu set linkGroup attribute name-of-LinkGroup HSM=osm</userinput></screen>
       </informalexample>

       <note>
	 <para>
	  Please note that that it is up to administrators that the
	  link groups' attributes are specified correctly.
	 </para>
	 <para>
	   For example &dcache; will not complain if the link group
	   that does not support tape backend will be declared as one
	   that supports <literal>custodial</literal>.
	 </para>
       </note>
    </section>

  </section>

  <section id="cf-pm-cm">
    <title>The Cost Module</title>

    <para>
      From the allowable pools as determined by the <glossterm
      linkend="gl-pm-comp-psu">pool selection unit</glossterm>, the pool
      manager determines the pool used for storing or reading a file
      by calculating a <glossterm linkend="gl-cost">cost</glossterm>
      value for each pool. The pool with the lowest cost is used.
    </para>

    <para>
      If a client requests to read a file which is stored on more than
      one allowable pool, the <glossterm
      linkend="gl-performance_cost">performance costs</glossterm> are
      calculated for these pools. In short, this cost value describes
      how much the pool is currently occupied with transfers.
    </para>

    <para>
      If a pool has to be selected for storing a file, which is either
      written by a client or <glossterm
      linkend="gl-restore">restored</glossterm> from a <glossterm
      linkend="gl-tape_backend">tape backend</glossterm>, this
      performance cost is combined with a <glossterm
      linkend="gl-space_cost">space cost</glossterm> value to a
      <glossterm linkend="gl-cost">total cost</glossterm> value for
      the decision. The space cost describes how much it
      <quote>hurts</quote> to free space on the pool for the file.
    </para>

    <para>
      The <glossterm linkend="gl-pm-comp-cm">cost module</glossterm> is
      responsible for calculating the cost values for all pools.  The
      pools regularly send all necessary information about space usage
      and request queue lengths to the cost module.  It can be
      regarded as a cache for all this information.  This way it is
      not necessary to send <quote>get cost</quote> requests to the
      pools for each client request. The cost module interpolates the
      expected costs until a new precise information package is coming
      from the pools.  This mechanism prevents clumping of requests.
    </para>

    <para>
      Calculating the cost for a data transfer is done in two
      steps. First, the cost module merges all information about space
      and transfer queues of the pools to calucate the performance and
      space costs separately.  Second, in the case of a write or stage
      request, these two numbers are merged to build the total cost
      for each pool.  The first step is isolated within a separate
      loadable class.  The second step is done by the cost
      module.
    </para>

    <section id="cf-pm-cm-perf">
      <title>The Performance Cost</title>

      <para>
	The load of a pool is determined by comparing the current
	number of active and waiting <glossterm>transfers</glossterm>
	to the maximum number of concurrent transfers allowed.

	This is done separately for each of the transfer types
	(<glossterm linkend="gl-store">store</glossterm>, <glossterm
	linkend="gl-restore">restore</glossterm>, pool-to-pool client,
	pool-to-pool server, and client request) with the following
	equation:
      </para>

      <para>
	perfCost(per Type) = ( activeTransfers + waitingTransfers ) / maxAllowed .
      </para>

      <para>
	The maximum number of concurrent transfers
	(<symbol>maxAllowed</symbol>) can be configured with the commands
	<xref linkend="cmd-st_set_max_active"/> (store),
	<xref linkend="cmd-rh_set_max_active"/> (restore),
	<xref linkend="cmd-mover_set_max_active"/> (client request),
	<xref linkend="cmd-p2p_set_max_active"/> (pool-to-pool server), and
	<xref linkend="cmd-pp_set_max_active"/> (pool-to-pool client).
        <!-- (NOCLUE: <command moreinfo="refentry"><xref linkend="cmd-flush_set_max_active"/></command>) -->

	<!-- NOCLUE: Some more info about the meanings of these parameters would be nice. -->
      </para>


      <para>
	Then the average is taken for each mover type where
	<symbol>maxAllowed</symbol> is not zero. For a pool where
	store, restore and client transfers are allowed, e.g.,
      </para>

      <para>
	perfCost(total) = ( perfCost(store) + perfCost(restore) + perfCost(client) ) / 3 ,
      </para>

      <para>
	and for a read only pool:
      </para>

      <para>
	perfCost(total) = ( perfCost(restore) + perfCost(client) ) / 2 .
      </para>

      <para>
	For a well balanced system, the performance cost should not exceed 1.0.
      </para>

    </section>

    <section id="cf-pm-cm-space">
      <title>The Space Cost</title>

      <para>
	In this section only the new scheme for calculating the space
	cost will be described. Be aware, that the old scheme will be
	used if the <glossterm linkend="gl-breakeven">breakeven
	parameter</glossterm> of a pool is larger or equal 1.0.
      </para>

      <para>
	The cost value used for determining a pool for storing a file
	depends either on the free space on the pool or on the age of
	the <glossterm linkend="gl-lru">least recently used (LRU)
	file</glossterm>, which whould have to be deleted.
      </para>

      <para>
	The space cost is calculated as follows:

	<informaltable frame="none">
	  <tgroup cols="6" colsep="none" rowsep="none" align="left">
	    <colspec colnum="1" colname="if" colwidth="20"/>
	    <colspec colnum="2" colname="formula" colwidth="5*"/>
	    <colspec colnum="3" colname="and" colwidth="25"/>
	    <colspec colnum="4" colname="formula" colwidth="3*"/>
	    <colspec colnum="5" colname="then" colwidth="25"/>
	    <colspec colnum="6" colname="formula" colwidth="8*"/>
	    <tbody>
	      <row>
		<entry>If </entry>
		<entry>
		  <phrase role="math">
		    freeSpace &gt; gapPara
		  </phrase>
		</entry>
		<entry></entry>
		<entry></entry>
		<entry> then </entry>
		<entry>
		  <phrase role="math">
		    spaceCost = 3 * newFileSize / freeSpace
		  </phrase>
		</entry>
	      </row>
	      <row>
		<entry>If </entry>
		<entry>
		  <phrase role="math">
		    freeSpace &lt;= gapPara
		  </phrase>
		</entry>
		<entry>
		  and
		</entry>
		<entry>
		  <phrase role="math">
		    lruAge &lt; 60
		  </phrase>
		</entry>
		<entry> then </entry>
		<entry>
		  <phrase role="math">
		    spaceCost = 1 + costForMinute
		  </phrase>
		</entry>
	      </row>
	      <row>
		<entry>If </entry>
		<entry>
		  <phrase role="math">
		    freeSpace &lt;= gapPara
		  </phrase>
		</entry>
		<entry>
		  and
		</entry>
		<entry>
		  <phrase role="math">
		    lruAge &gt;= 60
		  </phrase>
		</entry>
		<entry> then </entry>
		<entry>
		  <phrase role="math">
		    spaceCost = 1 + costForMinute * 60 / lruAge
		  </phrase>
		</entry>
	      </row>
	    </tbody>
	  </tgroup>
	</informaltable>

	where the variable names have the following meanings:

	<variablelist>
	  <varlistentry>
	    <term><phrase role="math">freeSpace</phrase></term>
	    <listitem>
	      <para>
		The free space left on the pool
	      </para>
	    </listitem>
	  </varlistentry>
	  <varlistentry>
	    <term><phrase role="math">newFileSize</phrase></term>
	    <listitem>
	      <para>
		The size of the file to be written to one of the pools, and at least 50MB.
	      </para>
	    </listitem>
	  </varlistentry>
	  <varlistentry>
	    <term><phrase role="math">lruAge</phrase></term>
	    <listitem>
	      <para>
		The age of the <glossterm linkend="gl-lru">least
		  recently used file</glossterm> on the pool.
	      </para>
	    </listitem>
	  </varlistentry>
	  <varlistentry>
	    <term><phrase role="math">gapPara</phrase></term>
	    <listitem>
	      <para>
		The gap parameter. Default is 4GB. The size of free
		space below which it will be assumed that the pool is
		full and consequently the least recently used file has
		to be removed. If, on the other hand, the free space
		is greater than <varname>gapPara</varname>, it will be
		expensive to store a file on the pool which exceeds
		the free space.
	      </para>
	      <para>
		It can be set per pool with the <xref
		linkend="cmd-set_gap"/> command. This has to be done
		in the pool cell and not in the pool manager
		cell. Nevertheless it only influences the cost
		calculation scheme within the pool manager and not the
		bahaviour of the pool itself.
	      </para>
	    </listitem>
	  </varlistentry>
	  <varlistentry>
	    <term><phrase role="math">costForMinute</phrase></term>
	    <listitem>
	      <para>
		A parameter which fixes the space cost of a
		one-minute-old LRU file to <phrase role="math">(1 +
		costForMinute)</phrase>.  It can be set with the <xref
		linkend="cmd-set_breakeven"/>, where
	      </para>

	      <para>
		  costForMinute = breakeven * 7 * 24 * 60.
	      </para>

	      <para>
		I.e. the the space cost of a one-week-old LRU file
		will be <phrase role="math">(1 +
		breakeven)</phrase>. Note again, that all this only
		applies if <phrase role="math">breakeven &lt;
		1.0</phrase>
	      </para>
	    </listitem>
	  </varlistentry>
	</variablelist>

	The prescription above can be stated a little differently as follows:

	<informaltable frame="none">
	  <tgroup cols="4">
	    <colspec colnum="1" colname="if"       colwidth="20"/>
	    <colspec colnum="2" colname="formula"  colwidth="*"/>
	    <colspec colnum="3" colname="then"     colwidth="25"/>
	    <colspec colnum="4" colname="formula2" colwidth="2*"/>
	    <tbody>
	      <row>
		<entry>If </entry>
		<entry>
		  freeSpace &gt; gapPara
		</entry>
		<entry> then </entry>
		<entry>
		    spaceCost = 3 * newFileSize / freeSpace
		</entry>
	      </row>
	      <row>
		<entry>If </entry>
		<entry>
		  freeSpace &lt;= gapPara
		</entry>
		<entry> then </entry>
		<entry>
		    spaceCost = 1 + breakeven * 7 * 24 * 60 * 60 / lruAge
		  ,
		</entry>
	      </row>
	    </tbody>
	  </tgroup>
	</informaltable>

	where <varname>newFileSize</varname> is at least 50MB and
	<varname>lruAge</varname> at least one minute.
      </para>

      <section>
	<title>Rationale</title>

	<!-- <para>
	NOCLUE
	</para> -->

	<para>
	  As the last version of the formula suggests, a pool can be
	  in two states: Either <phrase role="math">freeSpace &gt;
	  gapPara</phrase> or <phrase role="math">freeSpace &lt;=
	  gapPara</phrase> - either there is free space left to store
	  files without deleting cached files or there isn't.
	</para>

	<para>
	  Therefore, <varname>gapPara</varname> should be around the
	  size of the smallest files which frequently might be written
	  to the pool. If files smaller than
	  <varname>gapPara</varname> appear very seldom or never, the
	  pool might get stuck in the first of the two cases with a
	  high cost.
	</para>

	<para>
	  If the LRU file is smaller than the new file, other files
	  might have to be deleted. If these are much younger than the
	  LRU file, this space cost calculation scheme might not lead
	  to a selection of the optimal pool. However, in praxis this
	  happens very seldomly and this scheme turns out to be very
	  efficient.
	</para>

      </section>

    </section>

    <section id="cf-pm-cm-total">
      <title>The Total Cost</title>

    <para>
      The total cost is a linear combination of the <glossterm
      linkend="gl-performance_cost">performance</glossterm> and
      <glossterm linkend="gl-space_cost">space cost</glossterm>.

      I.e.

<!--
<screen><varname>cost</varname> = <varname>ccf</varname> * <varname>performance_cost</varname> + <varname>scf</varname> * <varname>space_cost</varname>,</screen>
-->
<!--	<m:math xmlns:m="http://www.w3.org/1998/Math/MathML">
	  <m:mrow>
	    <m:mi>totalCost</m:mi>
	    <m:mo>=</m:mo>
	    <m:mi>ccf</m:mi>
	    <m:mo>*</m:mo>
	    <m:mi>perfCost</m:mi>
	    <m:mo>+</m:mo>
	    <m:mi>scf</m:mi>
	    <m:mo>*</m:mo>
	    <m:mi>spaceCost</m:mi>
	  </m:mrow>
	</m:math>,-->
 	<!--
	<m:math xmlns:m="http://www.w3.org/1998/Math/MathML">
	  <m:mrow>
	  <m:apply>
	    <m:eq/>
	    <m:ci>totalCost</m:ci>
	    <m:apply>
	      <m:plus/>
	      <m:apply>
		<m:times/>
		<m:ci>ccf</m:ci>
		<m:ci>perfCost</m:ci>
	      </m:apply>
	      <m:apply>
		<m:times/>
		<m:ci>scf</m:ci>
		<m:ci>spaceCost</m:ci>
	      </m:apply>
	    </m:apply>
	  </m:apply>
	  </m:mrow>
	</m:math>
	-->
	  totalCost = ccf * perfCost + scf * spaceCost ,


      where <varname>ccf</varname> and <varname>scf</varname> are
      configurable with the command <xref
      linkend="cmd-set_pool_decision"/>.

      E.g.,

<screen>&dc-prompt-pm; <userinput>set pool decision <option>-spacecostfactor=3</option> <option>-cpucostfactor=1</option></userinput></screen>

      will give the <glossterm linkend="gl-space_cost">space
      cost</glossterm> three times the weight of the <glossterm
      linkend="gl-performance_cost">performance cost</glossterm>.
    </para>

    </section>

  </section>

</chapter>
