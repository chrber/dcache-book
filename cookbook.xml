<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE part PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" "http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">     

<part id="cb" xmlns:xi="http://www.w3.org/2001/XInclude">
  <!-- <?dbhtml dir="cookbook" ?> -->
  <title>Cookbook</title>
  
  <partintro>
    <para>
      This part contains guides for specific tasks a system
      administrator might want to perform. 
    </para> 
  </partintro>

  <chapter id="cb-general">
    <title>General Problems</title>

    <section id="cb-general-opteron">
      <title>Installing dCache on Opteron Machines</title>

      <para>
	The PNFS server, dCache server, and dCache client software
	have to be taken care of:
      </para>

      

      <section id="cb-general-opteron-server">
	<title>The dCache Server</title>

	<para>
	  The major part of the dCache server softwar is written in
	  Java. Therefore the Java Virtual Machine with 64 bit
	  extension needs to be installed. It either is included in
	  the regular Java distribution or additional packages have to
	  be downloaded.
	</para>

      </section>

      <section id="cb-general-opteron-client">
	<title>The dCache Client</title>

	<para>
	  The <glossterm id="gl-dcap-library">dCap library</glossterm>
	  and the command line tool <command>dccp</command> may be
	  downloaded from <ulink
	  url="http://www.dcache.org/downloads/">
	  http://www.dcache.org/downloads/</ulink> for several
	  architectures. The source of the client software may also be
	  downloaded from <ulink
	  url="http://www.dcache.org/downloads/cvs.shtml">
	  http://www.dcache.org/downloads/cvs.shtml</ulink> and
	  compiled. As of this writing, this has not been tested for
	  the Opteron architecture. Please, contact
	  <email>support@dcache.org</email> when encountering any
	  problems with this.
	</para>

      </section>

      <section id="cb-general-opteron-pnfs">
	<title>PNFS Server</title>

	<para>
	  The current version of the PNFS server software is written
	  in C and has never been compiled for any 64-bit
	  architecture. Since a Java implementation is in preparation,
	  there are no plans to do that. Therefore the PNFS server has
	  to be run in compat mode.
	</para>

      </section>

    </section>

  </chapter>

  <chapter id="cb-mon-op">
    <title>Monitoring and Operation of a dCache System</title>

    <para>Maybe this should go into the configuration part.</para>

    <para>
      This chapter describes the various methods to monitor a running
      dCache system and gives a general introduction on its operation.
      Every administrator should be familiar with this material.
    </para>
    
    <section id="cb-mon-op-web">
      <title>The Web Interface</title>

      <para>
	In the standard configuration the dCache web interface is
	started on the admin node and can be reached via port
	<literal>2288</literal> (configurable in
	<filename>config/dCacheSetup</filename><footnote>
	  <para>
	    Filenames will always be relative to the dCache
	    installation directory, which defaults to
	    <filename>/opt/d-cache/</filename>.
	  </para>
	</footnote>). Point a web browser to <ulink
	url="http://your-admin-node.your-domain:2288/">http://<replaceable>your-admin-node</replaceable>.<replaceable>your-domain</replaceable>:2288/</ulink>
	to get to the main menue of the dCache web interface.  The
	contents of the web interface are self-explanatory and are
	useful for most monitoring tasks.
    </para>
    </section>

    <section id="cb-mon-op-ssh">
      <title>The SSH Login Interface</title>

      

    </section>

    <section id="cb-mon-op-gui">
      <title>The Graphical Administration Interface</title>

      

    </section>

  </chapter>
  
  <chapter id="cb-pool">
    <title>Pool Operations</title>
    
    
    <section id="cb-pool-remove">
      <title>Removing a Pool</title>
      
      <section id="cb-pool-remove-precious">
	<title>Removing a Pool with Precious Files on it</title>
	
	<para>
	  TODO
	</para>
      </section>
      
      <section id="cb-pool-remove-cached">
	<title>Removing a Pool with only cached Files</title>
	
	<para>
	  TODO
	</para>
      </section>
      
    </section>
    
    <section id="cb-pool-rename">
      <title>Renaming a Pool</title>
      
      <para>A pool may be renamed with the following procedure,
	regardless of the type of files stored on it.</para>
      
      <para>
	Disable file transfers from and to the pool with
	<screen><dcpoolprompt></dcpoolprompt><command>pool disable</command> <option>-strict</option></screen>
	Then make shure, no transfers are being processed anymore.
	All the following commands should give no output:
	<screen><dcpoolprompt></dcpoolprompt><command>queue ls queue</command>
<dcpoolprompt></dcpoolprompt><command>mover ls</command>
<dcpoolprompt></dcpoolprompt><command>p2p ls</command>
<dcpoolprompt></dcpoolprompt><command>pp ls</command>
<dcpoolprompt></dcpoolprompt><command>st jobs ls</command>
<dcpoolprompt></dcpoolprompt><command>rh jobs ls</command></screen>
	Now the files on the pools have to be unregistered on the 
	PNFS server with
	<screen><dcpoolprompt></dcpoolprompt><command>pnfs unregister</command></screen>
	Even if the pool contains precious files, this is no problem, since
	we will register them again in a moment. The files might not be available
	for a short moment, though.
	Log out of the pool, and stop the service: TODO
	Rename the pool in the <filename><replaceable>poolDomain</replaceable>.poollist</filename>-file.
	Restart the service: TODO
	Register the files on the pool with
	<screen><dcpoolprompt/><command>pnfs register</command></screen>
      </para>
      
    </section>

    <section id="cb-pool-pin">
      <title>Pinning Files to a Pool</title>

      <para>
	You may pin a file locally within the private pool repository:
        <screen><command>rep set sticky</command> <replaceable>pnfsid</replaceable> <option>on|off</option></screen> 
        the 'sticky' mode will stay with the file as long as the file
        is in the pool.  If the file is removed from the pool and
        recreated afterwards this information gets lost.  
      </para>

      <para>
	You may use the same mechanism globally:  in the command line
	interface (local mode) there is the command
	<screen><command>set sticky</command> <replaceable>pnfsid</replaceable></screen>
	This command does: 
	<orderedlist>
	  <listitem>
	    flags the file as sticky in the name space database
	    (pnfs). So from now the filename is globally set sticky.
	  </listitem>
	  <listitem>
	    will go to all pools where it finds the file and will flag
	    it sticky in the pools.
	  </listitem>
	  <listitem>
	    all new copies of the file will become sticky.
	  </listitem>
	</orderedlist>
      </para>
    </section>
  </chapter>
  
  
<!--
  <chapter id="cb-pool-op">
    <title>Pool Operation</title> 

    <para>
      Each file in a pool has one of the 4 primary states: "cached",
      "precious", "from client", "from store". You can find out about
      it with "rep ls &lt;PNSFID&gt;": &lt;C- - -....&gt; means "cached", &lt;-P- - -...&gt;
      means "precious", &lt;- -C-...&gt; means from client, and &lt;- - -S...&gt;
      means from store. "rep set cached" simply sets it to cached. It
      could cause a file which is only stored on disk to be
      deleted. That is why it is potentially dangerous. Since your
      files are on tape, there is no danger.
    </para>

    <para>
      If the stageing hasnt finished before the time-out, the file
      might not be complete. You should be able to check that with the
      size and/or checksum: In PoolManager:
<screen>storageinfoof &lt;PNFSID&gt;</screen>
      gives the desired size and - maybe, depending on the method the
      file was written - the ALDER checksum in
      <quote>flag-c=1:&lt;alder32(hex)&gt;</quote>. You can compare it
      with the file on disk.
    </para>

    <para>
      My guess would be:

      If the state is "from store", the file is not complete and you
      have to stage it again. If it is complete, you can set it to
      cached with no harm. It should not be in any other state.
    </para>

    <para>
      If "ls rep" does not list it at all: I know that there is a
      mechanism to register files, which are on disk - including
      checking the checksums and generating the local control
      data. Unfortunately, i do not know of a way to trigger it other
      than to restart the whole pool. It will then do this
      registration for all files it finds which have incomplete
      controll information. This might take a while if there are a lot
      of them.
    </para>
  </chapter>
-->

  <chapter id="cb-net">
    <title>Complex Network Topologies</title>

    <para>
      This chapter contains solutions for several non-trivial network
      topologies. Even though not every case is covered, these cases
      might help solve other problems, as well. Intermediate knowledge
      about dCache is required. Since most tasks require changes in
      the start-up configuration,the background information on how to
      configure the cell start-up, given in <xref
      linkend="cf-cell-startup"/> will be useful.
    </para>

    <section id="cb-net-second-if">
      <title>GridFTP Connections via two or more Network Interfaces</title>
      
      <section>
	<title>Description</title>
	<para>
	  The host on which the GridFTP door is running has several
	  network interfaces and is supposed to accept client
	  connections via all those interfaces. The interfaces might
	  even belong to separate networks with no routing from one
	  network to the other.
	</para>

	<para>
          As long as the data connection is opened by the GridFTP
          server (active FTP mode), there is no problem with having
          more than one interface. However, when the client opens the
          data connection (passive FTP mode), the door (FTP server)
          has to supply it with the correct interface it should
          connect to. If this is the wrong interface, the client might
          not be able to connect to it, because there is no route or
          the connection might be inefficient.
	</para>

	<para>
	  Also, since a GridFTP server has to authenticate with an SSL
	  grid certificate and key, there needs to be a separate
	  certificate and key pair for each name of the host. Since
	  each network interface might have a different name, several
	  certificates and keys are needed and the correct one has to
	  be used, when authenticating via each of the interfaces.
	</para>
      </section>
      
      <section>
	<title>Solution</title>
	<para>
	  Start a separate GridFTP server cell on the host for each
	  interface on which connections should be
	  accepted. 
	</para>
	
	<para>
	  The cells may be started in one domain or in separate
	  domains. The cells have to have different names, since they
	  are <glossterm linkend="gl-well-known">well
	  known</glossterm> cells. Each cell has to be configured,
	  only to listen on the interface it should serve with the
	  <option>-listen</option> option. The locations of the grid
	  host certificate and key files for the interface have to be
	  specified explicitly with the <option>-service-cert</option>
	  and <option>-service-key</option> options.
	</para>

	<para>
	  The following example shows a setup for two network
	  interfaces with the hostnames
	  <literal>door-a.grid.domain</literal> (111.111.111.5) and
	  <literal>door-b.other.domain</literal> (222.222.222.5) which are served by two
	  GridFTP door cells in one domain:
	</para>
	
	<example>
	  <title>Batch file for two GridFTP doors serving separate network interfaces</title>
	  <programlisting>set printout default 2
set printout CellGlue none
onerror shutdown
check -strong setupFile
copy file:${setupFile} context:setupContext
import context -c setupContext
check -strong serviceLocatorPort serviceLocatorHost
check -strong sshPort ftpPort
create dmg.cells.services.RoutingManager  RoutingMgr
create dmg.cells.services.LocationManager lm \
       "${serviceLocatorHost} ${serviceLocatorPort}"

create dmg.cells.services.login.LoginManager <emphasis>GFTP-door-a</emphasis> \
            "2811 \
	     <emphasis>-listen=111.111.111.5 \</emphasis>
             -export \
             diskCacheV111.doors.GsiFtpDoorV1 \
             -prot=raw \
             <emphasis>-service-cert=/etc/grid-security/door-a.grid.domain-cert.pem \
             -service-key=/etc/grid-security/door-a.grid.domain-key.pem \</emphasis>
             -clientDataPortRange=${clientDataPortRange} \
             -root=${ftpBase} \
             -kpwd-file=${kpwdFile} \
             -tlog=/tmp/dcache-ftp-tlog \
             -maxLogin=100 \
             -brokerUpdateTime=5 \
             -protocolFamily=gsiftp \
             -loginBroker=LoginBroker \
             -poolManagerTimeout=5400 \
             -pnfsTimeout=120 \
             -maxRetries=80 \
             -maxStreamsPerClient=10 \
"

create dmg.cells.services.login.LoginManager <emphasis>GFTP-door-b</emphasis> \
            "2811 \
             <emphasis>-listen=222.222.222.5 \</emphasis>
             -export \
             diskCacheV111.doors.GsiFtpDoorV1 \
             -prot=raw \
             <emphasis>-service-cert=/etc/grid-security/door-b.other.domain-cert.pem \
             -service-key=/etc/grid-security/door-b.other.domain-key.pem \</emphasis>
             -clientDataPortRange=${clientDataPortRange} \
            -root=${ftpBase} \
             -kpwd-file=${kpwdFile} \
             -tlog=/tmp/dcache-ftp-tlog \
             -maxLogin=100 \
             -brokerUpdateTime=5 \
             -protocolFamily=gsiftp \
             -loginBroker=LoginBroker \
             -poolManagerTimeout=5400 \
             -pnfsTimeout=120 \
             -maxRetries=80 \
             -maxStreamsPerClient=10 \
"</programlisting>
	</example>

	<para>
	  This batch file is very similar to the batch file for the
	  GridFTP door in the standard setup. (Comments have been left
	  out.) It only contains an additional create command for the
	  second cell and the emphasized changes within the two create
	  commands: The cell names, the <option>-listen</option>
	  option with the IP address of the corresponding interface
	  and the <option>-service-cert</option> and
	  <option>-service-key</option> options with the host
	  certificate and key files.
	</para>
      </section>
      
    </section> 
    
    <section id="cb-net-pool-priv">
      <title>GridFTP with Pools in a Private Subnet</title>

      <section>
	<title>Description</title>
	<para>
	  If pool nodes of a dCache instance are connected to a
	  <glossterm linkend="gl-secondary-interface">secondary
	  interface</glossterm> of the GridFTP door, e.g. because they
	  are in a private subnet, the GridFTP door will still tell
	  the pool to connect to its primary interface, which might be
	  unreachable. 
	</para>

	<para>
	  The reason for this is that the control communication
	  between the door and the pool is done via the network of TCP
	  connections which have been established at start-up. In the
	  standard setup this communication is routed via the dCache
	  domain. However, for the data transfer, the pool connects to
	  the GridFTP door. The IP address it connects to is sent by
	  the GridFTP door to the pool via the control
	  connection. Since the GridFTP door cannot find out which of
	  its interfaces the pool should use, it normally sends the IP
	  address of the <glossterm
	  linkend="gl-primary-interface">primary
	  interface</glossterm>.
	</para>
      </section>

      <section>
	<title>Solution</title>
	<para>
	  Tell the GridFTP door explicitly which IP it should send to
	  the pool for the data connection with the
	  <option>-ftp-adapter-internal-interface</option>
	  option. E.g. if the pools should connect to the secondary
	  interface of the GridFTP door host which has the IP address
	  <literal>10.0.1.1</literal>, the following batch file would
	  be appropriate:
	</para>

	<example>
	  <title>Batch file for two GridFTP doors serving separate network interfaces</title>
	  <programlisting>set printout default 2
set printout CellGlue none
onerror shutdown
check -strong setupFile
copy file:${setupFile} context:setupContext
import context -c setupContext
check -strong serviceLocatorPort serviceLocatorHost
check -strong sshPort ftpPort
create dmg.cells.services.RoutingManager  RoutingMgr
create dmg.cells.services.LocationManager lm \
       "${serviceLocatorHost} ${serviceLocatorPort}"

create dmg.cells.services.login.LoginManager GFTP \
            "2811 \
             -export \
             diskCacheV111.doors.GsiFtpDoorV1 \
             -prot=raw \
             -clientDataPortRange=${clientDataPortRange} \
            -root=${ftpBase} \
             -kpwd-file=${kpwdFile} \
             -tlog=/tmp/dcache-ftp-tlog \
             -maxLogin=100 \
             -brokerUpdateTime=5 \
             -protocolFamily=gsiftp \
             -loginBroker=LoginBroker \
             -poolManagerTimeout=5400 \
             -pnfsTimeout=120 \
             -maxRetries=80 \
             -maxStreamsPerClient=10 \
	    <emphasis>-ftp-adapter-internal-interface=10.0.1.1 \</emphasis>
"</programlisting>
	</example>

	<para>
	  This batch file is very similar to the batch file for the
	  GridFTP door in the standard setup. (Comments have been left
	  out.) The emphasized last line has the desired effect.
	</para>
	  
      </section>

    </section>

    <section id="cb-net-dmz">
      <title>Doors in the DMZ</title>
      
      <section>
	<title>Description</title>
	<para>
	  Some doors - e.g. for grid access - are located in the DMZ
	  while the rest of the dCache instance is in the
	  intranet. The firewall is configured in such a way that the
	  doors cannot reach the location manager (usually on the
	  admin node together with the pool manager) via port NOCLUE.
	</para>
      </section>

      <section>
	<title>Solution</title>
	<para>
	  Please contact <email>support@dcache.org</email> if you need
	  a solution for this problem.
	</para>
      </section>

    </section>



  </chapter>


  <chapter id="cb-accounting">
    <title>Accounting</title>

    <para>
      TODO: There are a few loose ends in this chapter. A meta-intro might
      also be in order.
    </para>

    <para>
      The raw information about all dCache activities can be found in
      <filename>billing/<replaceable>YYYY</replaceable>/<replaceable>MM</replaceable>/billing-<replaceable>YYYY.MM.DD</replaceable></filename>.<footnote>
	<para>
	  Filenames will always be relative to the dCache installation
	  directory, which defaults to
	  <filename>/opt/d-cache/</filename>. 
	</para>
      </footnote>
      A typical line looks like
      <informalexample>
	<screen>05.31 22:35:16 [pool:<replaceable>pool-name</replaceable>:transfer] [000100000000000000001320,24675] myStore:STRING@osm 24675 474 true {GFtp-1.0 <replaceable>client-host-fqn</replaceable> 37592} {0:""}</screen>
      </informalexample>
      EXPLAIN! SYNOPSIS? Maybe too confusing
      
      It seems (though we are not sure) that the ftp transfer
      into the system was interrupted after having transferred
      14 bytes. Here you possibly found a small problem in the
      dCache :-) . The 14 in [PNFSID,14] is the number of bytes
      on the pool disk. The number '-1' SHOULD be the bytes
      transferred into the pool. This is inconsistent.
      It is supposed to be -1 if the mover HASN"T finished the transfer
      ftp protocolwise. On the other hand, if it was
      interrupted, it SHOULDN"T  say    {0,""} because
      this means : no error detected.
      The 474 is the number of milliseconds the transfer took
      and the 37592 is the client host data transfer listen port.
    </para>

    <para>
      The dCache web interface (described in <xref
      linkend="cb-mon-op-web"/>) contains under the menue point
      <quote>Actions Log</quote> summary information extracted from
      the information in the <filename>billing</filename>-directory.
    </para>

    <para>
      The accounting information can also be redirected into a
      database. For this the start-up configuration of the standard
      admin node has to be modified. (See <xref
      linkend="cf-cell-startup"/> for background information.) The
      billing information is written by the <quote>billing</quote>
      cell, which is started in the <quote>httpd</quote>
      domain. Therefore, the corresponding
      <command>create</command>-line in the
      <filename>config/httpd.batch</filename> file has to be changed
      to something like
      <programlisting>create ... TODO: FILL</programlisting>

      TODO: Create Tables? Schema?

      After that, the <quote>httpd</quote> domain has to be restarted with
<screen><rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/httpd stop
<rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/httpd -logfile=<replaceable>/opt/d-cache/</replaceable>log/httpd.log start</screen>
    </para>

    <para>
      There is also a nice little script which converts the billing
      files into something nice and neat. It can be found at
      <email>support@dcache.org</email>.
    </para>

  </chapter>

  <chapter id="cb-proto">
    <title>Protocols</title>
    
    <section id="cb-proto-dis-dcap">
      <title>Disableing unauthenticated dCap via SRM</title>

      <para>
	NOCLUE: Meta-intro? Where to put this? A more general section about
	protocolls in Configuration?
      </para>

      <para>
	In some cases SRM transfers fail because they are tried via
	the plain dCap protocol (URL starts with
	<literal>dcap://</literal>). Since plain dCap is
	unauthenticated, the dCache server will have no information
	about the user trying to access the system. While the transfer
	will succeed if the UNIX file permissions allow access to
	anybody (e.g. mode 777), it will fail otherwise. 
      </para>

      <para>
	Usually all doors are registered in SRM as potential access
	points for dCache. During a protocol negotiation the SRM
	chooses one of the available doors. You can force srmcp to use
	the gsidcap protocol (<option>-protocol=gsidcap</option>) or
	you can unregister plain, unauthenticated dCap from known
	protocols: From the file
	<filename>config/door.batch</filename> remove
	<option>-loginBroker=LoginBroker</option> and restart dcap
	door with

<screen><rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/door stop
<rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/door -logfile=<replaceable>/opt/d-cache/</replaceable>log/door.log start</screen>

      </para>

    </section>

  </chapter>

</part>
