<?xml version="1.0" encoding="UTF-8"?>
<!DOCTYPE part PUBLIC "-//OASIS//DTD DocBook XML V4.2//EN" "http://www.oasis-open.org/docbook/xml/4.2/docbookx.dtd">     

<part id="cb" xmlns:xi="http://www.w3.org/2001/XInclude">
  <!-- <?dbhtml dir="cookbook" ?> -->
  <title>Cookbook</title>
  
  <partintro>
    <para>
      This part contains guides for specific tasks a system
      administrator might want to perform. 
    </para> 
  </partintro>

  <chapter id="cb-general">
    <title>General Problems</title>

    <section id="cb-general-opteron">
      <title>Installing dCache on Opteron Machines</title>

      <para>
	The PNFS server, dCache server, and dCache client software
	have to be taken care of:
      </para>

      

      <section id="cb-general-opteron-server">
	<title>The dCache Server</title>

	<para>
	  The major part of the dCache server softwar is written in
	  Java. Therefore the Java Virtual Machine with 64 bit
	  extension needs to be installed. It either is included in
	  the regular Java distribution or additional packages have to
	  be downloaded.
	</para>

      </section>

      <section id="cb-general-opteron-client">
	<title>The dCache Client</title>

	<para>
	  The <glossterm id="gl-dcap-library">dCap library</glossterm>
	  and the command line tool <command>dccp</command> may be
	  downloaded from <ulink
	  url="http://www.dcache.org/downloads/">
	  http://www.dcache.org/downloads/</ulink> for several
	  architectures. The source of the client software may also be
	  downloaded from <ulink
	  url="http://www.dcache.org/downloads/cvs.shtml">
	  http://www.dcache.org/downloads/cvs.shtml</ulink> and
	  compiled. As of this writing, this has not been tested for
	  the Opteron architecture. Please, contact
	  <email>support@dcache.org</email> when encountering any
	  problems with this.
	</para>

      </section>

      <section id="cb-general-opteron-pnfs">
	<title>PNFS Server</title>

	<para>
	  The current version of the PNFS server software is written
	  in C and has never been compiled for any 64-bit
	  architecture. Since a Java implementation is in preparation,
	  there are no plans to do that. Therefore the PNFS server has
	  to be run in compat mode.
	</para>

      </section>

    </section>

  </chapter>

  <chapter id="cb-mon-op">
    <title>Monitoring and Operation of a dCache System</title>

    <para>Maybe this should go into the configuration part.</para>

    <para>
      This chapter describes the various methods to monitor a running
      dCache system and gives a general introduction on its operation.
      Every administrator should be familiar with this material.
    </para>
    
    <section id="cb-mon-op-web">
      <title>The Web Interface</title>

      <para>
	In the standard configuration the dCache web interface is
	started on the admin node and can be reached via port
	<literal>2288</literal> (configurable in
	<filename>config/dCacheSetup</filename><footnote>
	  <para>
	    Filenames will always be relative to the dCache
	    installation directory, which defaults to
	    <filename>/opt/d-cache/</filename>.
	  </para>
	</footnote>). Point a web browser to <ulink
	url="http://your-admin-node.your-domain:2288/">http://<replaceable>your-admin-node</replaceable>.<replaceable>your-domain</replaceable>:2288/</ulink>
	to get to the main menue of the dCache web interface.  The
	contents of the web interface are self-explanatory and are
	useful for most monitoring tasks.
    </para>
    </section>

    <section id="cb-mon-op-ssh">
      <title>The SSH Login Interface</title>

      

    </section>

    <section id="cb-mon-op-gui">
      <title>The Graphical Administration Interface</title>

      

    </section>

  </chapter>
  
  <chapter id="cb-pool">
    <title>Pool Operations</title>
    
    
    <section id="cb-pool-remove">
      <title>Removing a Pool</title>
      
      <section id="cb-pool-remove-precious">
	<title>Removing a Pool with Precious Files on it</title>
	
	<para>
	  TODO
	</para>
      </section>
      
      <section id="cb-pool-remove-cached">
	<title>Removing a Pool with only cached Files</title>
	
	<para>
	  TODO
	</para>
      </section>
      
    </section>
    
    <section id="cb-pool-rename">
      <title>Renaming a Pool</title>
      
      <para>A pool may be renamed with the following procedure,
	regardless of the type of files stored on it.</para>
      
      <para>
	Disable file transfers from and to the pool with
	<screen><dcpoolprompt></dcpoolprompt><command>pool disable</command> <option>-strict</option></screen>
	Then make shure, no transfers are being processed anymore.
	All the following commands should give no output:
	<screen><dcpoolprompt></dcpoolprompt><command>queue ls queue</command>
<dcpoolprompt></dcpoolprompt><command>mover ls</command>
<dcpoolprompt></dcpoolprompt><command>p2p ls</command>
<dcpoolprompt></dcpoolprompt><command>pp ls</command>
<dcpoolprompt></dcpoolprompt><command>st jobs ls</command>
<dcpoolprompt></dcpoolprompt><command>rh jobs ls</command></screen>
	Now the files on the pools have to be unregistered on the 
	PNFS server with
	<screen><dcpoolprompt></dcpoolprompt><command>pnfs unregister</command></screen>
	Even if the pool contains precious files, this is no problem, since
	we will register them again in a moment. The files might not be available
	for a short moment, though.
	Log out of the pool, and stop the service: TODO
	Rename the pool in the <filename><replaceable>poolDomain</replaceable>.poollist</filename>-file.
	Restart the service: TODO
	Register the files on the pool with
	<screen><dcpoolprompt/><command>pnfs register</command></screen>
      </para>
      
    </section>

    <section id="cb-pool-pin">
      <title>Pinning Files to a Pool</title>

      <para>
	You may pin a file locally within the private pool repository:
        <screen><command>rep set sticky</command> <replaceable>pnfsid</replaceable> <option>on|off</option></screen> 
        the 'sticky' mode will stay with the file as long as the file
        is in the pool.  If the file is removed from the pool and
        recreated afterwards this information gets lost.  
      </para>

      <para>
	You may use the same mechanism globally:  in the command line
	interface (local mode) there is the command
	<screen><command>set sticky</command> <replaceable>pnfsid</replaceable></screen>
	This command does: 
	<orderedlist>
	  <listitem>
	    flags the file as sticky in the name space database
	    (pnfs). So from now the filename is globally set sticky.
	  </listitem>
	  <listitem>
	    will go to all pools where it finds the file and will flag
	    it sticky in the pools.
	  </listitem>
	  <listitem>
	    all new copies of the file will become sticky.
	  </listitem>
	</orderedlist>
      </para>
    </section>
  </chapter>
  
  
<!--
  <chapter id="cb-pool-op">
    <title>Pool Operation</title> 

    <para>
      Each file in a pool has one of the 4 primary states: "cached",
      "precious", "from client", "from store". You can find out about
      it with "rep ls &lt;PNSFID&gt;": &lt;C- - -....&gt; means "cached", &lt;-P- - -...&gt;
      means "precious", &lt;- -C-...&gt; means from client, and &lt;- - -S...&gt;
      means from store. "rep set cached" simply sets it to cached. It
      could cause a file which is only stored on disk to be
      deleted. That is why it is potentially dangerous. Since your
      files are on tape, there is no danger.
    </para>

    <para>
      If the stageing hasnt finished before the time-out, the file
      might not be complete. You should be able to check that with the
      size and/or checksum: In PoolManager:
<screen>storageinfoof &lt;PNFSID&gt;</screen>
      gives the desired size and - maybe, depending on the method the
      file was written - the ALDER checksum in
      <quote>flag-c=1:&lt;alder32(hex)&gt;</quote>. You can compare it
      with the file on disk.
    </para>

    <para>
      My guess would be:

      If the state is "from store", the file is not complete and you
      have to stage it again. If it is complete, you can set it to
      cached with no harm. It should not be in any other state.
    </para>

    <para>
      If "ls rep" does not list it at all: I know that there is a
      mechanism to register files, which are on disk - including
      checking the checksums and generating the local control
      data. Unfortunately, i do not know of a way to trigger it other
      than to restart the whole pool. It will then do this
      registration for all files it finds which have incomplete
      controll information. This might take a while if there are a lot
      of them.
    </para>
  </chapter>
-->

  <chapter id="cb-net">
    <title>Complex Network Topologies</title>

    <para>
      This chapter contains solutions for several non-trivial network
      topologies. Even though not every case is covered, these cases
      might help solve other problems, as well. Intermediate knowledge
      about dCache is required. Since most tasks require changes in
      the start-up configuration,the background information on how to
      configure the cell start-up, given in <xref
      linkend="cf-cell-startup"/> will be useful.
    </para>

    <section id="cb-net-second-if">
      <title>GridFTP Connections via two or more Network Interfaces</title>
      
      <section>
	<title>Description</title>
	<para>
	  The host on which the GridFTP door is running has several
	  network interfaces and is supposed to accept client
	  connections via all those interfaces. The interfaces might
	  even belong to separate networks with no routing from one
	  network to the other.
	</para>

	<para>
          As long as the data connection is opened by the GridFTP
          server (active FTP mode), there is no problem with having
          more than one interface. However, when the client opens the
          data connection (passive FTP mode), the door (FTP server)
          has to supply it with the correct interface it should
          connect to. If this is the wrong interface, the client might
          not be able to connect to it, because there is no route or
          the connection might be inefficient.
	</para>

	<para>
	  Also, since a GridFTP server has to authenticate with an SSL
	  grid certificate and key, there needs to be a separate
	  certificate and key pair for each name of the host. Since
	  each network interface might have a different name, several
	  certificates and keys are needed and the correct one has to
	  be used, when authenticating via each of the interfaces.
	</para>
      </section>
      
      <section>
	<title>Solution</title>
	<para>
	  Start a separate GridFTP server cell on the host for each
	  interface on which connections should be
	  accepted. 
	</para>
	
	<para>
	  The cells may be started in one domain or in separate
	  domains. The cells have to have different names, since they
	  are <glossterm linkend="gl-well-known">well
	  known</glossterm> cells. Each cell has to be configured,
	  only to listen on the interface it should serve with the
	  <option>-listen</option> option. The locations of the grid
	  host certificate and key files for the interface have to be
	  specified explicitly with the <option>-service-cert</option>
	  and <option>-service-key</option> options.
	</para>

	<para>
	  The following example shows a setup for two network
	  interfaces with the hostnames
	  <literal>door-a.grid.domain</literal> (111.111.111.5) and
	  <literal>door-b.other.domain</literal> (222.222.222.5) which are served by two
	  GridFTP door cells in one domain:
	</para>
	
	<example>
	  <title>Batch file for two GridFTP doors serving separate network interfaces</title>
	  <programlisting>set printout default 2
set printout CellGlue none
onerror shutdown
check -strong setupFile
copy file:${setupFile} context:setupContext
import context -c setupContext
check -strong serviceLocatorPort serviceLocatorHost
check -strong sshPort ftpPort
create dmg.cells.services.RoutingManager  RoutingMgr
create dmg.cells.services.LocationManager lm \
       "${serviceLocatorHost} ${serviceLocatorPort}"

create dmg.cells.services.login.LoginManager <emphasis>GFTP-door-a</emphasis> \
            "2811 \
	     <emphasis>-listen=111.111.111.5 \</emphasis>
             -export \
             diskCacheV111.doors.GsiFtpDoorV1 \
             -prot=raw \
             <emphasis>-service-cert=/etc/grid-security/door-a.grid.domain-cert.pem \
             -service-key=/etc/grid-security/door-a.grid.domain-key.pem \</emphasis>
             -clientDataPortRange=${clientDataPortRange} \
             -root=${ftpBase} \
             -kpwd-file=${kpwdFile} \
             -tlog=/tmp/dcache-ftp-tlog \
             -maxLogin=100 \
             -brokerUpdateTime=5 \
             -protocolFamily=gsiftp \
             -loginBroker=LoginBroker \
             -poolManagerTimeout=5400 \
             -pnfsTimeout=120 \
             -maxRetries=80 \
             -maxStreamsPerClient=10 \
"

create dmg.cells.services.login.LoginManager <emphasis>GFTP-door-b</emphasis> \
            "2811 \
             <emphasis>-listen=222.222.222.5 \</emphasis>
             -export \
             diskCacheV111.doors.GsiFtpDoorV1 \
             -prot=raw \
             <emphasis>-service-cert=/etc/grid-security/door-b.other.domain-cert.pem \
             -service-key=/etc/grid-security/door-b.other.domain-key.pem \</emphasis>
             -clientDataPortRange=${clientDataPortRange} \
            -root=${ftpBase} \
             -kpwd-file=${kpwdFile} \
             -tlog=/tmp/dcache-ftp-tlog \
             -maxLogin=100 \
             -brokerUpdateTime=5 \
             -protocolFamily=gsiftp \
             -loginBroker=LoginBroker \
             -poolManagerTimeout=5400 \
             -pnfsTimeout=120 \
             -maxRetries=80 \
             -maxStreamsPerClient=10 \
"</programlisting>
	</example>

	<para>
	  This batch file is very similar to the batch file for the
	  GridFTP door in the standard setup. (Comments have been left
	  out.) It only contains an additional create command for the
	  second cell and the emphasized changes within the two create
	  commands: The cell names, the <option>-listen</option>
	  option with the IP address of the corresponding interface
	  and the <option>-service-cert</option> and
	  <option>-service-key</option> options with the host
	  certificate and key files.
	</para>
      </section>
      
    </section> 
    
    <section id="cb-net-pool-priv">
      <title>GridFTP with Pools in a Private Subnet</title>

      <section>
	<title>Description</title>
	<para>
	  If pool nodes of a dCache instance are connected to a
	  <glossterm linkend="gl-secondary-interface">secondary
	  interface</glossterm> of the GridFTP door, e.g. because they
	  are in a private subnet, the GridFTP door will still tell
	  the pool to connect to its primary interface, which might be
	  unreachable. 
	</para>

	<para>
	  The reason for this is that the control communication
	  between the door and the pool is done via the network of TCP
	  connections which have been established at start-up. In the
	  standard setup this communication is routed via the dCache
	  domain. However, for the data transfer, the pool connects to
	  the GridFTP door. The IP address it connects to is sent by
	  the GridFTP door to the pool via the control
	  connection. Since the GridFTP door cannot find out which of
	  its interfaces the pool should use, it normally sends the IP
	  address of the <glossterm
	  linkend="gl-primary-interface">primary
	  interface</glossterm>.
	</para>
      </section>

      <section>
	<title>Solution</title>
	<para>
	  Tell the GridFTP door explicitly which IP it should send to
	  the pool for the data connection with the
	  <option>-ftp-adapter-internal-interface</option>
	  option. E.g. if the pools should connect to the secondary
	  interface of the GridFTP door host which has the IP address
	  <literal>10.0.1.1</literal>, the following batch file would
	  be appropriate:
	</para>

	<example>
	  <title>Batch file for two GridFTP doors serving separate network interfaces</title>
	  <programlisting>set printout default 2
set printout CellGlue none
onerror shutdown
check -strong setupFile
copy file:${setupFile} context:setupContext
import context -c setupContext
check -strong serviceLocatorPort serviceLocatorHost
check -strong sshPort ftpPort
create dmg.cells.services.RoutingManager  RoutingMgr
create dmg.cells.services.LocationManager lm \
       "${serviceLocatorHost} ${serviceLocatorPort}"

create dmg.cells.services.login.LoginManager GFTP \
            "2811 \
             -export \
             diskCacheV111.doors.GsiFtpDoorV1 \
             -prot=raw \
             -clientDataPortRange=${clientDataPortRange} \
            -root=${ftpBase} \
             -kpwd-file=${kpwdFile} \
             -tlog=/tmp/dcache-ftp-tlog \
             -maxLogin=100 \
             -brokerUpdateTime=5 \
             -protocolFamily=gsiftp \
             -loginBroker=LoginBroker \
             -poolManagerTimeout=5400 \
             -pnfsTimeout=120 \
             -maxRetries=80 \
             -maxStreamsPerClient=10 \
	    <emphasis>-ftp-adapter-internal-interface=10.0.1.1 \</emphasis>
"</programlisting>
	</example>

	<para>
	  This batch file is very similar to the batch file for the
	  GridFTP door in the standard setup. (Comments have been left
	  out.) The emphasized last line has the desired effect.
	</para>
	  
      </section>

    </section>

    <section id="cb-net-dmz">
      <title>Doors in the DMZ</title>
      
      <section>
	<title>Description</title>
	<para>
	  Some doors - e.g. for grid access - are located in the DMZ
	  while the rest of the dCache instance is in the
	  intranet. The firewall is configured in such a way that the
	  doors cannot reach the location manager (usually on the
	  admin node together with the pool manager) via port NOCLUE.
	</para>
      </section>

      <section>
	<title>Solution</title>
	<para>
	  Please contact <email>support@dcache.org</email> if you need
	  a solution for this problem.
	</para>
      </section>

    </section>



  </chapter>


  <chapter id="cb-accounting">
    <title>Accounting</title>

    <para>
      TODO: There are a few loose ends in this chapter. A meta-intro might
      also be in order.
    </para>

    <para>
      The raw information about all dCache activities can be found in
      <filename>billing/<replaceable>YYYY</replaceable>/<replaceable>MM</replaceable>/billing-<replaceable>YYYY.MM.DD</replaceable></filename>.<footnote>
	<para>
	  Filenames will always be relative to the dCache installation
	  directory, which defaults to
	  <filename>/opt/d-cache/</filename>. 
	</para>
      </footnote>
      A typical line looks like
      <informalexample>
	<screen>05.31 22:35:16 [pool:<replaceable>pool-name</replaceable>:transfer] [000100000000000000001320,24675] myStore:STRING@osm 24675 474 true {GFtp-1.0 <replaceable>client-host-fqn</replaceable> 37592} {0:""}</screen>
      </informalexample>
      EXPLAIN! SYNOPSIS? Maybe too confusing
      
      It seems (though we are not sure) that the ftp transfer
      into the system was interrupted after having transferred
      14 bytes. Here you possibly found a small problem in the
      dCache :-) . The 14 in [PNFSID,14] is the number of bytes
      on the pool disk. The number '-1' SHOULD be the bytes
      transferred into the pool. This is inconsistent.
      It is supposed to be -1 if the mover HASN"T finished the transfer
      ftp protocolwise. On the other hand, if it was
      interrupted, it SHOULDN"T  say    {0,""} because
      this means : no error detected.
      The 474 is the number of milliseconds the transfer took
      and the 37592 is the client host data transfer listen port.
    </para>

    <para>
      The dCache web interface (described in <xref
      linkend="cb-mon-op-web"/>) contains under the menue point
      <quote>Actions Log</quote> summary information extracted from
      the information in the <filename>billing</filename>-directory.
    </para>

    <para>
      The accounting information can also be redirected into a
      database. For this the start-up configuration of the standard
      admin node has to be modified. (See <xref
      linkend="cf-cell-startup"/> for background information.) The
      billing information is written by the <quote>billing</quote>
      cell, which is started in the <quote>httpd</quote>
      domain. Therefore, the corresponding
      <command>create</command>-line in the
      <filename>config/httpd.batch</filename> file has to be changed
      to something like
      <programlisting>create ... TODO: FILL</programlisting>

      TODO: Create Tables? Schema?

      After that, the <quote>httpd</quote> domain has to be restarted with
<screen><rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/httpd stop
<rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/httpd -logfile=<replaceable>/opt/d-cache/</replaceable>log/httpd.log start</screen>
    </para>

    <para>
      There is also a nice little script which converts the billing
      files into something nice and neat. It can be found at
      <email>support@dcache.org</email>.
    </para>

  </chapter>

  <chapter id="cb-proto">
    <title>Protocols</title>
    
    <section id="cb-proto-dis-dcap">
      <title>Disableing unauthenticated dCap via SRM</title>

      <para>
	NOCLUE: Meta-intro? Where to put this? A more general section about
	protocolls in Configuration?
      </para>

      <para>
	In some cases SRM transfers fail because they are tried via
	the plain dCap protocol (URL starts with
	<literal>dcap://</literal>). Since plain dCap is
	unauthenticated, the dCache server will have no information
	about the user trying to access the system. While the transfer
	will succeed if the UNIX file permissions allow access to
	anybody (e.g. mode 777), it will fail otherwise. 
      </para>

      <para>
	Usually all doors are registered in SRM as potential access
	points for dCache. During a protocol negotiation the SRM
	chooses one of the available doors. You can force srmcp to use
	the gsidcap protocol (<option>-protocol=gsidcap</option>) or
	you can unregister plain, unauthenticated dCap from known
	protocols: From the file
	<filename>config/door.batch</filename> remove
	<option>-loginBroker=LoginBroker</option> and restart dcap
	door with

<screen><rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/door stop
<rootprompt/><replaceable>/opt/d-cache/</replaceable>jobs/door -logfile=<replaceable>/opt/d-cache/</replaceable>log/door.log start</screen>

      </para>

    </section>

  </chapter>

  <chapter id="cb-adv">
    <title>Advanced Tuning</title>

    <para>
      The use cases described in this chapter are only relevant for
      large-scale dCache Instances which require special tuning
      according to a longer experience with client behaviour.
    </para>

    <section id="cb-adv-multi-mover-queues">
      <title>Multiple Queues for Movers in each Pool</title>


      <section id="cb-adv-multi-mover-queues-d">
	<title>Description</title>

	<para>
	  Client requests to a dCache system may have rather diverse
	  bahaviour. Sometimes it is possible to classify them into
	  several typical usage patterns. An example are the following
	  two usage patterns: 
	  <example>
	    <title>To Concurrent Usage Patterns</title>
	    <para>
	      Data is copied with a high transfer rate to the dCache
	      system from an external source. This is done via the
	      GridFTP protocol. At the same time batch jobs on a local
	      farm process data. Since they only need a small part of
	      each file, they use the dCap protocol via the dCap
	      library and seek to the position in the file they are
	      interested in, read a few bytes, do a few hours of
	      calculations, and finally read some more data.
	    </para>
	  </example>
	  As long as the number of active requests do not exceed the
	  maximum number of allowed active requests, the two types of
	  requests are processed concurrently. The GridFTP transfers
	  complete at a high rate while the processing jobs take hours
	  to finish. This maximum number of allowed requests is set
	  with <cellcommandref linkend="cmd-mover_set_max_active"/>
	  and should be tuned according to capabilities of the pool
	  host.
	</para>

	<para>
	  However, if requests are queued, the slow processing jobs
	  might clog up the queue and not let the fast GridFTP request
	  through, even though the pool just sits there waiting for
	  the processing jobs to request more data. While this could
	  be temporarily remedied by setting the maximum active
	  requests to a higher value, then in turn GridFTP request
	  would put a very high load on the pool host.
	</para>
	
	<para>
	  The above example is pretty realistic: As a rule of thumb,
	  GridFTP requests are fastest, dCap requests with the
	  <command>dccp</command> program are a little slower and dCap
	  requests with the dCap library are very slow. However, the
	  usage patterns might be different at other sites and also
	  might change over time.
	</para>
      </section>

      <section id="cb-adv-multi-mover-queues-s">
	<title>Solution</title>

	<para>
	  Use separate queues for the movers, depending on the door
	  initiating them. This easily allows for a separation of
	  requests of separate protocols. Multiple mover queues are
	  only available in dCache version 1.6.6 or later.
	</para>

	<para>
	  A finer grained queue selection mechanism based on, e.g. the
	  IP of the client or the file which has been requested, is
	  not possible with this mechanism. However, the <glossterm
	  linkend="gl-pm-comp-psu">pool selection unit
	  (PSU)</glossterm> may provide a separation onto separate
	  pools using those criteria.
	</para>
	
	<para>
	  In the above example, two separate queues for fast GridFTP
	  transfers and slow dCap library access would solve the
	  problem. The maximum number of active movers for the GridFTP
	  queue should be set to a lower value compared to the dCap
	  queue since the fast GridFTP transfers will put a high load
	  on the system while the dCap requests will be mostly idle.
	</para>

      </section>

      <section id="cb-adv-multi-mover-queues-c">
	<title>Configuration</title>

	<para>
<screen>
	       With production-1-6-6 dCache supports up to 10 distinct mover queues per pool. They only cover client - dCache data movements (ftp,dcap,http). Pool to pool or HSM transfers are still handled by the already established queues. Each of the new queues is identified by its name, which is defined on creation of the pool. Different mover queues may have a different maximum number of movers allowed. (Which is actually the basic idea of diffent mover queues). Technically, different pools may have a different number of mover queues and/or different queue names, although this doesn't make too much sense. The first queue specified in the configuration (see below) is per definition the default mover queue, independed of its actual name. Requests, not requesting a particular mover queue or requests requesting a mover queue, not existing on the selected pool, are handled by the default mover queue.

    Each door may be configured to use a particular mover queue. The pool, selected for this request, doesn't depend on the selected mover queue. So a request may go to a pool which doesn't have the particular mover queue configured and will consequently end up in the default mover queue of that pool.

    For the dCap protocol, the corresponding door may be configured to allow the client to determine the mover queue name. In that case the client may use the extra option facility to specify a mover queue. 

Configuration

    Pool Configuration

        Pool startup option :

           create diskCacheV111.pools.MultiProtocolPool2  poolName  \
                    "basicPoolPath                                  \
                     -io-queues=queueName-1[,queueName-2[,...queueName-10]]    \
                     [OTHER OPTIONS]"
        

        Up to 10 queue may be given. The first name specified will become the default mover queue. The default mover queue is choosen if an incoming request doesn't specify a queue or specifies a queue not available on that pool. 

    Additional pool commands

        mover ls

            Compatibility command. Lists all requests from all queues. 

        mover ls -queue

            Lists all requests sorted according to the mover queue. 

        mover ls -queue=mover queue name

            Lists all requests of queue mover queue name. 

        mover set max active maxActiveCount -queue=mover queue name

            Sets the maximum number of allowed mover of queue mover queue name. 

        mover set max active maxActiveCount

            Compatibiltiy command. Sets the maximum number of allowed mover of the default mover queue. 

    Ftp Door (Abstract) configuration

        Gsi Ftp startup option :

            create dmg.cells.services.login.LoginManager GFTP \
                       "portName                 \
                        diskCacheV111.doors.GsiFtpDoorV1 \
                        -io-queue=queueName      \
                        [OTHER OPTIONS]"
        

        All requests send from this door will ask to be scheduled to the given mover queue. The selection of the pool is not affected. 

    DCap Door (Abstract) configuration

        DCap startup option :

           create dmg.cells.services.login.LoginManager DCap \
                       "${dCapPort} \
                        diskCacheV111.doors.DCapDoor \
                        -io-queue=queueName\
                        -io-queue-overwrite=allowed|denied \
                        [OTHER OPTIONS]"
        

        All requests send from this door will ask to be scheduled to the given mover queue. The selection of the pool is not affected. If io-queue-overwrite=allowed is given the dcap client is allowed to request a certain mover queue. For the dCap library this is done, using the extra option.

           dccp -X-io-queue=queueName   source destination [OPTION]
        

Cost module

    The cost is correctly calculated for the individual queues. The cm set magic on option is not yet honored. 

Queue web page

    The mover queue web page shows all queues defined by the individual pools. Queues, not defined in a pool, are marked with '-1' instead of the actual mover numbers. 


define context startPools endDefine
  create diskCacheV111.pools.MultiProtocolPool2 ${0} \
  "!MoverMap \
  ${1} \
  -recover-control=yes \
  -version=4 \
  -sticky=allowed \
  -sendHitInfoMessages=yes \
  -${2} -${3} -${4} -${5} -${6} -${7} -${8} \
"
</screen>

	</para>

      </section>

    </section>

  </chapter>

</part>
